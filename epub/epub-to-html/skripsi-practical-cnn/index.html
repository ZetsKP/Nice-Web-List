<html><head><meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<link href="style.css" rel="stylesheet" type="text/css" />

<!-- UNTUK SCROLL TO TOP -->
<link href="../../to-top.css" rel="stylesheet" type="text/css" />
<script src="../../jquery-2.1.3.js"></script>
<script src="../../to-top.js"></script>
<!-- ------------------- -->

<title>Practical Convolutional Neural Networks</title></head><body>

<!-- UNTUK SCROLL TO TOP -->
<a href="#daftar-isi" class="btn-scroll" id="bottom">TOP</a>
<!-- ------------------- -->
    
<div class="calibre" id="calibre_link-79">
        <section>

            
            <article>
                
<div class="title-page-name"><strong class="calibre1">Practical Convolutional Neural Networks&nbsp;<br class="title-page-name" />
<br class="title-page-name" />
<br class="title-page-name" /></strong>
<p class="calibre2">&nbsp;</p>
</div>
<div class="title-page-name">Implement advanced deep learning models using Python</div>
<p class="calibre2">&nbsp;</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2"></p>
<div class="title-page-name"><span>Mohit Sewak</span></div>
<div class="title-page-name"><span>Md. Rezaul Karim</span></div>
<div class="title-page-name"><span>Pradeep Pujari</span></div>
<p class="calibre2">&nbsp;</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2"></p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2"></p>
<p class="calibre2"><img class="alignnone" src="images/000048.png" /></p>
<div class="cdpalignleft"><strong class="calibre1"><strong class="calibre1">BIRMINGHAM - MUMBAI</strong></strong></div>
<p class="calibre2"></p>
<p class="calibre2"></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-80">
        <section>

            
            <article>
                


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-81">
        <section>

                            <header>
                    <h1 class="header-title">Practical Convolutional Neural Networks</h1>
                </header>
            
            <article>
                
<p class="calibre3">Copyright Â© 2018 Packt Publishing</p>
<p class="calibre3">All rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.</p>
<p class="calibre3">Every effort has been made in the preparation of this book to ensure the accuracy of the information presented. However, the information contained in this book is sold without warranty, either express or implied. Neither the author(s), nor Packt Publishing or its dealers and distributors, will be held liable for any damages caused or alleged to have been caused directly or indirectly by this book.</p>
<p class="calibre3">Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy of this information.</p>
<p class="calibre3"><strong class="calibre1">Commissioning Editor:</strong><span>&nbsp;Sunith Shetty</span><br class="title-page-name" />
<strong class="calibre1">Acquisition Editor:</strong><span>&nbsp;Vinay Argekar</span><br class="title-page-name" />
<strong class="calibre1">Content Development Editor:</strong><span>&nbsp;Cheryl Dsa</span><br class="title-page-name" />
<strong class="calibre1">Technical Editor:</strong><span>&nbsp;Sagar Sawant</span><br class="title-page-name" />
<strong class="calibre1">Copy Editor:</strong><span>&nbsp;Vikrant Phadke, Safis Editing</span><br class="title-page-name" />
<strong class="calibre1">Project Coordinator:</strong><span>&nbsp;Nidhi Joshi</span><br class="title-page-name" />
<strong class="calibre1">Proofreader:</strong><span>&nbsp;Safis Editing</span><br class="title-page-name" />
<strong class="calibre1">Indexer:</strong><span>&nbsp;Tejal Daruwale Soni</span><br class="title-page-name" />
<strong class="calibre1">Graphics:</strong><span>&nbsp;Tania Dutta</span><br class="title-page-name" />
<strong class="calibre1">Production Coordinator:</strong><span>&nbsp;Arvindkumar Gupta</span></p>
<p class="calibre3">First published: February 2018</p>
<p class="calibre3">Production reference: 1230218</p>
<p class="calibre3">Published by Packt Publishing Ltd.<br class="title-page-name" />
Livery Place<br class="title-page-name" />
35 Livery Street<br class="title-page-name" />
Birmingham<br class="title-page-name" />
B3 2PB, UK.</p>
<p class="calibre3">ISBN 978-1-78839-230-3</p>
<p class="calibre3"><a href="http://www.packtpub.com" target="_blank" class="calibre4">www.packtpub.com</a></p>
<p class="calibre2"></p>
<p class="calibre2"></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-82">
        <section>

            
            <article>
                
<div class="cdpalignleft"><img src="images/000013.jpg" class="calibre5" /></div>
<div class="cdpalignleft"><span><a href="https://mapt.io/" target="_blank" class="calibre6">mapt.io</a></span></div>
<p class="calibre2">Mapt is an online digital library that gives you full access to over 5,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-29">
        <section>

                            <header>
                    <h1 class="header-title">Why subscribe?</h1>
                </header>
            
            <article>
                
<ul class="calibre7">
<li class="calibre8">
<p class="calibre9">Spend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals</p>
</li>
<li class="calibre8">
<p class="calibre9"><span class="calibre10">Improve your learning with Skill Plans built especially for you</span></p>
</li>
<li class="calibre8">
<p class="calibre9"><span class="calibre10">Get a free eBook or video every month</span></p>
</li>
<li class="calibre8">
<p class="calibre9"><span class="calibre10">Mapt is fully searchable</span></p>
</li>
<li class="calibre8">
<p class="calibre9"><span class="calibre10">Copy and paste, print, and bookmark content</span></p>
</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-67">
        <section>

                            <header>
                    <h1 class="header-title">PacktPub.com</h1>
                </header>
            
            <article>
                
<p class="calibre2">Did you know that Packt offers eBook versions of every book published, with PDF and ePub files available? You can upgrade to the eBook version at <span class="calibre10"><a href="http://www.PacktPub.com" target="_blank" class="calibre11">www.PacktPub.com</a></span> and as a print book customer, you are entitled to a discount on the eBook copy. Get in touch with us at <kbd class="calibre12"><span>service@packtpub.com</span></kbd> for more details.</p>
<p class="calibre2">At <span class="calibre10"><a href="http://www.packtpub.com" target="_blank" class="calibre11">www.PacktPub.com</a></span>, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks.</p>
<p class="calibre2"></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-3">
        <section>

                            <header>
                    <h1 class="header-title">Contributors</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-73">
        <section>

                            <header>
                    <h1 class="header-title">About the authors</h1>
                </header>
            
            <article>
                
<p class="calibre2"><strong class="calibre13">Mohit Sewak</strong> is a senior cognitive data scientist with IBM, and a PhD scholar in AI and CS at BITS Pilani. He holds several patents and publications in AI, deep learning, and machine learning. He has been the lead data scientist for some very successful global AI/ ML software and industry solutions and was earlier engaged in solutioning and research for the Watson Cognitive Commerce product line. He has 14 years of rich experience in architecting and solutioning with TensorFlow, Torch, Caffe, Theano, Keras, Watson, and more.</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2"><strong class="calibre13">Md. Rezaul Karim</strong> is a research scientist at Fraunhofer FIT, Germany. He is also a PhD candidate at RWTH Aachen University, Germany. Before joining FIT, he worked as a researcher at the Insight Center for Data Analytics, Ireland. He was a lead engineer at Samsung Electronics, Korea.</p>
<p class="calibre2">He has 9 years of R&amp;D experience with C++, Java, R, Scala, and Python. He has published research papers on bioinformatics, big data, and deep learning. He has practical working experience with Spark, Zeppelin, Hadoop, Keras, Scikit-Learn, TensorFlow, Deeplearning4j, MXNet, and H2O.</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2"><span class="calibre10"><strong class="calibre13">Pradeep Pujari</strong> is machine learning engineer at Walmart Labs and a distinguished member of&nbsp;</span><span class="calibre10">ACM. His core domain expertise is in information retrieval, machine learning, and natural language processing. In his free time, he loves exploring AI technologies, reading, and mentoring.</span></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-83">
        <section>

                            <header>
                    <h1 class="header-title">About the reviewer</h1>
                </header>
            
            <article>
                
<p class="calibre2"><strong class="calibre13">Sumit Pal</strong>&nbsp;is a published author with Apress. He has more than 22 years of experience in software, from start-ups to enterprises, and is&nbsp;an independent consultant working with big data, data visualization, and data science. He builds end-to-end data-driven analytic systems.</p>
<p class="calibre2">He has worked for Microsoft (SQLServer), Oracle (OLAP Kernel), and Verizon. He advises clients on their data architectures and build solutions in Spark and Scala. He has spoken at many conferences in North America and Europe and has developed a big data analyst training for Experfy. He has an MS and BS in computer science.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-10">
        <section>

                            <header>
                    <h1 class="header-title">Packt is searching for authors like you</h1>
                </header>
            
            <article>
                
<p class="calibre2">If you're interested in becoming an author for Packt, please visit <a href="http://authors.packtpub.com" target="_blank" class="calibre11">authors.packtpub.com</a> and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.</p>
<p class="calibre2"></p>
<p class="calibre2"></p>
<p class="calibre2"></p>


            </article>

            
        </section>
    </div>

<!-- UNTUK SCROLL TO TOP -->
<div id="daftar-isi"></div>
<!-- ------------------- -->

<div class="calibre" id="calibre_link-78">
        <section>
            <header>
                <h1 class="calibre14">Table of Contents</h1>
            </header>

            <article>
                <nav type="toc" id="calibre_link-168">
                    <ol class="calibre15"><li class="front-matter" hidden="hidden">
            <a class="calibre11" href="#calibre_link-79">Title Page</a>
    
    
</li>
<li class="front-matter" hidden="hidden">
            <a class="calibre11" href="#calibre_link-80">Copyright and Credits</a>
    
    <ol class="calibre16"><li class="front-matter" hidden="hidden">
            <a class="calibre11" href="#calibre_link-81">Practical Convolutional Neural Networks</a>
    
    
</li>
</ol>
</li>
<li class="front-matter" hidden="hidden">
            <a class="calibre11" href="#calibre_link-82">Packt Upsell</a>
    
    <ol class="calibre16"><li class="front-matter" hidden="hidden">
            <a class="calibre11" href="#calibre_link-29">Why subscribe?</a>
    
    
</li>
<li class="front-matter" hidden="hidden">
            <a class="calibre11" href="#calibre_link-67">PacktPub.com</a>
    
    
</li>
</ol>
</li>
<li class="front-matter" hidden="hidden">
            <a class="calibre11" href="#calibre_link-3">Contributors</a>
    
    <ol class="calibre16"><li class="front-matter" hidden="hidden">
            <a class="calibre11" href="#calibre_link-73">About the authors</a>
    
    
</li>
<li class="front-matter" hidden="hidden">
            <a class="calibre11" href="#calibre_link-83">About the reviewer</a>
    
    
</li>
<li class="front-matter" hidden="hidden">
            <a class="calibre11" href="#calibre_link-10">Packt is searching for authors like you</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-84">Preface</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-13">Who this book is for</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-85">What this book covers</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-16">To get the most out of this book</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-86">Download the example code files</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-18">Download the color images</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-87">Conventions used</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-21">Get in touch</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-88">Reviews</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li class="chapter" value="1">
            <a class="calibre11" href="#calibre_link-24">Deep Neural Networks &ndash; Overview</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-89">Building blocks of a neural network</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-90">Introduction to TensorFlow</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-68">Installing TensorFlow</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-91">For macOS X/Linux variants</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-71">TensorFlow basics</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-92">Basic math with TensorFlow</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-74">Softmax in TensorFlow</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-93">Introduction to the MNIST dataset&nbsp;</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-94">The simplest artificial neural network</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-95">Building a single-layer neural network with TensorFlow</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-20">Keras deep learning library overview</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-96">Layers in the Keras model</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-23">Handwritten number recognition with Keras and MNIST</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-97">Retrieving training and test data</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-27">Flattened data</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-98">Visualizing the training data</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-32">Building the network</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-99">Training the network</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-34">Testing</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-100">Understanding backpropagation&nbsp;</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-101">Summary</a>
    
    
</li>
</ol>
</li>
<li class="chapter" value="2">
            <a class="calibre11" href="#calibre_link-8">Introduction to Convolutional Neural Networks</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-46">History of CNNs</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-102">Convolutional neural networks</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-77">How do computers interpret images?</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-103">Code for visualizing an image&nbsp;</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-104">Dropout</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-105">Input layer</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-106">Convolutional layer</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-107">Convolutional layers in Keras</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-108">Pooling layer</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-1">Practical example &ndash; image classification</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-109">Image augmentation</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-110">Summary</a>
    
    
</li>
</ol>
</li>
<li class="chapter" value="3">
            <a class="calibre11" href="#calibre_link-111">Build Your First CNN and Performance Optimization</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-7">CNN architectures and drawbacks of DNNs</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-112">Convolutional operations</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-12">Pooling, stride, and padding operations</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-55">Fully connected layer</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-113">Convolution and pooling operations in TensorFlow</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-42">Applying pooling operations in TensorFlow</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-114">Convolution operations in TensorFlow</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-115">Training a CNN</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-116">Weight and bias initialization</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-45">Regularization</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-117">Activation functions</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-47">Using sigmoid</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-4">Using tanh</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-50">Using ReLU</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-118">Building, training, and evaluating our first CNN</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-54">Dataset description</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-119">Step 1 &ndash; Loading the required packages</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-56">Step 2 &ndash; Loading the training/test images to generate train/test set</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-120">Step 3- Defining CNN hyperparameters</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-58">Step 4 &ndash; Constructing the CNN layers</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-121">Step 5 &ndash; Preparing the TensorFlow graph</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-60">Step 6 &ndash; Creating a CNN model</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-122">Step 7 &ndash; Running the TensorFlow graph to train the CNN model</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-63">Step 8 &ndash; Model evaluation</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-123">Model performance optimization</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-66">Number of hidden layers</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-124">Number of neurons per hidden layer</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-9">Batch normalization</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-125">Advanced regularization and avoiding overfitting</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-126">Applying dropout operations with TensorFlow</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-127">Which optimizer to use?</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-37">Memory tuning</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-36">Appropriate layer placement</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-128">Building the second CNN by putting everything together</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-38">Dataset description and preprocessing</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-129">Creating the CNN model</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-41">Training and evaluating the network</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-130">Summary</a>
    
    
</li>
</ol>
</li>
<li class="chapter" value="4">
            <a class="calibre11" href="#calibre_link-43">Popular CNN Model Architectures</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-131">Introduction to ImageNet</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-44">LeNet</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-132">AlexNet architecture</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-76">Traffic sign classifiers using AlexNet</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-133">VGGNet architecture</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-48">VGG16 image classification code example</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-134">GoogLeNet architecture</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-53">Architecture insights</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-135">Inception module</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-136">ResNet architecture</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-137">Summary</a>
    
    
</li>
</ol>
</li>
<li class="chapter" value="5">
            <a class="calibre11" href="#calibre_link-57">Transfer Learning</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-52">Feature extraction approach</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-59">Target dataset is small and is similar to the original training dataset</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-138">Target dataset is small but different from the original training dataset</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-61">Target dataset is large and similar to the original training dataset</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-139">Target dataset is large and different from the original training dataset</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-65">Transfer learning example</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-140">Multi-task learning</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-141">Summary</a>
    
    
</li>
</ol>
</li>
<li class="chapter" value="6">
            <a class="calibre11" href="#calibre_link-142">Autoencoders for CNN</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-70">Introducing to autoencoders</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-143">Convolutional autoencoder</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-72">Applications</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-144">An example of compression</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-69">Summary</a>
    
    
</li>
</ol>
</li>
<li class="chapter" value="7">
            <a class="calibre11" href="#calibre_link-31">Object Detection and Instance Segmentation with CNN</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-145">The differences between object detection and image classification</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-146">Why is object detection much more challenging than image classification?</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-147">Traditional, nonCNN approaches to object detection</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-148">Haar features, cascading classifiers, and the Viola-Jones algorithm</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-149">Haar Features</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-0">Cascading classifiers</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-150">The Viola-Jones algorithm</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-2">R-CNN&nbsp;&ndash;&nbsp;Regions with CNN features</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-151">Fast R-CNN&nbsp;&ndash;&nbsp;fast region-based CNN</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-6">Faster R-CNN&nbsp;&ndash; faster region proposal network-based CNN</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-152">Mask R-CNN&nbsp;&ndash; Instance segmentation with CNN</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-11">Instance segmentation in code</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-153">Creating the environment</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-14">Installing Python dependencies (Python2 environment)</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-5">Downloading and installing the COCO API and detectron library (OS shell commands)</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-17">Preparing the COCO dataset folder structure</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-154">Running the pre-trained model on the COCO dataset</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-19">References</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-155">Summary</a>
    
    
</li>
</ol>
</li>
<li class="chapter" value="8">
            <a class="calibre11" href="#calibre_link-22">GAN: Generating New Images with CNN</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-156">Pix2pix - Image-to-Image translation GAN</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-25">CycleGAN&nbsp;</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-157">Training a GAN model</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-28">GAN &ndash; code example</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-158">Calculating loss&nbsp;</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-33">Adding the optimizer</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-159">Semi-supervised learning and GAN</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-35">Feature matching</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-160">Semi-supervised classification using a GAN example</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-39">Deep convolutional GAN</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-161">Batch normalization</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-49">Summary</a>
    
    
</li>
</ol>
</li>
<li class="chapter" value="9">
            <a class="calibre11" href="#calibre_link-162">Attention Mechanism for CNN and Visual Models</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-62">Attention mechanism for image captioning</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-163">Types of Attention</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-75">Hard Attention</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-164">Soft Attention</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-30">Using attention to improve visual models</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-26">Reasons for sub-optimal performance of visual CNN models</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-64">Recurrent models of visual attention</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-15">Applying the RAM on a noisy MNIST sample</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-165">Glimpse Sensor in code</a>
    
    
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-40">References</a>
    
    
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-166">Summary</a>
    
    
</li>
</ol>
</li>
<li class="front-matter">
            <a class="calibre11" href="#calibre_link-51">Other Books You May Enjoy</a>
    
    <ol class="calibre16"><li class="front-matter">
            <a class="calibre11" href="#calibre_link-167">Leave a review - let other readers know what you think</a>
    
    
</li>
</ol>
</li>
</ol>
                </nav>
            </article>

            <footer class="calibre17"></footer>
        </section>
    </div>


<div class="calibre" id="calibre_link-84">
        <section>

                            <header>
                    <h1 class="header-title">Preface</h1>
                </header>
            
            <article>
                
<p class="calibre2">CNNs are revolutionizing several application domains, such as visual recognition systems, self-driving cars, medical discoveries, innovative e-commerce, and many more. This book gets you started with the building blocks of CNNs, while also guiding you through the best practices for implementing real-life CNN models and solutions. You will learn to create innovative solutions for image and video analytics to solve complex machine learning and computer vision problems.</p>
<p class="calibre2">This book starts with an overview of deep neural networks, with an example of image classification, and walks you through building your first CNN model. You will learn concepts such as transfer learning and autoencoders <span class="calibre10">with CNN that will enable you&nbsp;</span>to build very powerful models, even with limited supervised (<span class="calibre10">labeled image</span>) training data.</p>
<p class="calibre2">Later we build upon these learnings to achieve advanced vision-related algorithms and solutions for object detection, instance segmentation, generative (adversarial) networks, image captioning, attention mechanisms, and recurrent attention models for vision.<br class="calibre18" />
Besides giving you hands-on experience with the most intriguing vision models and architectures, this book explores cutting-edge and very recent researches in the areas of CNN and computer vision. This enable the user to foresee the future in this field and quick-start their innovation journey using advanced CNN solutions.<br class="calibre18" />
By the end of this book, you should be ready to implement advanced, effective, and efficient CNN models in your professional projects or personal initiatives while working on complex images and video datasets.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-13">
        <section>

                            <header>
                    <h1 class="header-title">Who this book is for</h1>
                </header>
            
            <article>
                
<p class="calibre2">This book is for data scientists, machine learning, and deep learning practitioners, and cognitive and artificial intelligence enthusiasts who want to move one step further in building CNNs. Get hands-on experience with extreme datasets and different CNN architectures to build efficient and smart ConvNet models. Basic knowledge of deep learning concepts and Python programming language is expected.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-85">
        <section>

                            <header>
                    <h1 class="header-title">What this book covers</h1>
                </header>
            
            <article>
                
<p class="calibre2"><a href="#calibre_link-24" class="calibre11">Chapter 1</a>, <em class="calibre19">Deep Neural Networks - Overview</em><span class="calibre10">, it gives a quick refresher of the science of deep neural networks and different frameworks that can be used to implement such networks, with the mathematics behind them.<br class="calibre18" /></span></p>
<p class="calibre2"><a href="#calibre_link-8" class="calibre11">Chapter 2</a>, <em class="calibre19">Introduction to Convolutional Neural Networks</em><span class="calibre10">, it introduces the readers to convolutional neural networks and shows how deep learning can be used to extract insights from images.</span></p>
<p class="calibre2"><a href="#calibre_link-111" class="calibre11">Chapter 3</a>, <em class="calibre19">Build Your First CNN and Performance Optimization</em><span class="calibre10">, constructs a simple CNN model for image classification from scratch, and explains how to tune hyperparameters and optimize training time and performance of CNNs for improved efficiency and accuracy respectively.</span></p>
<p class="calibre2"><a href="#calibre_link-43" class="calibre11">Chapter 4</a>, <em class="calibre19">Popular CNN Model Architectures</em><span class="calibre10">, shows the advantages and working of different popular (and award winning) CNN architectures, how they differ from each other, and how to use them.</span></p>
<p class="calibre2"><a href="#calibre_link-57" class="calibre11">Chapter 5</a>, T<em class="calibre19">ransfer Learning</em><span class="calibre10">, teaches you to take an existing pretrained network and adapt it to a new and different dataset. There is also a custom classification problem for a real-life application using a technique called&nbsp;<strong class="calibre13">transfer learning</strong>.</span></p>
<p class="calibre2"><a href="#calibre_link-142" class="calibre11">Chapter 6</a>, A<em class="calibre19">utoencoders for CNN</em><span class="calibre10">, introduces an unsupervised learning technique called <strong class="calibre13">autoencoders</strong>. We walk through different applications&nbsp;of autoencoders for CNN, such as image compression.</span></p>
<p class="calibre2"><a href="#calibre_link-31" class="calibre11">Chapter 7</a>, <em class="calibre19">Object Detection and Instance Segmentation with CNN</em><span class="calibre10">, teaches the difference between object detection, instance segmentation, and image classification. We then learn multiple techniques for object detection and instance segmentation with CNNs.</span></p>
<p class="calibre2"><a href="#calibre_link-22" class="calibre11">Chapter 8</a>, <em class="calibre19">GAN&mdash;Generating New Images with CNN</em><span class="calibre10">, explores generative CNN Networks, and then we combine them with our learned discriminative CNN networks to create new images with CNN/GAN.</span></p>
<p class="calibre2"><a href="#calibre_link-162" class="calibre11">Chapter 9</a>, <em class="calibre19">Attention Mechanism for CNN and Visual Models</em><span class="calibre10">, teaches the intuition behind attention in deep learning and learn how attention-based models are used to implement some advanced solutions (image captioning and RAM). We also understand the different types of attention and the role of reinforcement learning with respect to the hard attention mechanism.&nbsp;</span></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-16">
        <section>

                            <header>
                    <h1 class="header-title">To get the most out of this book</h1>
                </header>
            
            <article>
                
<p class="calibre2">This book is focused on building CNNs with&nbsp;Python programming language. We have used Python version 2.7 (2x) to build various applications and the open source and enterprise-ready professional software using Python, Spyder, Anaconda, and PyCharm. Many of the examples are also compatible with Python 3x. As a good practice, we encourage users to use Python virtual environments for implementing these codes.</p>
<p class="calibre2">We focus on how to utilize various Python and deep learning libraries (Keras, TensorFlow, and Caffe) in the best possible way to build real-world applications. In that spirit, we have tried to keep all of the code as friendly and readable as possible. We feel that this will enable our readers to easily understand the code and readily use it in different scenarios.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-86">
        <section>

                            <header>
                    <h1 class="header-title">Download the example code files</h1>
                </header>
            
            <article>
                
<p class="calibre2">You can download the example code files for this book from your account at <a href="http://www.packtpub.com" class="calibre11">www.packtpub.com</a>. If you purchased this book elsewhere, you can visit <a href="http://www.packtpub.com/support" class="calibre11">www.packtpub.com/support</a> and register to have the files emailed directly to you.</p>
<p class="calibre2">You can download the code files by following these steps:</p>
<ol class="calibre15">
<li class="calibre8">Log in or register at <a href="http://www.packtpub.com/support" class="calibre11">www.packtpub.com</a>.</li>
<li class="calibre8">Select the <span>SUPPORT</span> tab.</li>
<li class="calibre8">Click on <span>Code Downloads &amp; Errata</span>.</li>
<li class="calibre8">Enter the name of the book in the <span>Search</span> box and follow the onscreen instructions.</li>
</ol>
<p class="calibre2">Once the file is downloaded, please make sure that you unzip or extract the folder using the latest version of:</p>
<ul class="calibre7">
<li class="calibre8">WinRAR/7-Zip for Windows</li>
<li class="calibre8">Zipeg/iZip/UnRarX for Mac</li>
<li class="calibre8">7-Zip/PeaZip for Linux</li>
</ul>
<p class="calibre2"><span class="calibre10">The code bundle for the book is also hosted on GitHub at</span><span class="calibre10">&nbsp;</span><a href="https://github.com/PacktPublishing/Practical-Convolutional-Neural-Networks" target="_blank" class="calibre11"><span>https://github.com/PacktPublishing/Practical-Convolutional-Neural-Networks</span></a><span class="calibre10">.&nbsp;</span><span class="calibre10">In case there's an update to the code, it will be updated on the existing GitHub repository.<br class="calibre18" /></span></p>
<p class="calibre2"><span class="calibre10">We also have other code bundles from our rich catalog of books and videos available at</span><span class="calibre10">&nbsp;</span><strong class="calibre13"><span class="calibre10"><a href="https://github.com/PacktPublishing/" class="calibre11">https://github.com/PacktPublishing/</a></span></strong><span class="calibre10">. Check them out!</span></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-18">
        <section>

                            <header>
                    <h1 class="header-title">Download the color images</h1>
                </header>
            
            <article>
                
<p class="calibre2">We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: <a href="http://www.packtpub.com/sites/default/files/downloads/PracticalConvolutionalNeuralNetworks_ColorImages.pdf" target="_blank" class="calibre11">http://www.packtpub.com/sites/default/files/downloads/PracticalConvolutionalNeuralNetworks_ColorImages.pdf</a>.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-87">
        <section>

                            <header>
                    <h1 class="header-title">Conventions used</h1>
                </header>
            
            <article>
                
<p class="calibre2">There are a number of text conventions used throughout this book.</p>
<p class="calibre2"><kbd class="calibre12">CodeInText</kbd>: <span class="calibre10">Indicates c</span>ode words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. <span class="calibre10">Here is an example:</span> "Mount the downloaded <kbd class="calibre12">WebStorm-10*.dmg</kbd> disk image file as another disk in your system."</p>
<p class="calibre2">A block of code is set as follows:</p>
<pre class="calibre20"><span>import</span> tensorflow <span>as</span> tf

<span>#Creating TensorFlow object </span>
hello_constant = tf.constant(<span>'Hello World!', name = 'hello_constant'</span>)
#Creating a session object for execution of the computational graph
<span>with</span> tf.Session() <span>as</span> sess:</pre>
<p class="calibre2">When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:</p>
<pre class="calibre20">x = tf.subtract(1, <strong class="calibre1">2</strong>,name=None) # -1<br class="title-page-name" />y = <strong class="calibre1">tf.multiply</strong>(2, 5,name=None) # 10</pre>
<p class="calibre2"><strong class="calibre13">Bold</strong>: Indicates a new term, an important word, or w<span class="calibre10">ords that you see onscreen. For example, words in menus or dialog boxes appear in the text like this. Here is an example: "Select <span class="calibre10">System info</span> from the <span class="calibre10">Administration</span> panel.</span><span class="calibre10">"</span></p>
<div class="packt_infobox">Warnings or important notes appear like this.</div>
<div class="packt_tip">Tips and tricks appear like this.</div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-21">
        <section>

                            <header>
                    <h1 class="header-title">Get in touch</h1>
                </header>
            
            <article>
                
<p class="calibre2">Feedback from our readers is always welcome.</p>
<p class="calibre2"><strong class="calibre13">General feedback</strong>: Email <kbd class="calibre12">feedback@packtpub.com</kbd> and mention the book title in the subject of your message. If you have questions about any aspect of this book, please email us at <kbd class="calibre12">questions@packtpub.com</kbd>.</p>
<p class="calibre2"><strong class="calibre13">Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="http://www.packtpub.com/submit-errata" class="calibre11">www.packtpub.com/submit-errata</a>, selecting your book, clicking on the Errata Submission Form link, and entering the details.</p>
<p class="calibre2"><strong class="calibre13">Piracy</strong>: If you come across any illegal copies of our works in any form on the Internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <kbd class="calibre12">copyright@packtpub.com</kbd> with a link to the material.</p>
<p class="calibre2"><strong class="calibre13">If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com/" class="calibre11">authors.packtpub.com</a>.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-88">
        <section>

                            <header>
                    <h1 class="header-title">Reviews</h1>
                </header>
            
            <article>
                
<p class="calibre2">Please leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions, we at Packt can understand what you think about our products, and our authors can see your feedback on their book. Thank you!</p>
<p class="calibre2">For more information about Packt, please visit&nbsp;<a href="https://www.packtpub.com/" class="calibre11">packtpub.com</a>.</p>
<p class="calibre2"></p>
<p class="calibre2"></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-24">
        <section>

                            <header>
                    <h1 class="header-title">Deep Neural Networks &ndash; Overview</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre10">In the past few years, we have seen remarkable progress in the field of AI (deep learning). Today, deep learning is the cornerstone of many advanced</span> <span class="calibre10">technological&nbsp;</span><span class="calibre10">applications, from self-driving cars to generating art and music. Scientists aim to help computers to not only understand speech but also speak in natural languages. Deep learning is a kind of machine learning method that is based on learning data representation as opposed to task-specific algorithms. Deep learning enables the computer to build complex concepts from simpler and smaller concepts.&nbsp;For example, a deep learning system recognizes the image of a person by combining lower label edges and corners and combines them into parts of the body in a hierarchical way. The day is not so far away when deep learning will be extended to applications that enable machines to think on their own.</span><br class="calibre18" /></p>
<p class="calibre2"><span class="calibre10">In this chapter, we will cover the following topics:</span></p>
<ul class="calibre7">
<li class="calibre8">Building blocks of a neural network</li>
<li class="calibre8">Introduction to TensorFlow</li>
<li class="calibre8">Introduction to Keras</li>
<li class="calibre8">Backpropagation</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-89">
        <section>

                            <header>
                    <h1 class="header-title">Building blocks of a neural network</h1>
                </header>
            
            <article>
                
<p class="calibre2">A neural network is made up of many artificial neurons. Is it a representation of the brain or is it a mathematical representation of some knowledge?&nbsp;Here, we will simply try to understand how a neural network is used in practice.<span class="calibre10">&nbsp;</span>A <strong class="calibre13">convolutional neural network</strong> (<strong class="calibre13">CNN</strong>) is a very special kind of multi-layer neural network. CNN is designed to recognize visual patterns directly from images with minimal processing. A graphical representation of this network is produced in the<span class="calibre10">&nbsp;</span>following image.&nbsp;<span class="calibre10">The field of neural networks was originally inspired by the goal of modeling biological neural systems, but since then it has branched in different directions and has become a matter of engineering and attaining good results in machine learning tasks.</span></p>
<p class="calibre2">An artificial neuron is a function that takes an input and produces an output. The number of neurons that are used depends on the task at hand. It could be as low as two or as many as several thousands. There are numerous ways of connecting artificial neurons together to create a CNN. One such topology that is commonly used is known as a<span class="calibre10">&nbsp;</span><strong class="calibre13">feed-forward network</strong>:</p>
<div class="mce-root"><img src="images/000100.png" class="calibre21" /></div>
<p class="calibre2">Each neuron receives inputs from other neurons. The effect of each input line on the neuron is controlled by the weight. The weight can be positive or negative. The entire neural network learns to perform useful computations for recognizing objects by understanding the language. Now, we can connect those neurons into a network known as a feed-forward network. This means that the neurons in each layer feed their output forward to the next layer until we get a final output. This can be written as follows:</p>
<div class="mce-root"><img class="fm-editor-equation" src="images/000012.png" /></div>
<div class="mce-root"><img class="fm-editor-equation1" src="images/000055.png" /></div>
<p class="calibre2">The preceding forward-propagating neuron can be implemented as follows:</p>
<pre class="calibre20">import numpy as np<br class="title-page-name" />import math<br class="title-page-name" /><br class="title-page-name" /><br class="title-page-name" />class Neuron(object):<br class="title-page-name" />    def __init__(self):<br class="title-page-name" />        self.weights = np.array([1.0, 2.0])<br class="title-page-name" />        self.bias = 0.0<br class="title-page-name" />    def forward(self, inputs):<br class="title-page-name" />        """ Assuming that inputs and weights are 1-D numpy arrays and the bias is a number """<br class="title-page-name" />        a_cell_sum = np.sum(inputs * self.weights) + self.bias<br class="title-page-name" />        result = 1.0 / (1.0 + math.exp(-a_cell_sum)) # This is the sigmoid activation function<br class="title-page-name" />        return result<br class="title-page-name" />neuron = Neuron()<br class="title-page-name" />output = neuron.forward(np.array([1,1]))<br class="title-page-name" />print(output)</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-90">
        <section>

                            <header>
                    <h1 class="header-title">Introduction to TensorFlow</h1>
                </header>
            
            <article>
                
<p class="calibre2">TensorFlow is based on graph-based computation. Consider the following math expression, for example:</p>
<p class="mce-root1"><em class="calibre19">c=(a+b)</em>,&nbsp;<em class="calibre19">d = b + 5</em>,</p>
<p class="mce-root1">&nbsp;<em class="calibre19">e = c * d&nbsp;</em></p>
<p class="calibre2">In TensorFlow, this is represented as a computational graph, as shown here. This is powerful because computations are done in parallel:</p>
<div class="mce-root"><img src="images/000031.png" class="calibre22" /></div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-68">
        <section>

                            <header>
                    <h1 class="header-title">Installing TensorFlow</h1>
                </header>
            
            <article>
                
<p class="calibre2">There are two easy ways to install TensorFlow:</p>
<ul class="calibre7">
<li class="calibre8">Using a virtual environment (recommended and described here)</li>
<li class="calibre8">With a Docker image</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-91">
        <section>

                            <header>
                    <h1 class="header-title">For macOS X/Linux variants</h1>
                </header>
            
            <article>
                
<p class="calibre2">The following code snippet creates a Python virtual environment and installs TensorFlow in that environment. You should have Anaconda installed before you run this code:</p>
<pre class="calibre20"><span>#Creates a virtual environment named "tensorflow_env" assuming that python 3.7 version is already installed.</span><br class="title-page-name" />conda create -n tensorflow_env python=3.7 <br class="title-page-name" />#Activate points to the environment named "tensorflow" <span> </span>
<span>source</span> activate tensorflow_env
conda install pandas matplotlib jupyter notebook scipy scikit-learn<br class="title-page-name" />#installs latest tensorflow version into environment tensorflow_env
pip3 install tensorflow </pre>
<p class="calibre2">Please check out the latest updates on the official TensorFlow page,&nbsp;<a href="https://www.tensorflow.org/install/" target="_blank" class="calibre11">https://www.tensorflow.org/install/</a>.</p>
<p class="calibre2"><span class="calibre10">Try running the following code in your Python console to validate your installation. The console should print <kbd class="calibre12">Hello World!</kbd>&nbsp;if TensorFlow is installed and working:</span></p>
<pre class="calibre20"><span>import</span> tensorflow <span>as</span> tf

<span>#Creating TensorFlow object </span>
hello_constant = tf.constant(<span>'Hello World!', name = 'hello_constant'</span>)
#Creating a session object for execution of the computational graph
<span>with</span> tf.Session() <span>as</span> sess:
    <span>#Implementing the tf.constant operation in the session</span>
    output = sess.run(hello_constant)
    print(output)</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-71">
        <section>

                            <header>
                    <h1 class="header-title">TensorFlow basics</h1>
                </header>
            
            <article>
                
<p class="calibre2">In TensorFlow,<span class="calibre10">&nbsp;data isn't stored as integers, floats, strings, or other primitives. These values are encapsulated in an object called a <strong class="calibre13">tensor</strong>.&nbsp;It consists of a set of primitive values shaped into an array of any number of dimensions. The number of dimensions in a tensor is called its&nbsp;<strong class="calibre13">rank</strong>.<strong class="calibre13">&nbsp;</strong>In the preceding example, <kbd class="calibre12">hello_constant</kbd> is a constant string tensor with rank zero. A few more examples of constant tensors are as follows:</span></p>
<pre class="calibre20"># A is an int32 tensor with rank = 0<br class="title-page-name" />A = tf.constant(123) <br class="title-page-name" /># B is an int32 tensor with dimension of 1 ( rank = 1 ) <br class="title-page-name" />B = tf.constant([123,456,789]) <br class="title-page-name" /># C is an int32 2- dimensional tensor <br class="title-page-name" />C = tf.constant([ [123,456,789], [222,333,444] ])</pre>
<p class="calibre2">TensorFlow's core program is based on the idea of a computational graph. A computational graph is a directed graph consisting of the following two parts:&nbsp;</p>
<ul class="calibre7">
<li class="calibre8">Building a computational graph</li>
<li class="calibre8">Running a computational graph</li>
</ul>
<p class="calibre2">A computational graph executes within a <strong class="calibre13">session</strong>. A TensorFlow session is a runtime environment for the computational graph. It allocates the CPU or GPU and maintains the state of the TensorFlow runtime. The following code creates a session instance named <kbd class="calibre12">sess</kbd> using <kbd class="calibre12">tf.Session</kbd>. Then the <kbd class="calibre12">sess.run()</kbd> function evaluates the tensor and returns the results stored in the <kbd class="calibre12">output</kbd>&nbsp;variable. It finally prints as <kbd class="calibre12">Hello World!</kbd>:</p>
<pre class="calibre20">with tf.Session() as sess:<br class="title-page-name" />    # Run the tf.constant operation in the session<br class="title-page-name" />    output = sess.run(hello_constant)<br class="title-page-name" />    print(output)</pre>
<p class="calibre2">Using TensorBoard, we can visualize the graph. To run TensorBoard, use the following command:</p>
<pre class="calibre20">tensorboard --logdir=path/to/log-directory</pre>
<p class="calibre2">Let's create a piece of simple addition code as follows. Create a constant integer <kbd class="calibre12">x</kbd> with value <kbd class="calibre12">5</kbd>, set the value of a new variable <kbd class="calibre12">y</kbd> after adding <kbd class="calibre12">5</kbd> to it, and print it:</p>
<pre class="calibre20">constant_x = tf.constant(5, name='constant_x')<br class="title-page-name" />variable_y = tf.Variable(x + 5, name='variable_y')<br class="title-page-name" />print (variable_y)</pre>
<p class="calibre2">The difference is that <kbd class="calibre12">variable_y</kbd> isn't given the current value of <kbd class="calibre12">x + 5</kbd> as it should in Python code. Instead, it is an equation; that means, when <kbd class="calibre12">variable_y</kbd> is computed, take the value of <kbd class="calibre12">x</kbd> at that point in time and add <kbd class="calibre12">5</kbd> to it. The computation of the value of <kbd class="calibre12">variable_y</kbd> is never actually performed in the preceding code. This piece of code actually belongs to the computational graph building section of a typical TensorFlow program.&nbsp;<span class="calibre10">After running this, you'll get something like <kbd class="calibre12">&lt;tensorflow.python.ops.variables.Variable object at 0x7f074bfd9ef0&gt;</kbd>&nbsp;and not the actual value of <kbd class="calibre12">variable_y</kbd> as <kbd class="calibre12">10</kbd>.&nbsp;To fix this, we have to execute the code section of the computational graph, which looks like this:<br class="calibre18" /></span></p>
<pre class="calibre20">#initialize all variables<br class="title-page-name" />init = tf.global_variables_initializer()<br class="title-page-name" /># All variables are now initialized<br class="title-page-name" /><br class="title-page-name" />with tf.Session() as sess:<br class="title-page-name" />    sess.run(init)<br class="title-page-name" />    print(sess.run(variable_y))</pre>
<p class="calibre2">Here is the execution of some basic math functions, such as addition, subtraction, multiplication, and division with tensors. For more math functions, please refer to the documentation:</p>
<p class="calibre2"><span class="calibre10">For TensorFlow math functions, go to&nbsp;<a href="https://www.tensorflow.org/versions/r0.12/api_docs/python/math_ops/basic_math_functions" target="_blank" class="calibre11">https://www.tensorflow.org/versions/r0.12/api_docs/python/math_ops/basic_math_functions</a>.</span></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-92">
        <section>

                            <header>
                    <h1 class="header-title">Basic math with TensorFlow</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre10">The&nbsp;</span><kbd class="calibre12">tf.add()</kbd><span class="calibre10">&nbsp;function takes two numbers, two tensors, or one of each, and it returns their sum as a tensor:</span></p>
<pre class="calibre20">Addition<br class="title-page-name" />x = tf.add(1, 2, name=None) # 3</pre>
<p class="calibre2">Here's an example with subtraction and multiplication:</p>
<pre class="calibre20">x = tf.subtract(1, 2,name=None) # -1<br class="title-page-name" />y = tf.multiply(2, 5,name=None) # 10<br class="title-page-name" /> </pre>
<p class="calibre2">What if we want to use a non-constant? How to feed an input dataset to TensorFlow? For this, TensorFlow provides an API, <kbd class="calibre12">tf.placeholder()</kbd>, and uses <kbd class="calibre12">feed_dict</kbd>.</p>
<p class="calibre2">A <kbd class="calibre12">placeholder</kbd> is a variable that data is assigned&nbsp;to later in the&nbsp;<kbd class="calibre12">tf.session.run()</kbd> function. With the help of this, our operations can be created and we can build our computational graph without needing the data. Afterwards, this data is fed into the graph through these placeholders with the help of the&nbsp;<kbd class="calibre12">feed_dict</kbd> parameter in <kbd class="calibre12">tf.session.run()</kbd> to set the <kbd class="calibre12">placeholder</kbd> tensor. In the following example, the tensor <kbd class="calibre12">x</kbd> is set to the string <kbd class="calibre12">Hello World</kbd><span class="calibre10">&nbsp;before the session runs:</span></p>
<pre class="calibre20">x = tf.placeholder(tf.string)<br class="title-page-name" /><br class="title-page-name" />with tf.Session() as sess:<br class="title-page-name" />    output = sess.run(x, feed_dict={x: 'Hello World'})</pre>
<p class="calibre2">It's also possible to set more than one tensor using <kbd class="calibre12">feed_dict</kbd>, as follows:</p>
<pre class="calibre20">x = tf.placeholder(tf.string)<br class="title-page-name" />y = tf.placeholder(tf.int32, None)<br class="title-page-name" />z = tf.placeholder(tf.float32, None)<br class="title-page-name" /><br class="title-page-name" />with tf.Session() as sess:<br class="title-page-name" />    output = sess.run(x, feed_dict={x: 'Welcome to CNN', y: 123, z: 123.45}) </pre>
<p class="calibre2">Placeholders can also allow storage of arrays with the help of multiple dimensions. Please see the following example:</p>
<pre class="calibre20">import tensorflow as tf<br class="title-page-name" /><br class="title-page-name" />x = tf.placeholder("float", [None, 3])<br class="title-page-name" />y = x * 2<br class="title-page-name" /><br class="title-page-name" />with tf.Session() as session:<br class="title-page-name" />    input_data = [[1, 2, 3],<br class="title-page-name" />                 [4, 5, 6],]<br class="title-page-name" />    result = session.run(y, feed_dict={x: input_data})<br class="title-page-name" />    print(result)</pre>
<div class="packt_tip">This will throw an error as <kbd class="calibre12">ValueError: invalid literal for...</kbd>&nbsp;in cases where the data passed to the <kbd class="calibre12">feed_dict</kbd>&nbsp;parameter doesn't match the tensor type and can't be cast into the tensor type.</div>
<p class="calibre2">The <kbd class="calibre12">tf.truncated_normal()</kbd> function returns a tensor with random values from a normal distribution. This is mostly used for weight initialization in a network:</p>
<pre class="calibre20">n_features = 5<br class="title-page-name" />n_labels = 2<br class="title-page-name" />weights = tf.truncated_normal((n_features, n_labels))<br class="title-page-name" />with tf.Session() as sess:<br class="title-page-name" />  print(sess.run(weights))</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-74">
        <section>

                            <header>
                    <h1 class="header-title">Softmax in TensorFlow</h1>
                </header>
            
            <article>
                
<p class="calibre2">The softmax function converts its inputs, known as <strong class="calibre13">logit</strong> or <strong class="calibre13">logit scores</strong>, to be between 0 and 1, and also normalizes the outputs so that they all sum up to 1. In other words, the softmax&nbsp;<span class="calibre10">function turns your logits into probabilities.</span>&nbsp;Mathematically,<span class="calibre10">&nbsp;the&nbsp;</span>softmax function is defined as follows:</p>
<div class="mce-root"><img class="fm-editor-equation2" src="images/000001.png" /></div>
<p class="calibre2">In TensorFlow, the softmax function is implemented. It&nbsp;<span class="calibre10">takes logits and returns softmax activations that have the same type and shape as input logits, as shown in the following image:</span></p>
<div class="mce-root"><img src="images/000086.png" class="calibre23" /></div>
<p class="calibre2">The following code is used to implement this:</p>
<pre class="calibre20">logit_data = [2.0, 1.0, 0.1]<br class="title-page-name" />logits = tf.placeholder(tf.float32)<br class="title-page-name" />softmax = tf.nn.softmax(logits)<br class="title-page-name" /><br class="title-page-name" />with tf.Session() as sess:<br class="title-page-name" />    output = sess.run(softmax, feed_dict={logits: logit_data})<br class="title-page-name" />    print( output )</pre>
<p class="calibre2">The way we represent labels mathematically is often called <strong class="calibre13">one-hot encoding</strong>. Each label is represented by a vector that has 1.0 for the correct label and 0.0 for everything else. This works well for most problem cases. However, when the problem has millions of labels, one-hot encoding is not efficient, since most of the vector elements are zeros. We measure the similarity distance between two probability vectors, known as <strong class="calibre13">cross-entropy</strong>&nbsp;and denoted by <strong class="calibre13">D</strong>.</p>
<div class="packt_tip">
<p class="calibre9">Cross-entropy is not symmetric. That means:&nbsp;<em class="calibre19">D(S,L) != D(L,S)</em></p>
</div>
<p class="calibre2"><span class="calibre10">In machine learning, we define what it means for a model to be bad usually by a mathematical function. This function is called <strong class="calibre13">loss</strong>, <strong class="calibre13">cost</strong>, or <strong class="calibre13">objective</strong> function. One very common function used to determine the loss of a model is called the&nbsp;<strong class="calibre13">cross-entropy loss</strong>. This concept came from information theory (for more on this, please refer to Visual Information Theory at <a href="https://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" class="calibre11">https://colah.github.io/posts/2015-09-Visual-Information/</a>). Intuitively, the loss will be high if the model does a poor job of classifying on the training data, and it will be low otherwise, as shown here:</span></p>
<div class="mce-root"><img src="images/000062.png" class="calibre24" /></div>
<div class="cdpaligncenter">Cross-entropy loss function</div>
<p class="calibre2">In TensorFlow, we can write a cross-entropy function using <kbd class="calibre12">tf.reduce_sum()</kbd>; it takes an array of numbers and returns its sum as a tensor (see the following code block):</p>
<pre class="calibre20">x = tf.constant([[1,1,1], [1,1,1]])<br class="title-page-name" />with tf.Session() as sess:<br class="title-page-name" />    print(sess.run(tf.reduce_sum([1,2,3]))) #returns 6 <br class="title-page-name" />    print(sess.run(tf.reduce_sum(x,0))) #sum along x axis, prints [2,2,2]</pre>
<p class="calibre2"><span class="calibre10">But in practice,&nbsp;</span>while computing the softmax function, intermediate terms may be very large due to the exponentials. So, dividing large numbers can be numerically unstable. We should use TensorFlow's provided softmax and cross-entropy loss API. The following code snippet manually calculates cross-entropy loss and also prints the same using the TensorFlow API:</p>
<pre class="calibre20">import tensorflow as tf<br class="title-page-name" /><br class="title-page-name" />softmax_data = [0.1,0.5,0.4]<br class="title-page-name" />onehot_data = [0.0,1.0,0.0]<br class="title-page-name" /><br class="title-page-name" />softmax = tf.placeholder(tf.float32)<br class="title-page-name" />onehot_encoding = tf.placeholder(tf.float32)<br class="title-page-name" /><br class="title-page-name" />cross_entropy = - tf.reduce_sum(tf.multiply(onehot_encoding,tf.log(softmax)))<br class="title-page-name" /><br class="title-page-name" />cross_entropy_loss = tf.nn.softmax_cross_entropy_with_logits(logits=tf.log(softmax), labels=onehot_encoding)<br class="title-page-name" /><br class="title-page-name" />with tf.Session() as session:<br class="title-page-name" />    print(session.run(cross_entropy,feed_dict={softmax:softmax_data, onehot_encoding:onehot_data} ))<br class="title-page-name" />    print(session.run(cross_entropy_loss,feed_dict={softmax:softmax_data, onehot_encoding:onehot_data} ))<br class="title-page-name" /><br class="title-page-name" /></pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-93">
        <section>

                            <header>
                    <h1 class="header-title">Introduction to the MNIST dataset&nbsp;</h1>
                </header>
            
            <article>
                
<p class="calibre2">Here we use <strong class="calibre13">MNIST</strong> (<strong class="calibre13">Modified National Institute of Standards and Technology</strong>), which consists of images of handwritten numbers and their labels. Since its release in 1999, this classic dataset is used for benchmarking classification algorithms.&nbsp;</p>
<p class="calibre2">The data files <kbd class="calibre12">train.csv</kbd> and <kbd class="calibre12">test.csv</kbd> consist of <span class="calibre10">hand-drawn digits, from 0 through 9 in the form of&nbsp;</span>gray-scale images. A digital image is a mathematical function of the form <em class="calibre19">f(x,y)=pixel</em> value. The images are two dimensional.</p>
<p class="calibre2">We can perform any mathematical function on the image. By computing the&nbsp;gradient on the image, we can measure how fast pixel values are changing and the direction in which they are changing. For image recognition, we convert the image into grayscale for simplicity and have one color channel. <strong class="calibre13">RGB</strong> representation of an image consists of three color channels, <strong class="calibre13">RED</strong>, <strong class="calibre13">BLUE</strong>, and <strong class="calibre13">GREEN</strong>. In the RGB color scheme, an image is a stack of three images RED, BLUE, and GREEN. In a grayscale color scheme, color is not important. Color images are computationally harder to analyze because they take more space in memory. Intensity, which is a measure of the lightness and darkness of an image, is very useful for recognizing objects. In some applications, for example, detecting lane lines in a self-driving car application, color is important because it has to distinguish yellow lanes and white lanes. A grayscale image does not provide enough information to distinguish between white and yellow lane lines.</p>
<p class="calibre2">Any grayscale image is interpreted by the computer as a matrix with one entry for each image pixel. Each image is 28 x 28 pixels in height and width, to give a sum of 784 pixels. Each pixel has a single pixel-value associated with it. This value indicates the lightness or darkness of that particular pixel. This pixel-value is an integer ranging from 0 to 255, where a value of zero means darkest and 255 is the whitest, and a gray pixel is between 0 and 255.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-94">
        <section>

                            <header>
                    <h1 class="header-title">The simplest artificial neural network</h1>
                </header>
            
            <article>
                
<p class="calibre2">The following image represents a simple two-layer neural network:</p>
<div class="mce-root"><img src="images/000041.png" class="calibre25" /></div>
<div class="cdpaligncenter">Simple two-layer neural net</div>
<p class="calibre2">The first layer is the <strong class="calibre13">input layer</strong> and the last layer is the <strong class="calibre13">output layer</strong>. The middle layer is the <strong class="calibre13">hidden layer</strong>. If there is more than one hidden layer, then such a network is a deep neural network.</p>
<p class="calibre2">The input and output of each neuron in the hidden layer is connected to each neuron in the next layer.&nbsp;There can be any number of neurons in each layer depending on the problem. Let us consider an example. The simple example which you may already know is the popular hand written digit recognition that detects a number, say 5. This network will accept an image of 5 and will output 1 or 0. A 1 is to indicate the image in fact is a 5 and 0 otherwise. Once the network is created, it has to be trained. We can initialize with random weights and then feed input samples known as the <strong class="calibre13">training dataset</strong>. For each input sample, we check the output, compute the error rate and then adjust the weights so that whenever it sees 5 it outputs 1 and for everything else it outputs a zero. This type of training is called <strong class="calibre13">supervised learning</strong> and the method of adjusting the weights is called <strong class="calibre13">backpropagation</strong>. When constructing artificial neural network models, one of the primary considerations is how to choose activation functions for hidden and output layers. The three most commonly used activation functions are the sigmoid function, hyperbolic tangent function, and&nbsp;<strong class="calibre13">Rectified Linear Unit</strong><span class="calibre10">&nbsp;(</span><strong class="calibre13">ReLU</strong>). The beauty of the sigmoid function is that its derivative is<span class="calibre10">&nbsp;</span><span class="calibre10">evaluated at<em class="calibre19">&nbsp;z</em></span><span class="calibre10">&nbsp;and is</span> <span class="calibre10">simply&nbsp;<em class="calibre19">z</em>&nbsp;</span><span class="calibre10">multiplied by 1-minus <em class="calibre19">z</em></span><span class="calibre10">. That means:</span></p>
<p class="mce-root1"><span class="calibre10"><em class="calibre19">&nbsp;dy/dx =Ï(x)(1âÏ(x))</em></span></p>
<p class="calibre2">This helps us to efficiently calculate gradients used in neural networks in a convenient manner. If the feed-forward activations of the logistic function for a given layer is kept in memory, the gradients for that particular layer can be evaluated with the help of simple multiplication and subtraction rather than implementing and re-evaluating the sigmoid function, since it requires extra exponentiation.&nbsp;<span class="calibre10">The following image shows us the&nbsp;</span>ReLU&nbsp;<span class="calibre10">activation function, which is zero when&nbsp;</span><em class="calibre19">x &lt; 0</em><span class="calibre10">&nbsp;and then linear with slope 1 when&nbsp;</span><em class="calibre19">x &gt; 0</em>:</p>
<div class="mce-root"><img src="images/00009.jpeg" class="calibre26" /></div>
<p class="calibre2"><span class="calibre10">The ReLU&nbsp;is a nonlinear&nbsp;function that computes the function&nbsp;</span><em class="calibre19"><span class="calibre10"><span class="calibre10"><span class="calibre10"><span class="calibre10">f</span><span class="calibre10">(</span><span class="calibre10">x</span><span class="calibre10">)</span><span class="calibre10">=</span><span class="calibre10">max</span><span class="calibre10">(</span><span class="calibre10">0</span><span class="calibre10">,&nbsp;</span><span class="calibre10">x</span><span class="calibre10">)</span></span></span></span></em><span class="calibre10">. That means a ReLU function is 0 for negative inputs and <em class="calibre19">x</em> for all inputs <em class="calibre19">x &gt;0</em>. This means that the activation is thresholded at zero (see the preceding image on the left). TensorFlow implements the ReLU function in <kbd class="calibre12">tf.nn.relu()</kbd>:</span></p>
<div class="mce-root"><img class="fm-editor-equation3" src="images/000092.png" /></div>
<div class="packt_quote">Backpropagation, an abbreviation for "backward propagation of errors", is a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. The method calculates the gradient of a loss function with respect to all the weights in the network. The optimization method is fed with the gradient and uses it to get the weights updated to reduce the loss function.</div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-95">
        <section>

                            <header>
                    <h1 class="header-title">Building a single-layer neural network with TensorFlow</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let us build a single-layer neural net with TensorFlow step by step.&nbsp;In this example, we'll be using the MNIST dataset. This dataset is a set of 28 x 28 pixel grayscale images of hand written digits. This dataset consists of 55,000 training data, 10,000 test data, and 5,000 validation data. Every MNIST data point has two parts: an image of a handwritten digit and a corresponding label. The following code block loads data.&nbsp;<kbd class="calibre12">one_hot=True</kbd> means that the labels are one-hot encoded vectors instead of actual digits of the label. For example, if the label is <kbd class="calibre12">2</kbd>, you will see [0,0,1,0,0,0,0,0,0,0]. This allows us to directly use it in the output layer of the network:</p>
<pre class="calibre20">from tensorflow.examples.tutorials.mnist import input_data<br class="title-page-name" />mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)</pre>
<p class="calibre2"><span class="calibre10">Setting up placeholders and variables is done as follows:</span></p>
<pre class="calibre20"><span># All the pixels in the image (28 * 28 = 784)</span>
<span>features_count</span> <span>=</span> <span>784</span>
<span># there are 10 digits i.e labels</span>
<span>labels_count</span> <span>=</span> <span>10</span>
<span>batch_size</span> <span>=</span> <span>128</span>
<span>epochs</span> <span>=</span> <span>10</span>
<span>learning_rate</span> <span>=</span> <span>0.5</span>

<span>features</span> <span>=</span> <span>tf</span><span>.</span><span>placeholder</span><span>(</span><span>tf</span><span>.</span><span>float32</span><span>,</span> <span>[</span><span>None</span><span>,</span><span>features_count</span><span>])</span>
<span>labels</span> <span>=</span> <span>tf</span><span>.</span><span>placeholder</span><span>(</span><span>tf</span><span>.</span><span>float32</span><span>,</span> <span>[</span><span>None</span><span>,</span> <span>labels_count</span><span>])</span>

<span>#Set the weights and biases tensors</span>
<span>weights</span> <span>=</span> <span>tf</span><span>.</span><span>Variable</span><span>(</span><span>tf</span><span>.</span><span>truncated_normal</span><span>((</span><span>features_count</span><span>,</span> <span>labels_count</span><span>)))</span>
<span>biases</span> <span>=</span> <span>tf</span><span>.</span><span>Variable</span><span>(</span><span>tf</span><span>.</span><span>zeros</span><span>(</span><span>labels_count</span><span>),</span><span>name</span><span>=</span><span>'biases'</span><span>)</span></pre>
<p class="calibre2"><span class="calibre10">Let's set up the optimizer&nbsp;in TensorFlow:</span></p>
<pre class="calibre20"><span>loss,<br class="title-page-name" />optimizer</span> <span>=</span> <span>tf</span><span>.</span><span>train</span><span>.</span><span>GradientDescentOptimizer</span><span>(</span><span>learning_rate</span><span>)</span><span>.</span><span>minimize</span><span>(</span><span>loss</span><span>)</span>    </pre>
<p class="calibre2">Before we begin training, let's set up the variable initialization operation and an operation to measure the accuracy of our predictions, as follows:</p>
<pre class="calibre20"><span># Linear Function WX + b</span>
<span>logits</span> <span>=</span> <span>tf</span><span>.</span><span>add</span><span>(</span><span>tf</span><span>.</span><span>matmul</span><span>(</span><span>features</span><span>,</span> <span>weights</span><span>),</span><span>biases</span><span>)</span>

<span>prediction</span> <span>=</span> <span>tf</span><span>.</span><span>nn</span><span>.</span><span>softmax</span><span>(</span><span>logits</span><span>)</span>

<span># Cross entropy</span>
<span>cross_entropy</span> <span>=</span> <span>-</span><span>tf</span><span>.</span><span>reduce_sum</span><span>(</span><span>labels</span> <span>*</span> <span>tf</span><span>.</span><span>log</span><span>(</span><span>prediction</span><span>),</span> <span>reduction_indices</span><span>=</span><span>1</span><span>)</span>

<span># Training loss</span>
<span>loss</span> <span>=</span> <span>tf</span><span>.</span><span>reduce_mean</span><span>(</span><span>cross_entropy</span><span>)</span>

<span># Initializing all variables</span>
<span>init</span> <span>=</span> <span>tf</span><span>.</span><span>global_variables_initializer</span><span>()</span>

<span># Determining if the predictions are accurate</span>
<span>is_correct_prediction</span> <span>=</span> <span>tf</span><span>.</span><span>equal</span><span>(</span><span>tf</span><span>.</span><span>argmax</span><span>(</span><span>prediction</span><span>,</span> <span>1</span><span>),</span> <span>tf</span><span>.</span><span>argmax</span><span>(</span><span>labels</span><span>,</span> <span>1</span><span>))</span>
<span># Calculating prediction accuracy</span>
<span>accuracy</span> <span>=</span> <span>tf</span><span>.</span><span>reduce_mean</span><span>(</span><span>tf</span><span>.</span><span>cast</span><span>(</span><span>is_correct_prediction</span><span>,</span> <span>tf</span><span>.</span><span>float32</span><span>))</span></pre>
<p class="calibre2">Now we can begin training the model, as shown in the following code snippet:</p>
<pre class="calibre20"><span>#Beginning the session</span>
<span>with</span> <span>tf</span><span>.</span><span>Session</span><span>()</span> <span>as</span> <span>sess</span><span>:</span>
   <span># initializing all the variables</span>
   <span>sess</span><span>.</span><span>run</span><span>(</span><span>init</span><span>)</span>
   <span>total_batch</span> <span>=</span> <span>int</span><span>(</span><span>len</span><span>(</span><span>mnist</span><span>.</span><span>train</span><span>.</span><span>labels</span><span>)</span> <span>/</span> <span>batch_size</span><span>)</span>
   <span>for</span> <span>epoch</span> <span>in</span> <span>range</span><span>(</span><span>epochs</span><span>):</span>
        <span>avg_cost</span> <span>=</span> <span>0</span>
        <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>total_batch</span><span>):</span>
            <span>batch_x</span><span>,</span> <span>batch_y</span> <span>=</span> <span>mnist</span><span>.</span><span>train</span><span>.</span><span>next_batch</span><span>(</span><span>batch_size</span><span>=</span><span>batch_size</span><span>)</span>
            <span>_</span><span>,</span> <span>c</span> <span>=</span> <span>sess</span><span>.</span><span>run</span><span>([</span><span>optimizer</span><span>,</span><span>loss</span><span>],</span> <span>feed_dict</span><span>=</span><span>{</span><span>features</span><span>:</span> <span>batch_x</span><span>,</span> <span>labels</span><span>:</span> <span>batch_y</span><span>})</span>
            <span>avg_cost</span> <span>+=</span> <span>c</span> <span>/</span> <span>total_batch</span>
        <span>print</span><span>(</span><span>"Epoch:"</span><span>,</span> <span>(</span><span>epoch</span> <span>+</span> <span>1</span><span>),</span> <span>"cost ="</span><span>,</span> <span>"</span><span>{:.3f}</span><span>"</span><span>.</span><span>format</span><span>(</span><span>avg_cost</span><span>))</span>
   <span>print</span><span>(</span><span>sess</span><span>.</span><span>run</span><span>(</span><span>accuracy</span><span>,</span> <span>feed_dict</span><span>=</span><span>{</span><span>features</span><span>:</span> <span>mnist</span><span>.</span><span>test</span><span>.</span><span>images</span><span>,</span> <span>labels</span><span>:</span> <span>mnist</span><span>.</span><span>test</span><span>.</span><span>labels</span><span>}))</span></pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-20">
        <section>

                            <header>
                    <h1 class="header-title">Keras deep learning library overview</h1>
                </header>
            
            <article>
                
<p class="calibre2">Keras is a high-level deep neural networks API in Python that runs on top of TensorFlow, CNTK, or Theano.</p>
<p class="calibre2"><span class="calibre10">Here are some core concepts you need to know for working with Keras. TensorFlow is a deep learning library for numerical computation and machine intelligence. It is open source and uses data flow graphs for numerical computation. Mathematical operations are represented by nodes and multidimensional data arrays; that is, tensors are represented by graph edges.</span> <span class="calibre10">This framework is extremely technical and hence it is probably difficult for data analysts. Keras makes deep neural network coding simple. It also runs seamlessly on CPU and GPU machines.</span></p>
<p class="calibre2"><span class="calibre10">A <strong class="calibre13">model</strong> is the<strong class="calibre13">&nbsp;</strong>core data structure of Keras</span>. The sequential model, which consists of a linear stack of layers, is the simplest type of model.&nbsp;<span class="calibre10">It provides common functions, such as&nbsp;<kbd class="calibre12">fit()</kbd>, <kbd class="calibre12">evaluate()</kbd>, and <kbd class="calibre12">compile()</kbd>.</span></p>
<p class="calibre2">You can create a sequential model with the help of the following lines of code:<strong class="calibre13"><br class="calibre18" /></strong></p>
<pre class="calibre20">from keras.models import Sequential

#<span><span>Creating</span> the <span>Sequential</span> <span>model</span>
<span>model</span> = <span>Sequential</span>()</span></pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-96">
        <section>

                            <header>
                    <h1 class="header-title">Layers in the Keras model</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre10">A Keras layer is just like a neural network layer. There are fully connected layers, max pool layers, and activation layers.&nbsp;</span><span class="calibre10">A layer can be added to the model using the model's&nbsp;<kbd class="calibre12">add()</kbd> function. For example, a simple model can be represented by the following:</span></p>
<pre class="calibre20">from keras.models import Sequential
from keras.layers.core import Dense, Activation, Flatten

#<span><span>Creating</span> the <span>Sequential</span> <span>model</span>
<span>model</span> = <span>Sequential</span>()

#Layer 1 - <span>Adding</span> a flatten layer
<span>model</span>.<span>add</span>(Flatten(input_shape=(<span>32</span>, <span>32</span>, <span>3</span>)))

#Layer 2 - <span>Adding</span> a fully connected layer
<span>model</span>.<span>add</span>(Dense(<span>100</span>))

#Layer 3 - <span>Adding</span> a ReLU activation layer
<span>model</span>.<span>add</span>(Activation(<span>'relu'</span>))

#Layer 4- <span>Adding</span> a fully connected layer
<span>model</span>.<span>add</span>(Dense(<span>60</span>))

#Layer 5 - <span>Adding</span> an ReLU activation layer
<span>model</span>.<span>add</span>(Activation(<span>'relu'</span>))</span></pre>
<p class="calibre2"><span class="calibre10">Keras will automatically infer the shape of all layers after the first layer. This means you only have to set the input dimensions for the first layer.&nbsp;The first layer from the preceding code snippet, <kbd class="calibre12">model.add(Flatten(input_shape=(32, 32, 3)))</kbd>, sets the input dimension to (32, 32, 3) and the output dimension to (3072=32 x 32 x 3). The second layer takes in the output of the first layer and sets the output dimensions to (100). This chain of passing the output to the next layer continues until the last layer, which is the output of the model.</span></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-23">
        <section>

                            <header>
                    <h1 class="header-title">Handwritten number recognition with Keras and MNIST</h1>
                </header>
            
            <article>
                
<p class="calibre2">A typical neural network for a digit recognizer may have 784 input pixels connected to 1,000 neurons in the hidden layer, which in turn connects to 10 output targets &mdash; one for each digit. Each layer is fully connected to the layer above. A graphical representation of this network is shown as follows, where <kbd class="calibre12">x</kbd> are the inputs, <kbd class="calibre12">h</kbd> are the hidden neurons, and <kbd class="calibre12">y</kbd> are the output class variables:</p>
<div class="mce-root"><img src="images/000068.png" class="calibre26" /></div>
<p class="calibre2">In this notebook, we will build a neural network that will recognize handwritten numbers from 0-9.</p>
<p class="calibre2">The type of neural network that we are building is used in a number of real-world applications, such as recognizing phone numbers and sorting postal mail by address. To build this network, we will use the<span class="calibre10">&nbsp;</span><strong class="calibre13">MNIST</strong><span class="calibre10">&nbsp;</span>dataset.</p>
<p class="calibre2">We will begin as shown in the following code by importing all the required modules, after which the data will be loaded, and then finally building the network:</p>
<pre class="calibre20"><span># Import Numpy, keras and MNIST data</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>

<span>from</span> <span>keras.datasets</span> <span>import</span> <span>mnist</span>
<span>from</span> <span>keras.models</span> <span>import</span> <span>Sequential</span>
<span>from</span> <span>keras.layers.core</span> <span>import</span> <span>Dense</span><span>,</span> <span>Dropout</span><span>,</span> <span>Activation</span>
<span>from</span> <span>keras.utils</span> <span>import</span> <span>np_utils</span></pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-97">
        <section>

                            <header>
                    <h1 class="header-title">Retrieving training and test data</h1>
                </header>
            
            <article>
                
<p class="calibre2">The MNIST dataset already comprises both training and test data. There are 60,000 data points of training data and 10,000 points of test data. If you do not have the data file locally at the&nbsp;<kbd class="calibre12">'~/.keras/datasets/' +</kbd> path, it can be downloaded at this location.</p>
<p class="calibre2">Each MNIST data point has:</p>
<ul class="calibre7">
<li class="calibre8">An image of a handwritten digit</li>
<li class="calibre8">A corresponding label that is a number from 0-9 to help identify the image</li>
</ul>
<p class="calibre2">The images will be called, and will be the input to our neural network,<span class="calibre10">&nbsp;</span><strong class="calibre13">X</strong>; their corresponding labels<span class="calibre10">&nbsp;are&nbsp;</span><strong class="calibre13">y</strong>.</p>
<p class="calibre2">We want our labels as<span class="calibre10">&nbsp;</span>one-hot vectors. One-hot vectors are vectors of many zeros and one. It's easiest to see this in an example. The number 0 is represented as [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], and 4 is represented as [0, 0, 0, 0, 1, 0, 0, 0, 0, 0] as a one-hot vector.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-27">
        <section>

                            <header>
                    <h1 class="header-title">Flattened data</h1>
                </header>
            
            <article>
                
<p class="calibre2">We will use flattened data in this example, or a representation of MNIST images in one dimension rather than two can also be used. Thus, each 28 x 28 pixels number image will be represented as a 784 pixel 1 dimensional array.</p>
<p class="calibre2">By flattening the data, information about the 2D structure of the image is thrown; however, our data is simplified. With the help of this, all our training data can be contained in one array of shape&nbsp;<span class="calibre10">(60,000, 784), wherein the first dimension represents the number of training images and the second depicts the number of pixels in each image. This kind of data&nbsp;is easy to analyze using a simple neural network, as follows:</span></p>
<pre class="calibre20"><span># Retrieving the training and test data</span>
<span>(</span><span>X_train</span><span>,</span> <span>y_train</span><span>),</span> <span>(</span><span>X_test</span><span>,</span> <span>y_test</span><span>)</span> <span>=</span> <span>mnist</span><span>.</span><span>load_data</span><span>()</span>


<span>print</span><span>(</span><span>'X_train shape:'</span><span>,</span> <span>X_train</span><span>.</span><span>shape</span><span>)</span>
<span>print</span><span>(</span><span>'X_test shape: '</span><span>,</span> <span>X_test</span><span>.</span><span>shape</span><span>)</span>
<span>print</span><span>(</span><span>'y_train shape:'</span><span>,</span><span>y_train</span><span>.</span><span>shape</span><span>)</span>
<span>print</span><span>(</span><span>'y_test shape: '</span><span>,</span> <span>y_test</span><span>.</span><span>shape</span><span>)</span></pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-98">
        <section>

                            <header>
                    <h1 class="header-title">Visualizing the training data</h1>
                </header>
            
            <article>
                
<p class="calibre2">The following function will help you visualize the MNIST data. By passing in the index of a training example, the <kbd class="calibre12">show_digit</kbd><span class="calibre10">&nbsp;</span><span class="calibre10">function</span><span class="calibre10">&nbsp;</span><span class="calibre10">will display that training image along with its corresponding label in the title:</span></p>
<pre class="calibre20"><span># Visualize the data</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>%</span><span>matplotlib</span> <span>inline</span>

<span>#Displaying a training image by its index in the MNIST set</span>
<span>def</span> <span>display_digit</span><span>(</span><span>index</span><span>):</span>
    <span>label</span> <span>=</span> <span>y_train</span><span>[</span><span>index</span><span>]</span><span>.</span><span>argmax</span><span>(</span><span>axis</span><span>=</span><span>0</span><span>)</span>
    <span>image</span> <span>=</span> <span>X_train</span><span>[</span><span>index</span><span>]</span>
    <span>plt</span><span>.</span><span>title</span><span>(</span><span>'Training data, index: </span><span>%d</span><span>,  Label: </span><span>%d</span><span>'</span> <span>%</span> <span>(</span><span>index</span><span>,</span> <span>label</span><span>))</span>
    <span>plt</span><span>.</span><span>imshow</span><span>(</span><span>image</span><span>,</span> <span>cmap</span><span>=</span><span>'gray_r'</span><span>)</span>
    <span>plt</span><span>.</span><span>show</span><span>()</span>
    
<span># Displaying the first (index 0) training image</span>
<span>display_digit</span><span>(0</span><span>)</span></pre>
<pre class="calibre20"><span>X_train</span> <span>=</span> <span>X_train</span><span>.</span><span>reshape</span><span>(</span><span>60000</span><span>,</span> <span>784</span><span>)</span>
<span>X_test</span> <span>=</span> <span>X_test</span><span>.</span><span>reshape</span><span>(</span><span>10000</span><span>,</span> <span>784</span><span>)</span>
<span>X_train</span> <span>=</span> <span>X_train</span><span>.</span><span>astype</span><span>(</span><span>'float32'</span><span>)</span>
<span>X_test</span> <span>=</span> <span>X_test</span><span>.</span><span>astype</span><span>(</span><span>'float32'</span><span>)</span>
<span>X_train</span> <span>/=</span> <span>255</span>
<span>X_test</span> <span>/=</span> <span>255</span>
<span>print</span><span>(</span><span>"Train the matrix shape"</span><span>,</span> <span>X_train</span><span>.</span><span>shape</span><span>)</span>
<span>print</span><span>(</span><span>"Test the matrix shape"</span><span>,</span> <span>X_test</span><span>.</span><span>shape</span><span>)<br class="title-page-name" /><br class="title-page-name" /></span></pre>
<pre class="calibre20"><span>#One Hot encoding of labels.</span>
<span>from</span> <span>keras.utils.np_utils</span> <span>import</span> <span>to_categorical</span>
<span>print</span><span>(</span><span>y_train</span><span>.</span><span>shape</span><span>)</span>
<span>y_train</span> <span>=</span> <span>to_categorical</span><span>(</span><span>y_train</span><span>,</span> <span>10</span><span>)</span>
<span>y_test</span> <span>=</span> <span>to_categorical</span><span>(</span><span>y_test</span><span>,</span> <span>10</span><span>)</span>
<span>print</span><span>(</span><span>y_train</span><span>.</span><span>shape</span><span>)</span></pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-32">
        <section>

                            <header>
                    <h1 class="header-title">Building the network</h1>
                </header>
            
            <article>
                
<p class="calibre2">For this example, you'll define the following:</p>
<ul class="calibre7">
<li class="calibre8">The input layer, which you&nbsp;<span>should expect for each piece of MNIST data</span>, as it tells the network the number of inputs</li>
<li class="calibre8">Hidden layers, as they recognize patterns in data and also connect the input layer to the output layer</li>
<li class="calibre8">The output layer, as it defines how the network learns and gives a label as the output for a given image, as follows:</li>
</ul>
<pre class="calibre20"><span># Defining the neural network</span>
<span>def</span> <span>build_model</span><span>():</span>
    <span>model</span> <span>=</span> <span>Sequential</span><span>()</span>
    <span>model</span><span>.</span><span>add</span><span>(</span><span>Dense</span><span>(</span><span>512</span><span>,</span> <span>input_shape</span><span>=</span><span>(</span><span>784</span><span>,)))</span>
    <span>model</span><span>.</span><span>add</span><span>(</span><span>Activation</span><span>(</span><span>'relu'</span><span>))</span> <span># An "activation" is just a non-linear function that is applied to the output</span>
 <span># of the above layer. In this case, with a "rectified linear unit",</span>
 <span># we perform clamping on all values below 0 to 0.</span>
                           
    <span>model</span><span>.</span><span>add</span><span>(</span><span>Dropout</span><span>(</span><span>0.2</span><span>))</span>   <span>#With the help of Dropout helps we can protect the model from memorizing or "overfitting" the training data</span>
    <span>model</span><span>.</span><span>add</span><span>(</span><span>Dense</span><span>(</span><span>512</span><span>))</span>
    <span>model</span><span>.</span><span>add</span><span>(</span><span>Activation</span><span>(</span><span>'relu'</span><span>))</span>
    <span>model</span><span>.</span><span>add</span><span>(</span><span>Dropout</span><span>(</span><span>0.2</span><span>))</span>
    <span>model</span><span>.</span><span>add</span><span>(</span><span>Dense</span><span>(</span><span>10</span><span>))</span>
    <span>model</span><span>.</span><span>add</span><span>(</span><span>Activation</span><span>(</span><span>'softmax'</span><span>))</span> <span># This special "softmax" activation,</span>
    <span>#It also ensures that the output is a valid probability distribution,</span>
    <span>#Meaning that values obtained are all non-negative and sum up to 1.</span>
    <span>return</span> <span>model<br class="title-page-name" /></span></pre>
<pre class="calibre20"><span>#Building the model</span>
<span>model</span> <span>=</span> <span>build_model</span><span>()</span></pre>
<pre class="calibre20"><span>model</span><span>.</span><span>compile</span><span>(</span><span>optimizer</span><span>=</span><span>'rmsprop'</span><span>,</span>
          <span>loss</span><span>=</span><span>'categorical_crossentropy'</span><span>,</span>
          <span>metrics</span><span>=</span><span>[</span><span>'accuracy'</span><span>])</span></pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-99">
        <section>

                            <header>
                    <h1 class="header-title">Training the network</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now that we've constructed the network, we feed it with data and train it, as follows:</p>
<pre class="calibre20"><span># Training</span>
<span>model</span><span>.</span><span>fit</span><span>(</span><span>X_train</span><span>,</span> <span>y_train</span><span>,</span> <span>batch_size</span><span>=</span><span>128</span><span>,</span> <span>nb_epoch</span><span>=</span><span>4</span><span>,</span> <span>verbose</span><span>=</span><span>1</span><span>,</span><span>validation_data</span><span>=</span><span>(</span><span>X_test</span><span>,</span> <span>y_test</span><span>))</span></pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-34">
        <section>

                            <header>
                    <h1 class="header-title">Testing</h1>
                </header>
            
            <article>
                
<p class="calibre2">After you're satisfied with the training output and accuracy, you can run the network on the<span class="calibre10">&nbsp;</span><strong class="calibre13">test dataset</strong><span class="calibre10">&nbsp;</span>to measure its performance!</p>
<div class="packt_tip">Keep in mind to perform this&nbsp;<span>only</span><span>&nbsp;</span><span>after you've completed the training and are satisfied with the results.</span></div>
<p class="calibre2">A good result will obtain<span class="calibre10">&nbsp;an accuracy&nbsp;</span><strong class="calibre13">higher than 95%</strong>. Some simple models have been known to achieve even up to 99.7% accuracy! We can test the model, as shown here:</p>
<pre class="calibre20"><span># Comparing the labels predicted by our model with the actual labels</span>

<span>score</span> <span>=</span> <span>model</span><span>.</span><span>evaluate</span><span>(</span><span>X_test</span><span>,</span> <span>y_test</span><span>,</span> <span>batch_size</span><span>=</span><span>32</span><span>,</span> <span>verbose</span><span>=</span><span>1</span><span>,</span><span>sample_weight</span><span>=</span><span>None</span><span>)</span>
<span># Printing the result</span>
<span>print</span><span>(</span><span>'Test score:'</span><span>,</span> <span>score</span><span>[</span><span>0</span><span>])</span>
<span>print</span><span>(</span><span>'Test accuracy:'</span><span>,</span> <span>score</span><span>[</span><span>1</span><span>])</span></pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-100">
        <section>

                            <header>
                    <h1 class="header-title">Understanding backpropagation&nbsp;</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we will understand an intuition about backpropagation. This is a way of computing gradients using the chain rule. Understanding this process and its subtleties is critical for you to be able to understand and effectively develop, design, and debug neural networks.</p>
<p class="calibre2">In general, given a function <em class="calibre19">f(x)</em>, where <em class="calibre19">x</em> is a vector of inputs, we want to compute the gradient of <em class="calibre19">f</em> at <em class="calibre19">x</em> denoted by<span class="calibre10">&nbsp;</span><em class="calibre19">â(f(x))</em>. This is because in the case of neural networks, the function <em class="calibre19">f</em> is basically a loss function (<em class="calibre19">L</em>) and the input <em class="calibre19">x</em> is the combination of weights and training data. The symbol<em class="calibre19">&nbsp;</em><span class="calibre10"><em class="calibre19">â</em>&nbsp; is pronounced&nbsp;as <strong class="calibre13">nabla</strong>:</span></p>
<p class="mce-root1"><em class="calibre19">(xi, yi ) i = 1......N</em></p>
<p class="calibre2">Why do we take the gradient on weight parameters?</p>
<p class="calibre2">It is given that the training data is usually fixed and the parameters are variables that we have control over. We usually compute the gradient of the parameters so that we can use it for parameter updates. The gradient&nbsp;<span class="calibre10"><em class="calibre19">âf</em> is the vector of partial derivatives, that is:</span></p>
<p class="mce-root1"><em class="calibre19">âf = [ df/dx, df/dy] = [y,x]</em></p>
<p class="calibre2">In a nutshell, backpropagation will consist of:</p>
<ul class="calibre7">
<li class="calibre8">Doing a feed-forward operation</li>
<li class="calibre8">Comparing the output of the model with the desired output</li>
<li class="calibre8">Calculating the error</li>
<li class="calibre8">Running the feedforward operation backwards (backpropagation) to spread the error to each of the weights</li>
<li class="calibre8">Using this to update the weights, and get a better model</li>
<li class="calibre8">Continuing this until we have a model that is good</li>
</ul>
<p class="calibre2">We will be building a neural network that recognizes digits from 0 to 9. This kind of network application is used for sorting postal mail by zip code, recognizing phone numbers and house numbers from images, extracting package quantities from image of the package and so on.</p>
<p class="calibre2">In most cases, backpropagation is implemented in a framework, such as TensorFlow. However, it is not always true that by simply adding an arbitrary number of hidden layers, backpropagation will magically work on the dataset. The fact is if the weight initialization is sloppy, these non linearity functions can saturate and stop learning. That means training loss will be flat and refuse to go down. This is known as the&nbsp;<strong class="calibre13">vanishing gradient problem</strong>.&nbsp;</p>
<p class="calibre2">If your weight<span class="calibre10">&nbsp;</span>matrix<span class="calibre10">&nbsp;</span><em class="calibre19">W</em><span class="calibre10">&nbsp;</span>is initialized too large, the output of the matrix multiply too could probably have a very large range, which in turn will make all the outputs in the vector<em class="calibre19">&nbsp;z</em><span class="calibre10">&nbsp;</span>almost binary: either 1 or 0. However, if this is the case, then<span class="calibre10">,&nbsp;</span><em class="calibre19">z*(1-z)</em>, which is the local gradient of the sigmoid non-linearity, will become<span class="calibre10">&nbsp;</span><em class="calibre19">zero&nbsp;</em>(vanish)<span class="calibre10">&nbsp;</span><span class="calibre10">in both cases</span>,<strong class="calibre13">&nbsp;</strong>which will<strong class="calibre13">&nbsp;</strong><span class="calibre10">make the gradient for both</span><span class="calibre10">&nbsp;</span><em class="calibre19">x</em><span class="calibre10">&nbsp;</span><span class="calibre10">and</span><strong class="calibre13">&nbsp;</strong><em class="calibre19">W</em><span class="calibre10">&nbsp;also&nbsp;</span><span class="calibre10">zero. The rest of the backward pass will also come out all zero from this point onward on account of the multiplication in the chain rule.</span></p>
<p class="calibre2"><span class="calibre10">Another nonlinear activation function is ReLU, which thresholds neurons at zero shown as follows. The forward and backward pass for a fully connected layer that uses ReLU would at the core include:</span></p>
<pre class="calibre20">z = np.maximum(0, np.dot(W, x)) #Representing forward pass<br class="title-page-name" />dW = np.outer(z &gt; 0, x) #Representing backward pass: local gradient for W</pre>
<p class="calibre2">If you observe this for a while, you'll see that should a neuron get clamped to zero in the forward pass (that is,<span class="calibre10">&nbsp;</span><em class="calibre19">z = 0</em>, it doesn't fire), then its weights will get a zero gradient. This can lead to what is called the <strong class="calibre13">dead ReLU</strong> problem. This means if a ReLU neuron is unfortunately initialized in such a way that it never fires, or if a neuron's weights ever get knocked off with a large update during training into this regime, in such cases this neuron will remain permanently dead. It is similar to permanent, irrecoverable brain damage. Sometimes, you can even forward the entire training set through a trained network and finally realize that a large fraction (about 40%) of your neurons were zero the entire time.</p>
<p class="calibre2">In<span class="calibre10">&nbsp;</span>calculus, the<span class="calibre10">&nbsp;</span>chain rule<span class="calibre10">&nbsp;</span>is<span class="calibre10">&nbsp;used&nbsp;</span>for computing the<span class="calibre10">&nbsp;</span>derivative<span class="calibre10">&nbsp;</span>of the<span class="calibre10">&nbsp;</span>composition<span class="calibre10">&nbsp;</span>of two or more<span class="calibre10">&nbsp;</span>functions. That is, if we have two functions as&nbsp;<em class="calibre19">f&nbsp;</em>and<span class="calibre10">&nbsp;</span><em class="calibre19">g</em>, then the chain rule represents the derivative of their composition<span class="calibre10">&nbsp;</span><em class="calibre19"><span class="calibre10">f&nbsp;â&nbsp;g.</span></em><span class="calibre10">&nbsp;T</span>he function that maps<span class="calibre10">&nbsp;</span><em class="calibre19">x</em><span class="calibre10">&nbsp;</span>to<span class="calibre10">&nbsp;</span><em class="calibre19">f(g(x))</em>) in terms of the derivatives of<span class="calibre10">&nbsp;</span><em class="calibre19">f</em><span class="calibre10">&nbsp;</span>and<span class="calibre10">&nbsp;</span><em class="calibre19">g</em><span class="calibre10">&nbsp;</span>and the<span class="calibre10">&nbsp;</span>product of functions<span class="calibre10">&nbsp;is expressed&nbsp;</span>as follows:</p>
<div class="mce-root"><img class="fm-editor-equation4" src="images/000040.png" /></div>
<p class="calibre2">There is a more explicit way to represent this in terms of the variable. Let<span class="calibre10">&nbsp;</span><em class="calibre19"><span class="calibre10">F&nbsp;=&nbsp;f&nbsp;â&nbsp;g</span></em>, or equivalently,<span class="calibre10">&nbsp;</span><em class="calibre19"><span class="calibre10">F(x) =&nbsp;f(g(x))</span></em><span class="calibre10">&nbsp;</span>for all<span class="calibre10">&nbsp;</span><em class="calibre19">x</em>. Then one can also write:</p>
<p class="mce-root1"><em class="calibre19">F'(x)=f'(g(x))g'(x).</em></p>
<p class="calibre2">The chain rule can be written with the help of<span class="calibre10">&nbsp;</span>Leibniz's notation&nbsp;in the following way. If a variable<span class="calibre10">&nbsp;</span><em class="calibre19">z</em><span class="calibre10">&nbsp;</span>is dependent on a&nbsp;<span class="calibre10">variable</span><em class="calibre19">&nbsp;y</em>, which in turn is dependent on a variable<span class="calibre10">&nbsp;</span><em class="calibre19">x</em> (such that<span class="calibre10">&nbsp;</span><em class="calibre19">y</em><span class="calibre10">&nbsp;</span>and<span class="calibre10">&nbsp;</span><em class="calibre19">z</em><span class="calibre10">&nbsp;</span>are dependent variables), then<span class="calibre10">&nbsp;</span><em class="calibre19">z</em>&nbsp;depends on<span class="calibre10">&nbsp;</span><em class="calibre19">x</em><span class="calibre10">&nbsp;</span>as well <span class="calibre10">via the intermediate</span> <em class="calibre19">y</em>. The chain rule then states:</p>
<div class="mce-root"><img class="fm-editor-equation5" src="images/000019.png" /></div>
<p class="calibre2"><em class="calibre19">z = 1/(1 + np.exp(-np.dot(W, x)))</em> # forward pass<br class="calibre18" />
<em class="calibre19">dx = np.dot(W.T, z*(1-z))</em> # backward pass: local gradient for <em class="calibre19">x</em><br class="calibre18" />
<em class="calibre19">dW = np.outer(z*(1-z), x)</em> # backward pass: local gradient for <em class="calibre19">W</em></p>
<p class="calibre2">The forward pass on the left in the following figure calculates <em class="calibre19">z</em> as a function <em class="calibre19">f(x,y)</em>&nbsp;using the input variables&nbsp;<em class="calibre19">x</em>&nbsp;and <em class="calibre19">y</em>. The right side of the figures represents the backward pass. Receiving <em class="calibre19">dL/dz</em>, the gradient of the loss function with respect to <em class="calibre19">z</em>, the gradients of <em class="calibre19">x</em>&nbsp;and <em class="calibre19">y</em>&nbsp;on the loss function can be calculated by applying the chain rule, as shown in the following figure:</p>
<div class="mce-root"><img src="images/000108.png" class="calibre27" /></div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-101">
        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we laid the foundation of neural networks and walked through the simplest artificial neural network. We learned how to build a single layer neural network using TensorFlow.</p>
<p class="calibre2">We studied the differences in the layers in the Keras model and demonstrated the famous handwritten number recognition with Keras and MNIST.</p>
<p class="calibre2">Finally, we understood what backpropagation is and used the MNIST dataset to build our network and train and test our data.</p>
<p class="calibre2">In the next chapter, we will introduce you to CNNs.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-8">
        <section>

                            <header>
                    <h1 class="header-title">Introduction to Convolutional Neural Networks</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre10"><strong class="calibre13">Convolutional Neural Networks</strong> (<strong class="calibre13">CNNs</strong>) are everywhere. In the last five years, we have seen a dramatic rise in the performance of visual recognition systems due to the introduction of deep architectures for feature learning and classification. CNNs have achieved good performance in a variety of areas, such as automatic speech understanding, computer vision, language translation, self-driving cars, and games such as Alpha Go. Thus, the applications of&nbsp;CNNs are almost limitless. DeepMind (from Google) recently published WaveNet, which uses a CNN&nbsp;</span><span class="calibre10">to generate speech that mimics any&nbsp;human voice&nbsp;</span><span class="calibre10">(</span><a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" target="_blank" class="calibre11">https://deepmind.com/blog/wavenet-generative-model-raw-audio/</a><span class="calibre10">).</span></p>
<p class="calibre2">In this chapter, we will cover the following topics:</p>
<ul class="calibre7">
<li class="calibre8">History of CNNs</li>
<li class="calibre8">Overview of a CNN</li>
<li class="calibre8">Image augmentation</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-46">
        <section>

                            <header>
                    <h1 class="header-title">History of CNNs</h1>
                </header>
            
            <article>
                
<p class="calibre2">There have been numerous attempts to recognize pictures by machines for decades. It is a challenge to mimic the visual recognition system of the human brain in a computer. Human vision is the hardest&nbsp;to mimic and most complex sensory cognitive system of the brain. We will not discuss biological neurons<span class="calibre10">&nbsp;</span><span class="calibre10">here, that is,</span>&nbsp;the primary visual cortex, but rather focus on artificial neurons. Objects in the physical world are three dimensional, whereas pictures of those objects are two dimensional. In this book, we will introduce neural networks without appealing to brain analogies. In 1963, computer scientist Larry Roberts, who is also known as the <strong class="calibre13">father of computer vision</strong>, described the possibility of extracting 3D geometrical information from 2D perspective views of blocks <span class="calibre10">in his research dissertation titled<em class="calibre19">&nbsp;</em></span><strong class="calibre13">BLOCK WORLD</strong>. This was the first breakthrough in the world of computer vision. Many researchers worldwide in machine learning and artificial intelligence followed this work and studied computer vision in the context of BLOCK WORLD. Human beings can recognize blocks regardless of any&nbsp;orientation or lighting changes that may happen. In this&nbsp;dissertation, he said that it is important to understand simple edge-like shapes in images. He extracted these edge-like shapes from blocks in order to make the computer understand that these two blocks are the same irrespective of orientation:</p>
<div class="mce-root"><img src="images/000074.png" class="calibre26" /></div>
<p class="calibre2"><span class="calibre10">The vision starts with a simple structure.&nbsp;</span>This is the beginning of computer vision as an engineering model. David Mark, an MIT computer vision scientist, gave us the next important concept, that vision is hierarchical. He wrote a very influential book named <em class="calibre19">VISION</em>. This is a simple book. He said that an image consists&nbsp;<span class="calibre10">of</span><span class="calibre10">&nbsp;</span><span class="calibre10">several layers. These two principles form the basis of deep learning architecture, although they do not tell us what kind of mathematical model to use.</span></p>
<p class="calibre2">In the 1970s, the first visual recognition algorithm, known as the&nbsp;<strong class="calibre13">generalized cylinder model</strong>, came from the AI lab at Stanford University. The idea here is that the world is composed of simple shapes and any real-world object is a combination of these simple shapes. At the same time, another model, known as the&nbsp;<strong class="calibre13">pictorial structure model</strong>, was published from SRI Inc. The concept is still the same as the generalized cylinder model, but the parts are connected by springs; thus, it introduced a concept of variability. The first visual recognition algorithm was used in a digital camera by Fujifilm in 2006.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-102">
        <section>

                            <header>
                    <h1 class="header-title">Convolutional neural networks</h1>
                </header>
            
            <article>
                
<p class="calibre2">CNNs, or ConvNets, are quite similar to regular neural networks. They are still made up of neurons with weights that can be learned from data. Each neuron receives some inputs and performs a dot product. They still have a loss function on the last fully connected layer. They can still use a nonlinearity function. All of the tips and techniques that we learned from the last chapter are still valid for CNN.&nbsp;<span class="calibre10">As we saw in the previous chapter, a regular neural network receives input data as a single vector and passes through a series of&nbsp;</span>hidden layers<span class="calibre10">. Every hidden layer consists of a set of neurons, wherein every neuron is fully connected to all the other neurons in the previous layer. Within a single layer, each neuron is completely independent and they do not share any connections. The last fully connected layer, also called the <strong class="calibre13">output layer</strong>, contains class scores in the case of an image classification problem. Generally, there are three main layers in a simple ConvNet. They are the&nbsp;<strong class="calibre13">convolution layer</strong>, the&nbsp;<strong class="calibre13">pooling layer</strong>, and the&nbsp;<strong class="calibre13">fully connected layer</strong>. We can see a simple neural network in the following image:</span></p>
<div class="mce-root"><img src="images/000102.png" class="calibre28" /></div>
<div class="cdpaligncenter">A&nbsp;regular three-layer neural network</div>
<p class="calibre2">So, what changes? Since a CNN mostly takes images as input, this allows us to encode a few properties into the network, thus reducing the number of parameters.</p>
<p class="calibre2">In the case of real-world image data, CNNs perform better than <strong class="calibre13">Multi-Layer Perceptrons</strong> (<strong class="calibre13">MLPs</strong>). There are two reasons for this:</p>
<ul class="calibre7">
<li class="calibre8">In the last chapter,&nbsp;w<span>e saw&nbsp;</span>that in order to feed an image to an MLP, we convert the input matrix into a simple numeric vector with no spatial structure. It has no knowledge that these numbers are spatially arranged. So, CNNs are built for this very reason; that is, to elucidate the patterns in multidimensional data. Unlike MLPs, CNNs understand the fact that image pixels that are closer in proximity to each other are more heavily related than pixels that are further apart:<br class="title-page-name" />
<div class="mce-root"><em class="calibre29">CNN = Input layer + hidden layer + fully connected layer</em></div>
</li>
<li class="calibre8">CNNs differ from MLPs in the types of hidden layers that can be included in the model. A<span>&nbsp;ConvNet arranges its neurons in three dimensions: <strong class="calibre1">width</strong>, <strong class="calibre1">height</strong>, and <strong class="calibre1">depth</strong>. Each layer transforms its 3D input volume into a 3D output volume of neurons using activation functions. For example, in the following figure, the red input layer holds the image. Thus its width and height are the dimensions of the image, and the depth is three since there are Red, Green, and Blue channels:</span></li>
</ul>
<div class="mce-root"><img src="images/000032.png" class="calibre30" /></div>
<div class="packt_tip">ConvNets are deep neural networks that share their parameters across space.</div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-77">
        <section>

                            <header>
                    <h1 class="header-title">How do computers interpret images?</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre10">Essentially, every image can be represented as a matrix of pixel values. In other words, images can be thought of as a function (<em class="calibre19">f</em>) that maps from <em class="calibre19">R<sup class="calibre31">2</sup></em> to <em class="calibre19">R</em>.</span></p>
<p class="calibre2"><em class="calibre19">f(x, y)</em> gives the intensity value at the position <em class="calibre19">(x, y)</em>. In practice, the value of the function ranges&nbsp;<span class="calibre10">only</span><span class="calibre10">&nbsp;</span><span class="calibre10">from <em class="calibre19">0</em> to <em class="calibre19">255</em>. Similarly, a color image can be represented as a stack of three functions. We can write this as a vector of:</span></p>
<p class="mce-root1"><em class="calibre19">&nbsp;f( x, y) = [ r(x,y) g(x,y) b(x,y)]</em></p>
<p class="calibre2">Or we can write this as a mapping:</p>
<p class="mce-root1"><em class="calibre19">f: R x R --&gt; R3</em></p>
<p class="calibre2">So, a color image is also a function, but in this case, a value at each <em class="calibre19">(x,y)</em> position is not a single number. Instead it is a vector that has three different light intensities corresponding to three color channels. The following is the code for seeing the details of an image as input to a computer.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-103">
        <section>

                            <header>
                    <h1 class="header-title">Code for visualizing an image&nbsp;</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let's take a look at how an image can be visualized with the following code:</p>
<pre class="calibre20"><span>#import all required lib<br class="title-page-name" />import matplotlib.pyplot as plt</span><br class="title-page-name" /><span>%matplotlib inline</span><br class="title-page-name" /><span>import numpy as np</span><br class="title-page-name" /><span>from skimage.io import imread</span><br class="title-page-name" /><span>from skimage.transform import resize</span></pre>
<pre class="calibre20"># Load a color image in grayscale<br class="title-page-name" />image = imread('sample_digit.png',as_grey=True)<br class="title-page-name" />image = resize(image,(28,28),mode='reflect')<br class="title-page-name" />print('This image is: ',type(image), <br class="title-page-name" />         'with dimensions:', image.shape)<br class="title-page-name" /><br class="title-page-name" />plt.imshow(image,cmap='gray')<br class="title-page-name" /><br class="title-page-name" /></pre>
<p class="calibre2">We obtain the following image as a result:</p>
<div class="mce-root"><img src="images/000043.png" class="calibre26" /></div>
<pre class="calibre20">def visualize_input(img, ax):<br class="title-page-name" /><br class="title-page-name" />    ax.imshow(img, cmap='gray')<br class="title-page-name" />    width, height = img.shape<br class="title-page-name" />    thresh = img.max()/2.5<br class="title-page-name" />    for x in range(width):<br class="title-page-name" />        for y in range(height):<br class="title-page-name" />            ax.annotate(str(round(img[x][y],2)), xy=(y,x),<br class="title-page-name" />                        horizontalalignment='center',<br class="title-page-name" />                        verticalalignment='center',<br class="title-page-name" />                        color='white' if img[x][y]&lt;thresh else 'black')<br class="title-page-name" /><br class="title-page-name" />fig = plt.figure(figsize = (12,12)) <br class="title-page-name" />ax = fig.add_subplot(111)<br class="title-page-name" />visualize_input(image, ax)</pre>
<p class="calibre2">The following result is obtained:</p>
<div class="mce-root"><img src="images/000029.png" class="calibre32" /></div>
<p class="calibre2">In the previous chapter, we used an MLP-based approach to recognize images. There are two issues with that approach:</p>
<ul class="calibre7">
<li class="calibre8">It increases the number of parameters</li>
<li class="calibre8">It only accepts vectors as input, that is, flattening a matrix to a vector</li>
</ul>
<p class="calibre2">This means we must find a new way to process images, in which 2D information is not completely lost. CNNs address this issue. Furthermore, CNNs accept matrices as input. Convolutional layers preserve spatial structures. First, we define a convolution window, also called a&nbsp;<strong class="calibre13">filter</strong>, or&nbsp;<strong class="calibre13">kernel</strong>; then slide this over the image.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-104">
        <section>

                            <header>
                    <h1 class="header-title">Dropout</h1>
                </header>
            
            <article>
                
<p class="calibre2">A neural network can be thought of as a search problem. Each node in the neural network is searching for correlation between the input data and the correct output data.</p>
<p class="calibre2">Dropout&nbsp;randomly turns nodes off while forward-propagating and thus helps ward off weights from converging to identical positions. After this is done, it turns on all the nodes and back-propagates. Similarly, we can set some of the layer's values to zero at random during forward propagation in order to perform dropout on a layer.</p>
<div class="packt_tip">
<p class="calibre9">Use dropout only during training. Do not use it at runtime or on your testing dataset.</p>
</div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-105">
        <section>

                            <header>
                    <h1 class="header-title">Input layer</h1>
                </header>
            
            <article>
                
<p class="calibre2">The <strong class="calibre13">input layer</strong> holds the image data. In the following figure, the input layer consists of three inputs. In a <strong class="calibre13">fully connected layer</strong>, the neurons between two adjacent layers are fully <span class="calibre10">connected&nbsp;</span>pairwise but do not share any connection within a layer. In other words, the neurons in this layer have full connections to all activations in the previous layer. Therefore, their activations can be computed with a simple matrix multiplication, optionally adding a bias term. The difference between a fully connected and convolutional layer is that neurons in a <span class="calibre10">convolutional&nbsp;</span>layer are connected to a local region in the input, and that they also share parameters:</p>
<div class="mce-root"><img src="images/000076.png" class="calibre33" /></div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-106">
        <section>

                            <header>
                    <h1 class="header-title">Convolutional layer</h1>
                </header>
            
            <article>
                
<p class="calibre2">The main objective of convolution in relation to ConvNet is to extract features from the input image. This layer does most of the computation in a ConvNet. We will not go into the mathematical details of convolution here but will get an understanding of how it works over images.</p>
<p class="calibre2">The ReLU activation function is extremely useful in CNNs.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-107">
        <section>

                            <header>
                    <h1 class="header-title">Convolutional layers in Keras</h1>
                </header>
            
            <article>
                
<p class="calibre2">To create a convolutional layer in Keras, you must first import the required modules as follows:</p>
<pre class="calibre20"><span>from</span> keras.layers <span>import</span> Conv2D</pre>
<p class="calibre2">Then, you can create a convolutional layer by using the following format:</p>
<pre class="calibre20">Conv2D(filters, kernel_size, strides, padding, activation=<span>'relu'</span>, input_shape)</pre>
<p class="calibre2">You must pass the following arguments:</p>
<ul class="calibre7">
<li class="calibre8"><kbd class="calibre12">filters</kbd>: The number of filters.</li>
<li class="calibre8"><kbd class="calibre12">kernel_size</kbd><span>:&nbsp;A n</span>umber specifying both the height and width of the (square) convolution window. There are <span>also&nbsp;</span>some additional optional arguments that you might like to tune.</li>
<li class="calibre8"><kbd class="calibre12">strides</kbd>: The stride of the convolution. If you don't specify anything, this is set to one.</li>
<li class="calibre8"><kbd class="calibre12">padding</kbd>: This is either <kbd class="calibre12">valid</kbd>&nbsp;or <kbd class="calibre12">same</kbd>. If you don't specify anything, the padding is set to <kbd class="calibre12">valid</kbd>.</li>
<li class="calibre8"><kbd class="calibre12">activation</kbd>: This is typically <kbd class="calibre12">relu</kbd>. If you don't specify anything, no activation is applied. You are strongly encouraged to add a ReLU activation function to every convolutional layer in your networks.&nbsp;</li>
</ul>
<div class="packt_infobox">It is possible to represent both&nbsp;<kbd class="calibre12">kernel_size</kbd> and <kbd class="calibre12">strides</kbd> as either a number or a tuple.</div>
<p class="calibre2">When using your convolutional layer as the first layer (appearing after the input layer) in a model, you must provide an additional <kbd class="calibre12">input_shape</kbd> argument&mdash;<kbd class="calibre12">input_shape</kbd><span class="calibre10">. It is a tuple specifying the height, width, and depth (in that order) of the input.</span></p>
<div class="packt_infobox"><span>Please make sure that the&nbsp;</span>&nbsp;<kbd class="calibre12">input_shape</kbd> argument is not included if the convolutional layer is not the first layer in your network.</div>
<p class="calibre2">There are many other tunable arguments that you can set to change the behavior of your convolutional layers:</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">Example 1</strong>: In order to build a CNN with an input layer that accepts <span>images of 200 x 200 pixels in&nbsp;</span>grayscale. In such cases, the next layer would be a convolutional layer of 16 filters with width and height as 2. As we go ahead with the convolution we can set the filter to jump 2 pixels together. Therefore, we can build a convolutional, layer with a filter that doesn't pad the images with zeroes with the following code:</li>
</ul>
<pre class="calibre34">Conv2D(filters=16, kernel_size=2, strides=2, activation='relu', input_shape=(200, 200, 1))</pre>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">Example 2</strong>: After we build our CNN model, we can have the next layer in it to be a convolutional layer. This layer will have 32 filters with width and height as 3, which would take the layer that was constructed in the previous example as its input. Here, as we proceed with the convolution, we will set the filter to jump one pixel at a time, such that the convolutional layer will be able to see all the regions of the previous layer too. Such a convolutional layer can be constructed with the help of the following code:</li>
</ul>
<pre class="calibre34">Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')</pre>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">Example 3</strong>: You can also construct convolutional layers in Keras of size 2 x 2, with 64 filters and a ReLU activation function. Here, the convolution utilizes a stride of 1 with padding set to <kbd class="calibre12">valid</kbd> and all other arguments set to their default values. Such a convolutional layer can be built using the following code:</li>
</ul>
<pre class="calibre34">Conv2D(64, (2,2), activation='relu')</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-108">
        <section>

                            <header>
                    <h1 class="header-title">Pooling layer</h1>
                </header>
            
            <article>
                
<p class="calibre2">As we have seen, a convolutional layer is a stack of feature maps, with one feature map for each filter. More filters increase the dimensionality of convolution. Higher dimensionality indicates more parameters. So, the pooling layer controls overfitting by progressively reducing the spatial size of the representation to reduce the number of parameters and computation. The pooling layer often takes the convolutional layer as input. The most commonly used pooling approach is <strong class="calibre13">max pooling</strong>. In addition to max pooling, pooling units can also perform other functions such as <strong class="calibre13">average pooling</strong>. In a CNN, we can control the behavior of the convolutional layer by specifying the size of each filter and the number of filters. To increase the number of nodes in a convolutional layer, we can increase the number of filters, and to increase the size of the pattern, we can increase the size of the filter. There are also a few other hyperparameters that can be tuned. One of them is the stride of the convolution. Stride is the amount by which the filter slides over the image. A stride of 1 moves the filter by 1 pixel horizontally and vertically. Here, the convolution becomes the same as the width and depth of the input image. A stride of 2 makes a convolutional layer of half of the width and height of the image. If the filter extends outside of the image, then <span class="calibre10">we can&nbsp;</span>either ignore these unknown values or replace them with zeros. This is known as <strong class="calibre13">padding</strong>. In Keras, we can set <kbd class="calibre12">padding = 'valid'</kbd> if it is acceptable to lose a few values. Otherwise, set <kbd class="calibre12">padding = 'same'</kbd>:</p>
<div class="mce-root"><img class="alignnone1" src="images/000042.png" /></div>
<p class="calibre2">A very simple ConvNet looks like this:</p>
<div class="mce-root"><img src="images/000011.png" class="calibre35" /></div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-1">
        <section>

                            <header>
                    <h1 class="header-title">Practical example &ndash; image classification</h1>
                </header>
            
            <article>
                
<p class="calibre2">The convolutional layer helps to detect regional patterns in an image. The max pooling layer, present after the <span class="calibre10">convolutional&nbsp;</span>layer, helps reduce dimensionality. Here is an example of image classification using all the principles we studied in the previous sections. One important notion is to first make all the images into a standard size before doing anything else. The first <span class="calibre10">convolution&nbsp;</span>layer requires an additional <kbd class="calibre12">input.shape()</kbd> parameter. In this section, we will train a CNN to classify images from the CIFAR-10 database. CIFAR-10 is a dataset of 60,000 color images of 32 x 32 size. These images are labeled into 10 categories with 6,000 images each. These categories are airplane, automobile, bird, cat, dog, deer, frog, horse, ship, and truck. Let's see how to do this with the following code:</p>
<pre class="calibre20">import keras<br class="title-page-name" />import numpy as np<br class="title-page-name" />import matplotlib.pyplot as plt<br class="title-page-name" />%matplotlib inline<br class="title-page-name" /><br class="title-page-name" />fig = plt.figure(figsize=(20,5))<br class="title-page-name" />for i in range(36):<br class="title-page-name" />    ax = fig.add_subplot(3, 12, i + 1, xticks=[], yticks=[])<br class="title-page-name" />    ax.imshow(np.squeeze(x_train[i]))from keras.datasets import cifar10</pre>
<pre class="calibre20"># rescale [0,255] --&gt; [0,1]<br class="title-page-name" />x_train = x_train.astype('float32')/255<br class="title-page-name" />from keras.utils import np_utils<br class="title-page-name" /><br class="title-page-name" /># one-hot encode the labels<br class="title-page-name" />num_classes = len(np.unique(y_train))<br class="title-page-name" />y_train = keras.utils.to_categorical(y_train, num_classes)<br class="title-page-name" />y_test = keras.utils.to_categorical(y_test, num_classes)<br class="title-page-name" /><br class="title-page-name" /># break training set into training and validation sets<br class="title-page-name" />(x_train, x_valid) = x_train[5000:], x_train[:5000]<br class="title-page-name" />(y_train, y_valid) = y_train[5000:], y_train[:5000]<br class="title-page-name" /><br class="title-page-name" /># print shape of training set<br class="title-page-name" />print('x_train shape:', x_train.shape)<br class="title-page-name" /><br class="title-page-name" /># printing number of training, validation, and test images<br class="title-page-name" />print(x_train.shape[0], 'train samples')<br class="title-page-name" />print(x_test.shape[0], 'test samples')<br class="title-page-name" />print(x_valid.shape[0], 'validation samples')x_test = x_test.astype('float32')/255<br class="title-page-name" /><br class="title-page-name" /><br class="title-page-name" />from keras.models import Sequential<br class="title-page-name" />from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout<br class="title-page-name" /><br class="title-page-name" />model = Sequential()<br class="title-page-name" />model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', <br class="title-page-name" />                        input_shape=(32, 32, 3)))<br class="title-page-name" />model.add(MaxPooling2D(pool_size=2))<br class="title-page-name" />model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))<br class="title-page-name" />model.add(MaxPooling2D(pool_size=2))<br class="title-page-name" />model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))<br class="title-page-name" />model.add(MaxPooling2D(pool_size=2))<br class="title-page-name" />model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))<br class="title-page-name" />model.add(MaxPooling2D(pool_size=2))<br class="title-page-name" />model.add(Dropout(0.3))<br class="title-page-name" />model.add(Flatten())<br class="title-page-name" />model.add(Dense(500, activation='relu'))<br class="title-page-name" />model.add(Dropout(0.4))<br class="title-page-name" />model.add(Dense(10, activation='softmax'))<br class="title-page-name" /><br class="title-page-name" />model.summary()<br class="title-page-name" /><br class="title-page-name" /># compile the model<br class="title-page-name" />model.compile(loss='categorical_crossentropy', optimizer='rmsprop', <br class="title-page-name" />                  metrics=['accuracy'])<br class="title-page-name" />from keras.callbacks import ModelCheckpoint <br class="title-page-name" /><br class="title-page-name" /># train the model<br class="title-page-name" />checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, <br class="title-page-name" />                               save_best_only=True)<br class="title-page-name" />hist = model.fit(x_train, y_train, batch_size=32, epochs=100,<br class="title-page-name" />          validation_data=(x_valid, y_valid), callbacks=[checkpointer], <br class="title-page-name" />          verbose=2, shuffle=True)</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-109">
        <section>

                            <header>
                    <h1 class="header-title">Image augmentation</h1>
                </header>
            
            <article>
                
<p class="calibre2">While training a CNN model, we do not want the model to change any prediction based on the size, angle, and position of the image. The image is represented as a matrix of pixel values, so the size, angle, and position have a huge effect on the pixel values. To make the model more size-invariant, we can add different sizes of the image to the training set. Similarly, in order to make the model more rotation-invariant, we can add images with different angles. This process is known as <strong class="calibre13">image data augmentation</strong>. This also helps to avoid overfitting. Overfitting happens when a model is exposed to very few samples. Image data augmentation is one way to reduce overfitting, but it may not be enough because augmented images are still correlated. Keras provides an image augmentation class called&nbsp;<kbd class="calibre12">ImageDataGenerator</kbd> that defines the configuration for image data augmentation. This also provides other features such as:</p>
<ul class="calibre7">
<li class="calibre8">Sample-wise and feature-wise standardization</li>
<li class="calibre8">Random rotation, shifts, shear, and zoom of the image</li>
<li class="calibre8">Horizontal and vertical flip</li>
<li class="calibre8"><span>ZCA whitening</span></li>
<li class="calibre8">Dimension reordering</li>
<li class="calibre8">Saving the changes to disk</li>
</ul>
<p class="calibre2">An augmented image generator object can be created as follows:</p>
<pre class="calibre20">imagedatagen = ImageDataGenerator()</pre>
<p class="calibre2"><span class="calibre10">This API generates batches of tensor image data in real-time data augmentation, instead of processing an entire image dataset in memory. This API is designed to create augmented image data during the model fitting process. Thus, it reduces the memory overhead but adds some time cost for model training.</span></p>
<p class="calibre2">After it is created and configured, you must fit your data.&nbsp;<span class="calibre10">This computes any statistics required to perform the transformations to image data.</span> <span class="calibre10">This is done by calling the&nbsp;</span><kbd class="calibre12">fit()</kbd><span class="calibre10">&nbsp;function on the data generator and passing it to the training dataset, as follows:</span></p>
<pre class="calibre20"><span><span>imagedatagen</span>.</span><span>fit</span><span>(</span><span>train_data</span><span>)</span></pre>
<p class="calibre2"><span class="calibre10">The batch size can be configured, the data generator can be prepared, and batches of images can be received by calling the&nbsp;</span><kbd class="calibre12">flow()</kbd>&nbsp;<span class="calibre10">function:</span></p>
<pre class="cdpalignleft1"><span>imagedatagen.flow(x_train, y_train, batch_size=</span><span>32</span><span>)</span></pre>
<p class="calibre2"><span class="calibre10">Finally,&nbsp;</span><span class="calibre10">call the&nbsp;</span><kbd class="calibre12">fit_generator()</kbd><span class="calibre10">&nbsp;function instead of calling the&nbsp;<kbd class="calibre12">fit()</kbd>&nbsp;function on the model:</span></p>
<pre class="calibre20">fit_generator(imagedatagen, samples_per_epoch=len(X_train), epochs=200)</pre>
<p class="calibre2"><span class="calibre10">Let's look at some examples to understand how</span><span class="calibre10">&nbsp;the image augmentation API in Keras works.&nbsp;</span>We will use the MNIST handwritten digit recognition task in these examples.</p>
<p class="calibre2">Let's begin by taking a look at the first nine images in the training dataset:</p>
<pre class="calibre20">#Plot images <br class="title-page-name" />from keras.datasets import mnist<br class="title-page-name" />from matplotlib import pyplot<br class="title-page-name" />#loading data<br class="title-page-name" />(X_train, y_train), (X_test, y_test) = mnist.load_data()<br class="title-page-name" />#creating a grid of 3x3 images<br class="title-page-name" />for i in range(0, 9):<br class="title-page-name" />  pyplot.subplot(330 + 1 + i)<br class="title-page-name" />  pyplot.imshow(X_train[i], cmap=pyplot.get_cmap('gray'))<br class="title-page-name" />#Displaying the plot<br class="title-page-name" />pyplot.show()</pre>
<p class="calibre2">The following code snippet creates augmented images from the CIFAR-10 dataset. We will add these images to the training set of the last example and see how the classification accuracy increases:</p>
<pre class="calibre20">from keras.preprocessing.image import ImageDataGenerator<br class="title-page-name" /># creating and configuring augmented image generator<br class="title-page-name" />datagen_train = ImageDataGenerator(<br class="title-page-name" /> width_shift_range=0.1, # shifting randomly images horizontally (10% of total width)<br class="title-page-name" /> height_shift_range=0.1, # shifting randomly images vertically (10% of total height)<br class="title-page-name" /> horizontal_flip=True) # flipping randomly images horizontally<br class="title-page-name" /># creating and configuring augmented image generator<br class="title-page-name" />datagen_valid = ImageDataGenerator(<br class="title-page-name" /> width_shift_range=0.1, # shifting randomly images horizontally (10% of total width)<br class="title-page-name" /> height_shift_range=0.1, # shifting randomly images vertically (10% of total height)<br class="title-page-name" /> horizontal_flip=True) # flipping randomly images horizontally<br class="title-page-name" /># fitting augmented image generator on data<br class="title-page-name" />datagen_train.fit(x_train)<br class="title-page-name" />datagen_valid.fit(x_valid)<br class="title-page-name" /><br class="title-page-name" /></pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-110">
        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">We began this chapter by briefly looking into the history of CNNs. We introduced you to the implementation of visualizing images.&nbsp;</p>
<p class="calibre2">We studied image classification with the help of a practical example, using all the principles we learned about in the chapter. Finally, we learned how image augmentation helps us avoid overfitting and studied the various other features provided by image augmentation.</p>
<p class="calibre2">In the next chapter, we will learn how to build a simple image classifier CNN model from scratch.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-111">
        <section>

                            <header>
                    <h1 class="header-title">Build Your First CNN and Performance Optimization</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre10">A</span> <strong class="calibre13">convolutional neural network</strong> <span class="calibre10">(</span><strong class="calibre13">CNN</strong><span class="calibre10">) is a type of</span> <strong class="calibre13">feed-forward neural network</strong> <span class="calibre10">(</span><strong class="calibre13">FNN</strong><span class="calibre10">) in which the connectivity pattern between its neurons is inspired by an animal's visual cortex. In the last few years, CNNs have demonstrated superhuman performance in image search services, self-driving cars, automatic video classification, voice recognition, and</span> <strong class="calibre13">natural language processing&nbsp;</strong><span class="calibre10">(</span><strong class="calibre13">NLP</strong><span class="calibre10">).</span></p>
<p class="calibre2"><span class="calibre10">Considering these motivations, in this chapter, we will construct a simple CNN model for image classification from scratch, followed by some theoretical aspects, such as convolutional and pooling operations. Then we will discuss how to tune hyperparameters and optimize the training time of CNNs for improved classification accuracy. Finally, we will build the second CNN model by considering some best practices.</span> <span class="calibre10">In a nutshell, the following topics will be covered in this chapter:</span><br class="calibre18" /></p>
<ul class="calibre7">
<li class="calibre8">CNN architectures and drawbacks of DNNs</li>
<li class="calibre8">The convolution operations and pooling layers</li>
<li class="calibre8">Creating and training a CNN for image classification</li>
<li class="calibre8">Model performance optimization</li>
<li class="calibre8">Creating an improved CNN for optimized performance</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-7">
        <section>

                            <header>
                    <h1 class="header-title">CNN architectures and drawbacks of DNNs</h1>
                </header>
            
            <article>
                
<p class="chapter-content">In <a href="#calibre_link-8" target="_blank" class="calibre11">Chapter 2</a>,&nbsp;<em class="calibre19">Introduction to Convolutional Neural Networks</em>, we discussed that a regular multilayer perceptron works fine for small images (for example, MNIST or CIFAR-10). However, it breaks down for larger images because of the huge number of parameters it requires. For example, a 100 Ã 100 image has 10,000 pixels, and if the first layer has just 1,000 neurons (which already severely restricts the amount of information transmitted to the next layer), this means 10 million connections; and that is just for the first layer.</p>
<p class="chapter-content">CNNs solve this problem using partially connected layers. Because consecutive layers are only partially connected and because it heavily reuses its weights, a CNN has far fewer parameters than a fully connected DNN, which makes it much faster to train, reduces the risk of overfitting, and requires much less training data. Moreover, when a CNN has learned a kernel that can detect a particular feature, it can detect that feature anywhere on the image. In contrast, when a DNN learns a feature in one location, it can detect it only in that particular location.</p>
<p class="chapter-content">Since images typically have very repetitive features, CNNs are able to generalize much better than DNNs for image processing tasks such as classification, using fewer training examples. Importantly, a DNN has no prior knowledge of how pixels are organized; it does not know that nearby pixels are close. A CNN's architecture embeds this prior knowledge. Lower layers typically identify features in small areas of the images, while higher layers combine the lower-level features into larger features. This works well with most natural images, giving CNNs a decisive head start compared to DNNs:</p>
<div class="mce-root"><img src="images/000093.png" class="calibre26" /></div>
<div class="cdpaligncenter">Figure 1: Regular DNN versus CNN, where each layer has neurons arranged in 3D</div>
<p class="calibre2">For example, in <em class="calibre19">Figure 1</em>, on the left, you can see a regular three-layer neural network. On the right, a ConvNet arranges its neurons in three dimensions (width, height, and depth) as visualized in one of the layers. Every layer of a ConvNet transforms the 3D input volume to a 3D output volume of neuron activations. The red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be three (red, green, and blue channels). Therefore, all the multilayer neural networks we looked at had layers composed of a long line of neurons, and we had to flatten input images or data to 1D before feeding them to the neural network.</p>
<p class="calibre2">However, what happens once you try to feed them a 2D image directly? The answer is that in CNNs, each layer is represented in 2D, which makes it easier to match neurons with their corresponding inputs. We will see examples of this in upcoming sections. Another important fact is that all the neurons in a feature map share the same parameters, so it dramatically reduces the number of parameters in the model; but more importantly, it means that once the CNN has learned to recognize a pattern in one location, it can recognize it in any other location.</p>
<p class="calibre2">In contrast, once a regular DNN has learned to recognize a pattern in one location, it can recognize it only in that particular location. In multilayer networks such as MLP or DBN, the outputs of all neurons of the input layer are connected to each neuron in the hidden layer, and then the output will again act as the input to the fully connected layer. In CNN networks, the connection scheme that defines the convolutional layer is significantly different. The convolutional layer is the main type of layer in a CNN, where each neuron is connected to a certain region of the input area called the <strong class="calibre13">receptive field</strong>.</p>
<p class="calibre2">In a typical CNN architecture, a few convolutional layers are connected in a cascade style. Each layer is followed by a <strong class="calibre13">Rectified Linear Unit</strong> (<strong class="calibre13">ReLU</strong>) layer, then a pooling layer, then one or more convolutional layers (+ReLU), then another pooling layer, and finally one or more fully connected layers. Pretty much depending on problem type, the network might be deep though. The output from each convolution layer is a set of objects called <strong class="calibre13">feature maps</strong>, generated by a single kernel filter. Then the feature maps can be used to define a new input to the next layer.</p>
<p class="calibre2">Each neuron in a CNN network produces an output, followed by an activation threshold, which is proportional to the input and not bound:</p>
<div class="mce-root"><img src="images/000054.png" class="calibre36" /></div>
<div class="cdpaligncenter">Figure 2: A conceptual architecture of a CNN</div>
<p class="calibre2">As you can see in <em class="calibre19">Figure 2</em>, the pooling layers are usually placed after the convolutional layers <span class="calibre10">(for example, between two convolutional layers)</span>. A pooling layer into subregions then divides the convolutional region. Then, a single representative value is selected, using either a max-pooling or an average pooling technique, to reduce the computational time of subsequent layers. This way, a CNN can be thought of as a feature extractor. To understand this more clearly, refer to the following figure:</p>
<div class="mce-root"><img src="images/000049.png" class="calibre26" /></div>
<p class="calibre2">In this way, the robustness of the feature with respect to its spatial position is increased too. To be more specific, when feature maps are used as image properties and pass through the grayscale image, it gets smaller and smaller as it progresses through the network; but it also typically gets deeper and deeper, as more feature maps will be added.</p>
<p class="calibre2">We've already discussed the limitations of such FFNN - that is, a very high number of neurons would be necessary, even in a shallow architecture, due to the very large input sizes associated with images, where each pixel is a relevant variable. The convolution operation brings a solution to this problem as it reduces the number of free parameters, allowing the network to be deeper with fewer parameters.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-112">
        <section>

                            <header>
                    <h1 class="header-title">Convolutional operations</h1>
                </header>
            
            <article>
                
<p class="calibre2">A convolution is a mathematical operation that slides one function over another and measures the integral of their pointwise multiplication. It has deep connections with the Fourier transformation and the Laplace transformation and is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very similar to convolutions.</p>
<div class="packt_infobox">In mathematics, convolution is a mathematical operation on two functions that produces a third function&mdash;that is, the modified (convoluted) version of one of the original functions. The resulting function gives in integral of the pointwise multiplication of the two functions as a function of the amount that one of the original functions is translated. Interested readers can refer to this URL for more information: <a href="https://en.wikipedia.org/wiki/Convolution" class="calibre11">https://en.wikipedia.org/wiki/Convolution</a>.</div>
<p class="calibre2">Thus, the most important building block of a CNN is the convolutional layer. Neurons in the first convolutional layer are not connected to every single pixel in the input image (that is, like FNNs&mdash;for example, MLP and DBN) but only to pixels in their receptive fields. See <em class="calibre19">Figure 3</em>. In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer:</p>
<div class="mce-root"><img src="images/000022.png" class="calibre37" /></div>
<div class="cdpaligncenter">Figure 3: Each convolutional neuron processes data only for its receptive field</div>
<p class="calibre2">In <a href="#calibre_link-8" target="_blank" class="calibre11">Chapter 2</a>,&nbsp;<em class="calibre19">Introduction to Convolutional Neural Networks</em>, we have seen that all multilayer neural networks (for example, MLP) have layers composed of so many neurons, and we have to flatten input images to 1D before feeding them to the neural network. Instead, in a CNN, each layer is represented in 2D, which makes it easier to match neurons with their corresponding inputs.</p>
<div class="packt_infobox">The receptive fields concept is used by CNNs to exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent layers.</div>
<p class="calibre2">This architecture allows the network to concentrate on low-level features in the first hidden layer, and then assemble them into higher-level features in the next hidden layer, and so on. This hierarchical structure is common in real-world images, which is one of the reasons why CNNs work so well for image recognition.</p>
<p class="calibre2">Finally, it not only requires a low number of neurons but also reduces the number of trainable parameters significantly. For example, regardless of image size, building regions of size 5 x 5, each with the same-shared weights, requires only 25 learnable parameters. In this way, it resolves the vanishing or exploding gradients problem in training traditional multilayer neural networks with many layers by using backpropagation.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-12">
        <section>

                            <header>
                    <h1 class="header-title">Pooling, stride, and padding operations</h1>
                </header>
            
            <article>
                
<p class="calibre2">Once you've understood how convolutional layers work, the pooling layers are quite easy to grasp. A pooling layer typically works on every input channel independently, so the output depth is the same as the input depth. You may alternatively pool over the depth dimension, as we will see next, in which case the image's spatial dimensions (for example, height and width) remain unchanged but the number of channels is reduced. Let's see a formal definition of pooling layers from the well-known TensorFlow website:</p>
<div class="packt_quote">"The pooling ops sweep a rectangular window over the input tensor, computing a reduction operation for each window (average, max, or max with argmax). Each pooling op uses rectangular windows of size called ksize separated by offset strides. For example, if strides are all ones, every window is used, if strides are all twos, every other window is used in each dimension, and so on."</div>
<p class="calibre2">Therefore, in summary, just like convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. However, we must define its size, the stride, and the padding type. So in summary, the output can be computed as follows:</p>
<pre class="calibre20">output[i] = reduce(value[strides * i:strides * i + ksize]),</pre>
<p class="calibre2">Here, the indices also take the padding values&nbsp;<span class="calibre10">into consideration</span><span class="calibre10">.</span></p>
<div class="packt_infobox">A pooling neuron has no weights. Therefore, all it does is aggregate the inputs using an aggregation function such as max or mean.</div>
<p class="calibre2">In other words, the goal of using pooling is to subsample the input image in order to reduce the computational load, memory usage, and number of parameters. This helps to avoid overfitting in the training stage. Reducing the input image size also makes the neural network tolerate a little bit of image shift. The spatial semantics of the convolution ops depend on the padding scheme chosen.</p>
<p class="calibre2">Padding is an operation to increase the size of the input data. In the case of one-dimensional data, you just append/prepend the array with a constant; in two-dimensional data, you surround the matrix with these constants. In n-dimensional, you surround your n-dimensional hypercube with the constant. In most of the cases, this constant is zero and it is called <strong class="calibre13">zero padding</strong>:</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">VALID padding</strong>: Only drops the rightmost columns (or bottommost rows)</li>
<li class="calibre8"><strong class="calibre1">SAME padding</strong>: Tries to pad evenly left and right, but if the number of columns to be added is odd, it will add the extra column to the right, as is the case in this example</li>
</ul>
<p class="calibre2">Let's explain the preceding definition graphically, in the following figure. If we want&nbsp;a layer to have the same height and width as the previous layer, it is common to add zeros around the inputs, as shown in the diagram. This is called <strong class="calibre13">SAME</strong> or <strong class="calibre13">zero</strong> <strong class="calibre13">padding</strong>.</p>
<div class="packt_tip">The term <strong class="calibre1">SAME</strong> means that the output feature map has the same spatial dimensions as the input feature map.</div>
<p class="calibre2">On the other hand, zero padding is introduced to make the shapes match as needed, equally on every side of the input map. <strong class="calibre13">VALID</strong> means no padding and only drops the rightmost columns (or bottommost rows):</p>
<div class="mce-root"><img src="images/000101.png" class="calibre38" /></div>
<div class="cdpaligncenter">Figure 4: SAME versus VALID padding with CNN</div>
<p class="calibre2">In the following example (<em class="calibre19">Figure 5</em>), we use a 2 Ã 2 pooling kernel and a stride of 2 with no padding. Only the <strong class="calibre13">max</strong> input value in each kernel makes it to the next layer since the other inputs are dropped (we will see this later on):</p>
<div class="mce-root"><img src="images/000063.png" class="calibre39" /></div>
<div class="cdpaligncenter">Figure 5: An example using max pooling, that is, subsampling</div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-55">
        <section>

                            <header>
                    <h1 class="header-title">Fully connected layer</h1>
                </header>
            
            <article>
                
<p class="calibre2">At the top of the stack, a regular fully connected layer (also known as <strong class="calibre13">FNN</strong> or <strong class="calibre13">dense layer</strong>) is added; it acts similar to an MLP, which might be composed of a few fully connected layers (+ReLUs). The final layer outputs (for example, softmax) the prediction. An example is a softmax layer that outputs estimated class probabilities for a multiclass classification.</p>
<p class="calibre2">Fully connected layers connect every neuron in one layer to every neuron in another layer. Although fully connected FNNs can be used to learn features as well as classify data, it is not practical to apply this architecture to images.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-113">
        <section>

                            <header>
                    <h1 class="header-title">Convolution and pooling operations in TensorFlow</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now that we have seen how convolutional and pooling operations are performed theoretically, let's see how we can perform these operation hands-on using TensorFlow. So let's get started.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-42">
        <section>

                            <header>
                    <h1 class="header-title">Applying pooling operations in TensorFlow</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre10">Using TensorFlow, a</span> <span class="calibre10">subsampling layer can normally be represented by a <kbd class="calibre12">max_pool</kbd> operation by maintaining the initial parameters of the layer.</span> For <kbd class="calibre12">max_pool</kbd>, it has the following signature in TensorFlow:</p>
<pre class="calibre20">tf.nn.max_pool(value, ksize, strides, padding, data_format, name) </pre>
<p class="calibre2">Now let's learn how to create a function that utilizes the preceding signature and returns a tensor with type <kbd class="calibre12">tf.float32</kbd>, that is, the max pooled output tensor:</p>
<pre class="calibre20">import tensorflow as tf<br class="title-page-name" /> 
def maxpool2d(x, k=2): 
   return tf.nn.max_pool(x,  
               ksize=[1, k, k, 1],  
               strides=[1, k, k, 1],  
               padding='SAME') </pre>
<p class="calibre2">In the preceding code segment, the parameters can be described as follows:</p>
<ul class="calibre7">
<li class="calibre8"><kbd class="calibre12">value</kbd>: This is a 4D tensor of <kbd class="calibre12">float32</kbd> elements and shape (batch length, height, width, and channels)</li>
<li class="calibre8"><kbd class="calibre12">ksize</kbd>: A list of integers representing the window size on each dimension</li>
<li class="calibre8"><kbd class="calibre12">strides</kbd>: The step of the moving windows on each dimension</li>
<li class="calibre8"><kbd class="calibre12">data_format</kbd>: <kbd class="calibre12">NHWC</kbd>, <kbd class="calibre12">NCHW</kbd>, and <kbd class="calibre12">NCHW_VECT_C</kbd> are supported</li>
<li class="calibre8"><kbd class="calibre12">ordering</kbd>: <kbd class="calibre12">NHWC</kbd> or <kbd class="calibre12">NCHW</kbd></li>
<li class="calibre8"><kbd class="calibre12">padding</kbd>: <kbd class="calibre12">VALID</kbd> or <kbd class="calibre12">SAME</kbd></li>
</ul>
<p class="calibre2">However, depending upon the layering structures in a CNN, there are other pooling operations supported by TensorFlow, as follows:</p>
<ul class="calibre7">
<li class="calibre8"><kbd class="calibre12">tf.nn.avg_pool</kbd>: This returns a reduced tensor with the average of each window</li>
<li class="calibre8"><kbd class="calibre12">tf.nn.max_pool_with_argmax</kbd>: This returns the <kbd class="calibre12">max_pool</kbd> tensor and a tensor with the flattened index of <kbd class="calibre12">max_value</kbd></li>
<li class="calibre8"><kbd class="calibre12">tf.nn.avg_pool3d</kbd>: This performs an <kbd class="calibre12">avg_pool</kbd> operation with a cubic-like</li>
<li class="calibre8">window; the input has an added depth</li>
<li class="calibre8"><kbd class="calibre12">tf.nn.max_pool3d</kbd>: This performs the same function as (...) but applies the max operation</li>
</ul>
<p class="calibre2"><span class="calibre10">Now let's see a concrete example of how the padding thing works in TensorFlow. Suppose we have an input image <kbd class="calibre12">x</kbd> with shape</span> <kbd class="calibre12">[2, 3]</kbd> and one channel. Now we want to see the effect of both <kbd class="calibre12">VALID</kbd> and <kbd class="calibre12">SAME</kbd> paddings:</p>
<ul class="calibre7">
<li class="calibre8"><kbd class="calibre12">valid_pad</kbd>: Max pool with 2 x 2 kernel, stride 2, and <kbd class="calibre12">VALID</kbd> padding</li>
<li class="calibre8"><kbd class="calibre12">same_pad</kbd>: Max pool with 2 x 2 kernel, stride 2, and <kbd class="calibre12">SAME</kbd> padding</li>
</ul>
<p class="calibre2">Let's see how we can attain this in Python and TensorFlow. Suppose we have an input image of shape <kbd class="calibre12">[2, 4]</kbd>, which is one channel:</p>
<pre class="calibre20">import tensorflow as tf 
x = tf.constant([[2., 4., 6., 8.,], 
                 [10., 12., 14., 16.]]) </pre>
<p class="calibre2">Now let's give it a shape accepted by <kbd class="calibre12">tf.nn.max_pool</kbd>:</p>
<pre class="calibre20">x = tf.reshape(x, [1, 2, 4, 1]) </pre>
<p class="calibre2"><span class="calibre10">If we want to apply the</span> <kbd class="calibre12">VALID</kbd> <span class="calibre10">padding with the max pool with a 2 x 2 kernel, stride 2</span>:</p>
<pre class="calibre20">VALID = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID') </pre>
<p class="calibre2">On the other hand, using the max pool with a 2 x 2 kernel, stride 2 and <kbd class="calibre12">SAME</kbd> padding:</p>
<pre class="calibre20">SAME = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME') </pre>
<p class="calibre2">For <kbd class="calibre12">VALID</kbd> padding, since there is no padding, the output shape is <kbd class="calibre12">[1, 1]</kbd>. However, for the <kbd class="calibre12">SAME</kbd> padding, since we pad the image to the shape <kbd class="calibre12">[2, 4]</kbd> (with -&nbsp;<kbd class="calibre12">inf</kbd>) and then apply the max pool, the output shape is <kbd class="calibre12">[1, 2]</kbd>. Let's validate them:</p>
<pre class="calibre20">print(VALID.get_shape())  
print(SAME.get_shape())  </pre>
<pre class="calibre20">&gt;&gt;&gt; 
(1, 1, 2, 1) 
(1, 1, 2, 1) </pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-114">
        <section>

                            <header>
                    <h1 class="header-title">Convolution operations in TensorFlow</h1>
                </header>
            
            <article>
                
<p class="calibre2">TensorFlow provides a variety of methods for convolution. The canonical form is applied by the <kbd class="calibre12">conv2d</kbd> operation. Let's have a look at the usage of this operation:</p>
<pre class="calibre20">conv2d(<br class="title-page-name" />     input,<br class="title-page-name" />     filter,<br class="title-page-name" />     strides,<br class="title-page-name" />     padding,<br class="title-page-name" />     use_cudnn_on_gpu=True,<br class="title-page-name" />     data_format='NHWC',<br class="title-page-name" />     dilations=[1, 1, 1, 1],<br class="title-page-name" />     name=None<br class="title-page-name" /> )</pre>
<p class="calibre2">The parameters we use are as follows:</p>
<ul class="calibre7">
<li class="calibre8"><kbd class="calibre12">input</kbd>: The operation will be applied to this original tensor. It has a definite format of four dimensions, and the default dimension order is shown next.</li>
<li class="calibre8"><kbd class="calibre12">filter</kbd>: This is a tensor representing a kernel or filter. It has a very generic method: (<kbd class="calibre12">filter_height</kbd>, <kbd class="calibre12">filter_width</kbd>, <kbd class="calibre12">in_channels</kbd>, and <kbd class="calibre12">out_channels</kbd>).</li>
<li class="calibre8"><kbd class="calibre12">strides</kbd>: This is a list of four <kbd class="calibre12">int</kbd> tensor datatypes, which indicate the sliding windows for each dimension.</li>
<li class="calibre8"><kbd class="calibre12">padding</kbd>: This can be <kbd class="calibre12">SAME</kbd> or <kbd class="calibre12">VALID</kbd>.&nbsp;<kbd class="calibre12">SAME</kbd> will try to conserve the initial tensor dimension, but <kbd class="calibre12">VALID</kbd> will allow it to grow if the output size and padding are computed. We will see later how to perform padding along with the pooling layers.</li>
<li class="calibre8"><kbd class="calibre12">use_cudnn_on_gpu</kbd>: This indicates whether to use the <kbd class="calibre12">CUDA GPU CNN</kbd> library to accelerate calculations.</li>
<li class="calibre8"><kbd class="calibre12">data_format</kbd>: This specifies the order in which data is organized (<kbd class="calibre12">NHWC</kbd> or <kbd class="calibre12">NCWH</kbd>).</li>
<li class="calibre8"><kbd class="calibre12">dilations</kbd>: This signifies an optional list of <kbd class="calibre12">ints</kbd>. It defaults to (1, 1, 1, 1). 1D tensor of length 4. The dilation factor for each dimension of input. If it is set to k &gt; 1, there will be k-1 skipped cells between each filter element on that dimension. The dimension order is determined by the value of <kbd class="calibre12">data_format</kbd>; see the preceding code example for details. Dilations in the batch and depth dimensions must be 1.</li>
<li class="calibre8"><kbd class="calibre12">name</kbd>: A name for the operation (optional).</li>
</ul>
<p class="calibre2">The following is an example of a convolutional layer. It concatenates a convolution, adds a bias parameter sum, and finally returns the activation function we have chosen for the whole layer (in this case, the ReLU operation, which is a frequently used one):</p>
<pre class="calibre20">def conv_layer(data, weights, bias, strides=1): 
   x = tf.nn.conv2d(x,  
               weights,  
               strides=[1, strides, strides, 1],  
               padding='SAME') 
   x = tf.nn.bias_add(x, bias) 
   return tf.nn.relu(x) </pre>
<p class="calibre2">Here, x is the 4D t<span class="calibre10">ensor input (batch size, height, width, and channel).</span> TensorFlow also offers a few other kinds of convolutional layers. For example:</p>
<ul class="calibre7">
<li class="calibre8"><kbd class="calibre12">tf.layers.conv1d()</kbd> creates a convolutional layer for 1D inputs. This is useful, for example, in NLP, where a sentence may be represented as a 1D array of words, and the receptive field covers a few neighboring words.</li>
<li class="calibre8"><kbd class="calibre12">tf.layers.conv3d()</kbd> creates a convolutional layer for 3D inputs.</li>
<li class="calibre8"><kbd class="calibre12">tf.nn.atrous_conv2d()</kbd> creates an a trous convolutional layer (<em class="calibre29">a</em> tro<em class="calibre29">us</em> is French for with holes). This is equivalent to using a regular convolutional layer with a filter dilated by inserting rows and columns of zeros. For example, a 1 Ã 3 filter equal to (1, 2, 3) may be dilated with a dilation rate of 4, resulting in a dilated filter (1, 0, 0, 0, 2, 0, 0, 0, 3). This allows the convolutional layer to have a larger receptive field at no computational price and using no extra parameters.</li>
<li class="calibre8"><kbd class="calibre12">tf.layers.conv2d_transpose ()</kbd> creates a transpose convolutional layer, sometimes called a <strong class="calibre1">deconvolutional layer,</strong> which up-samples an image. It does so by inserting zeros between the inputs, so you can think of this as a regular convolutional layer using a fractional stride.</li>
<li class="calibre8"><kbd class="calibre12">tf.nn.depthwise_conv2d()</kbd> creates a depth-wise convolutional layer that applies every filter to every individual input channel independently. Thus, if there are <em class="calibre29">f<sub class="calibre40">n</sub></em> filters and <em class="calibre29">f<sub class="calibre40">n</sub></em><sub class="calibre40">â²</sub> input channels, then this will output <em class="calibre29">f<sub class="calibre40">n&nbsp;</sub></em>Ã <em class="calibre29">f<sub class="calibre40">n</sub></em><sub class="calibre40">â²</sub> feature maps.</li>
<li class="calibre8"><kbd class="calibre12">tf.layers.separable_conv2d()</kbd> creates a separable convolutional layer that first acts like a depth-wise convolutional layer and then applies a 1 Ã 1 convolutional layer to the resulting feature maps. This makes it possible to apply filters to arbitrary sets of inputs channels.</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-115">
        <section>

                            <header>
                    <h1 class="header-title">Training a CNN</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the previous section, we have seen how to construct a CNN and apply different operations on its different layers. Now when it comes to training a CNN, it is much trickier as it needs a lot of considerations to control those operations such as applying appropriate activation function, weight and bias initialization, and of course, using optimizers intelligently.</p>
<p class="calibre2">There are&nbsp;also some advanced considerations such as hyperparameter tuning for optimized too. However, that will be discussed in the next section. We first start our discussion with weight and bias initialization.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-116">
        <section>

                            <header>
                    <h1 class="header-title">Weight and bias initialization</h1>
                </header>
            
            <article>
                
<p class="calibre2">One of the most common initialization techniques in training a DNN is random initialization. The idea of using random initialization is just sampling each weight from a normal distribution of the input dataset with low deviation. Well, a low deviation allows you to bias the network towards the simple 0 solutions.</p>
<p class="calibre2">But what does it mean? The thing is that, the initialization can be completed without the bad repercussions of actually initializing the weights to 0. Secondly, Xavier initialization is often used to train CNNs. It is similar to random initialization but often turns out to work much better. Now let me explain the reason for this:</p>
<ul class="calibre7">
<li class="calibre8">Imagine that you initialize the network weights randomly but they turn out to start too small. Then the signal shrinks as it passes through each layer until it is too tiny to be useful.</li>
<li class="calibre8">On the other hand, if the weights in a network start too large, then the signal grows as it passes through each layer until it is too massive to be useful.</li>
</ul>
<p class="calibre2">The good thing is that using Xavier initialization makes sure the weights are just right, keeping the signal in a reasonable range of values through many layers. In summary, it can automatically determine the scale of initialization based on the number of input and output neurons.</p>
<div class="packt_infobox"><span>Interested readers should refer to this publication for detailed information: Xavier Glorot and Yoshua Bengio, <em class="calibre29">Understanding the difficulty of training deep FNNs</em>, Proceedings of the 13th International Conference on <strong class="calibre1">Artificial Intelligence and Statistics</strong> (<strong class="calibre1">AISTATS</strong>) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: W&amp;CP.</span></div>
<p class="calibre2">Finally, you may ask an intelligent question, <em class="calibre19">Can't I get rid of the random initialization while training a regular DNN (for example, MLP or DBN)</em>? Well, recently, some researchers have been talking about random orthogonal matrix initializations that perform better than just any random initialization for training DNNs:</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">When it comes to initializing the biases</strong>, it is possible and common to initialize the biases to be zero since the asymmetry breaking is provided by the small random numbers in the weights. Setting the biases to a small constant value such as 0.01 for all biases ensures that all ReLU units can propagate some gradient. However, it neither performs well nor does consistent improvement. Therefore, sticking with zero is recommended.</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-45">
        <section>

                            <header>
                    <h1 class="header-title">Regularization</h1>
                </header>
            
            <article>
                
<p class="calibre2">There are several ways of controlling training of CNNs to prevent overfitting in the training phase. For example, L2/L1 regularization, max norm constraints, and drop out:</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">L2 regularization</strong>: This is perhaps the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. For example, using the gradient descent parameter update, L2 regularization ultimately means that every weight is decayed linearly: <em class="calibre29">W += -</em>lambda * <em class="calibre29">W</em> towards zero.</li>
<li class="calibre8"><strong class="calibre1">L1 regularization</strong>: This is another relatively common form of regularization, where for each weight <em class="calibre29">w</em> we add the term <em class="calibre29">Î»â£wâ£</em> to the objective. However, it is also possible to possible to combine the L1 regularization with the L2 regularization: <em class="calibre29">Î»1â£wâ£+Î»2w2</em>, which is commonly known as <strong class="calibre1">Elastic-net regularization</strong>.</li>
<li class="calibre8"><strong class="calibre1">Max-norm constraints</strong>: Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint.</li>
</ul>
<p class="calibre2">Finally, dropout is an advanced variant of regularization, which will be discussed later in this chapter.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-117">
        <section>

                            <header>
                    <h1 class="header-title">Activation functions</h1>
                </header>
            
            <article>
                
<p class="calibre2">The activation ops provide different types of nonlinearities for use in neural networks. These include smooth nonlinearities, such as <kbd class="calibre12">sigmoid</kbd>, <kbd class="calibre12">tanh</kbd>, <kbd class="calibre12">elu</kbd>, <kbd class="calibre12">softplus</kbd>, and <kbd class="calibre12">softsign</kbd>. On the other hand, some continuous but not-everywhere-differentiable functions that can be used are <kbd class="calibre12">relu</kbd>, <kbd class="calibre12">relu6</kbd>, <kbd class="calibre12">crelu</kbd>, and <kbd class="calibre12">relu_x</kbd>. All activation ops apply component-wise and produce a tensor of the same shape as the input tensor. Now let us see how to use a few commonly used activation functions in TensorFlow syntax.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-47">
        <section>

                            <header>
                    <h1 class="header-title">Using sigmoid</h1>
                </header>
            
            <article>
                
<p class="calibre2">In TensorFlow, the signature <kbd class="calibre12">tf.sigmoid(x, name=None)</kbd> computes sigmoid of <kbd class="calibre12">x</kbd> element-wise using <em class="calibre19">y = 1 / (1 + exp(-x))</em> and returns a tensor with the same type <kbd class="calibre12">x</kbd>. Here is the parameter description:</p>
<ul class="calibre7">
<li class="calibre8"><kbd class="calibre12">x</kbd>: A tensor. This must be one of the following types: <kbd class="calibre12">float32</kbd>, <kbd class="calibre12">float64</kbd>, <kbd class="calibre12">int32</kbd>, <kbd class="calibre12">complex64</kbd>, <kbd class="calibre12">int64</kbd>, or <kbd class="calibre12">qint32</kbd>.</li>
<li class="calibre8"><kbd class="calibre12">name</kbd>: A name for the operation (optional).</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-4">
        <section>

                            <header>
                    <h1 class="header-title">Using tanh</h1>
                </header>
            
            <article>
                
<p class="calibre2">In TensorFlow, the signature <kbd class="calibre12">tf.tanh(x, name=None)</kbd> computes a&nbsp;hyperbolic tangent of <kbd class="calibre12">x</kbd> element-wise and returns a tensor with the same type <kbd class="calibre12">x</kbd>. Here is the parameter description:</p>
<ul class="calibre7">
<li class="calibre8"><kbd class="calibre12">x</kbd>: A tensor or sparse. This is a tensor with type <kbd class="calibre12">float</kbd>, <kbd class="calibre12">double</kbd>, <kbd class="calibre12">int32</kbd>, <kbd class="calibre12">complex64</kbd>, <kbd class="calibre12">int64</kbd>, or <kbd class="calibre12">qint32</kbd>.</li>
<li class="calibre8"><kbd class="calibre12">name</kbd>: A name for the operation (optional).</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-50">
        <section>

                            <header>
                    <h1 class="header-title">Using ReLU</h1>
                </header>
            
            <article>
                
<p class="calibre2">In TensorFlow, the signature <kbd class="calibre12">tf.nn.relu(features, name=None)</kbd>&nbsp;computes a rectified linear using <kbd class="calibre12">max(features, 0)</kbd> and returns a tensor having the same type as features. Here is the parameter description:</p>
<ul class="calibre7">
<li class="calibre8"><kbd class="calibre12">features</kbd>: A tensor. This must be one of the following types: <kbd class="calibre12">float32</kbd>, <kbd class="calibre12">float64</kbd>, <kbd class="calibre12">int32</kbd>, <kbd class="calibre12">int64</kbd>, <kbd class="calibre12">uint8</kbd>, <kbd class="calibre12">int16</kbd>, <kbd class="calibre12">int8</kbd>, <kbd class="calibre12">uint16</kbd>, and <kbd class="calibre12">half</kbd>.</li>
<li class="calibre8"><kbd class="calibre12">name</kbd>: A name for the operation (optional).</li>
</ul>
<p class="calibre2">For more on how to use other activation functions, please refer to the&nbsp;TensorFlow website. Up to this point, we have the minimal theoretical knowledge to build our first CNN network for making a prediction.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-118">
        <section>

                            <header>
                    <h1 class="header-title">Building, training, and evaluating our first CNN</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the next section, we will<span class="calibre10">&nbsp;look at</span> how to classify and distinguish between dogs from cats based on their raw images. We will also look at how to implement our first CNN model to deal with the raw and color image having three channels. This network design and implementation are not straightforward; TensorFlow low-level APIs will be used for this. However, do not worry; later in this chapter, we will see another example of implementing a CNN using TensorFlow's high-level contrib API. Before we formally start, a short description of the dataset is a mandate.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-54">
        <section>

                            <header>
                    <h1 class="header-title">Dataset description</h1>
                </header>
            
            <article>
                
<p class="calibre2">For this example, we will use the dog versus cat dataset from Kaggle that was provided for the infamous Dogs versus Cats classification problem as a playground competition with kernels enabled. The dataset can be downloaded from <a href="https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data" class="calibre11">https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data</a>.</p>
<p class="calibre2">The train folder contains 25,000 images of dogs and cats. Each image in this folder has the label as part of the filename. The test folder contains 12,500 images, named according to a numeric ID. For each image in the test set, you should predict a probability that the image is a dog (1 = dog, 0 = cat); that is, a binary classification problem. For this example, there are three Python scripts.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-119">
        <section>

                            <header>
                    <h1 class="header-title">Step 1 &ndash; Loading the required packages</h1>
                </header>
            
            <article>
                
<p class="calibre2">Here we import the required packages and libraries. Note that depending upon the platform, your imports might be different:</p>
<pre class="calibre20">import time 
import math 
import random 
import os 
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import tensorflow as tf 
import Preprocessor 
import cv2 
import LayersConstructor 
from sklearn.metrics import confusion_matrix 
from datetime import timedelta 
from sklearn.metrics.classification import accuracy_score 
from sklearn.metrics import precision_recall_fscore_support </pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-56">
        <section>

                            <header>
                    <h1 class="header-title">Step 2 &ndash; Loading the training/test images to generate train/test set</h1>
                </header>
            
            <article>
                
<p class="calibre2">We set the number of color channels as 3 for the images. In the previous section, we have seen that it should be 1 for grayscale images:</p>
<pre class="calibre20">num_channels = 3 </pre>
<p class="calibre2">For the simplicity, we assume the image dimensions should be squares only. Let's set the size to be <kbd class="calibre12">128</kbd>:</p>
<pre class="calibre20">img_size = 128 </pre>
<p class="calibre2">Now that we have the image size (that is, <kbd class="calibre12">128</kbd>) and the number of the channel (that is, 3), the size of the image when flattened to a single dimension would be the multiplication of the image dimension and the number of channels, as follows:</p>
<pre class="calibre20">img_size_flat = img_size * img_size * num_channels </pre>
<p class="calibre2">Note that, at a later stage, we might need to reshape the image for the max pooling and convolutional layers, so we need to reshape the image. For our case, it would be the tuple with height and width of images used to reshape arrays:</p>
<pre class="calibre20">img_shape = (img_size, img_size)  </pre>
<p class="calibre2">We should have explicitly defined the labels (that is, classes) since we only have the raw color image, and so the images do not have the labels like other numeric machine learning dataset, have. Let's explicitly define the class info as follows:</p>
<pre class="calibre20">classes = ['dogs', 'cats']  
num_classes = len(classes) </pre>
<p class="calibre2">We need to define the batch size that needs to be trained on our CNN model later on:</p>
<pre class="calibre20">batch_size = 14  </pre>
<p class="calibre2">Note that we also can define what portion of the training set will be used as the validation split. Let's assume that 16% will be used, for simplicity:</p>
<pre class="calibre20">validation_size = 0.16  </pre>
<p class="calibre2">One <span class="calibre10">important</span> thing to set <span class="calibre10">is</span> how long to wait after the validation loss stops improving before terminating the training. We should use none if we do not want to implement early stopping:</p>
<pre class="calibre20">early_stopping = None   </pre>
<p class="calibre2">Now, download the dataset and you have to do one thing manually: separate the images of dogs and cats and place them in two separate folders. To be more specific, suppose you put your training set under the path <kbd class="calibre12">/home/DoG_CaT/data/train/</kbd>. In the train folder, create two separate folders <kbd class="calibre12">dogs</kbd> and <kbd class="calibre12">cats</kbd> but only show the path to <kbd class="calibre12">DoG_CaT/data/train/</kbd>. We also assume that our test set is in the <kbd class="calibre12">/home/DoG_CaT/data/test/</kbd> directory. In addition, you can define the checkpoint directory where the logs and model checkpoint files will be written:</p>
<pre class="calibre20">train_path = '/home/DoG_CaT/data/train/' 
test_path = '/home/DoG_CaT/data/test/' 
checkpoint_dir = "models/" </pre>
<p class="calibre2"><span class="calibre10">Then we start reading the training set and prepare it for the CNN model. For processing the test and train set, we have another script <kbd class="calibre12">Preprocessor.py</kbd>. Nonetheless, it would be better to prepare the test set as well</span><strong class="calibre13">:</strong></p>
<pre class="calibre20">data = Preprocessor.read_train_sets(train_path, img_size, classes, validation_size=validation_size) </pre>
<p class="calibre2">The preceding line of code reads the raw images of cats and dogs and creates the training set. The <kbd class="calibre12">read_train_sets()</kbd> function goes as follows:</p>
<pre class="calibre20">def read_train_sets(train_path, image_size, classes, validation_size=0): 
  class DataSets(object): 
      pass 
      data_sets = DataSets() 
      images, labels, ids, cls = load_train(train_path, image_size, classes) 
      images, labels, ids, cls = shuffle(images, labels, ids, cls) <br class="title-page-name" />  
      if isinstance(validation_size, float): 
          validation_size = int(validation_size * images.shape[0]) 
          validation_images = images[:validation_size] 
          validation_labels = labels[:validation_size] 
          validation_ids = ids[:validation_size] 
          validation_cls = cls[:validation_size] 
          train_images = images[validation_size:] 
          train_labels = labels[validation_size:] 
          train_ids = ids[validation_size:] 
          train_cls = cls[validation_size:] 
          data_sets.train = DataSet(train_images, train_labels, train_ids, train_cls) 
          data_sets.valid = DataSet(validation_images, validation_labels, validation_ids, validation_cls) 
  return data_sets </pre>
<p class="calibre2">In the previous code segment, we have used the method <kbd class="calibre12">load_train()</kbd> to load the images which is an instance of a class called <kbd class="calibre12">DataSet</kbd>:</p>
<pre class="calibre20">def load_train(train_path, image_size, classes): 
    images = [] 
    labels = [] 
    ids = [] 
    cls = [] 
 
    print('Reading training images') 
    for fld in classes:    
        index = classes.index(fld) 
        print('Loading {} files (Index: {})'.format(fld, index)) 
        path = os.path.join(train_path, fld, '*g') 
        files = glob.glob(path) <br class="title-page-name" />        for fl in files: 
            image = cv2.imread(fl) 
            image = cv2.resize(image, (image_size, image_size), cv2.INTER_LINEAR) 
            images.append(image) 
            label = np.zeros(len(classes)) 
            label[index] = 1.0 
            labels.append(label) 
            flbase = os.path.basename(fl) 
            ids.append(flbase) 
            cls.append(fld) <br class="title-page-name" />    images = np.array(images) 
    labels = np.array(labels) 
    ids = np.array(ids) 
    cls = np.array(cls) 
    return images, labels, ids, cls </pre>
<p class="calibre2">The <kbd class="calibre12">DataSet</kbd> class, which is used to generate the batches of&nbsp;the training set, is as follows:</p>
<pre class="calibre20">class DataSet(object): 
   
  def next_batch(self, batch_size): 
    """Return the next `batch_size` examples from this data set.""" 
    start = self._index_in_epoch 
    self._index_in_epoch += batch_size 
    if self._index_in_epoch &gt; self._num_examples: 
      # Finished epoch 
      self._epochs_completed += 1 
      start = 0 
      self._index_in_epoch = batch_size 
      assert batch_size &lt;= self._num_examples 
    end = self._index_in_epoch 
    return self._images[start:end], self._labels[start:end], self._ids[start:end], self._cls[start:end] </pre>
<p class="calibre2">Then, similarly, we prepare the test set from the test images that are mixed (dogs and cats):</p>
<pre class="calibre20">test_images, test_ids = Preprocessor.read_test_set(test_path, img_size) </pre>
<p class="calibre2">We have the <kbd class="calibre12">read_test_set()</kbd> function for ease, as follows:</p>
<pre class="calibre20">def read_test_set(test_path, image_size): 
  images, ids  = load_test(test_path, image_size) 
  return images, ids </pre>
<p class="calibre2">Now, similar to the training set, we have a dedicated function called&nbsp;<kbd class="calibre12">load_test ()</kbd> for loading the test set, which goes as follows:</p>
<pre class="calibre20">def load_test(test_path, image_size): 
  path = os.path.join(test_path, '*g') 
  files = sorted(glob.glob(path)) 
 
  X_test = [] 
  X_test_id = [] 
  print("Reading test images") 
  for fl in files: 
      flbase = os.path.basename(fl) 
      img = cv2.imread(fl) 
      img = cv2.resize(img, (image_size, image_size), cv2.INTER_LINEAR) 
      X_test.append(img) 
      X_test_id.append(flbase) <br class="title-page-name" />  X_test = np.array(X_test, dtype=np.uint8) 
  X_test = X_test.astype('float32') 
  X_test = X_test / 255 
  return X_test, X_test_id </pre>
<p class="calibre2">Well done! We can now see some randomly selected images. For this, we have the helper function called <kbd class="calibre12">plot_images()</kbd>; it creates a figure with 3 x 3 sub-plots. So, all together, nine images will be plotted, along with their true label. It goes as follows:</p>
<pre class="calibre20">def plot_images(images, cls_true, cls_pred=None): 
    if len(images) == 0: 
        print("no images to show") 
        return  
    else: 
        random_indices = random.sample(range(len(images)), min(len(images), 9))         
        images, cls_true  = zip(*[(images[i], cls_true[i]) for i in random_indices])     
    fig, axes = plt.subplots(3, 3) 
    fig.subplots_adjust(hspace=0.3, wspace=0.3) 
    for i, ax in enumerate(axes.flat): 
        # Plot image. 
        ax.imshow(images[i].reshape(img_size, img_size, num_channels)) 
        if cls_pred is None: 
            xlabel = "True: {0}".format(cls_true[i]) 
        else: 
            xlabel = "True: {0}, Pred: {1}".format(cls_true[i], cls_pred[i]) 
        ax.set_xlabel(xlabel)         
        ax.set_xticks([]) 
        ax.set_yticks([])     
    plt.show() </pre>
<p class="calibre2">Let's get some random images and their labels from the train set:</p>
<pre class="calibre20">images, cls_true  = data.train.images, data.train.cls </pre>
<p class="calibre2">Finally, we plot the images and labels using our helper-function in the preceding code:</p>
<pre class="calibre20">  
plot_images(images=images, cls_true=cls_true) </pre>
<p class="calibre2">The preceding line of code generates the true labels of the images that are randomly selected:</p>
<div class="mce-root"><img src="images/000044.png" class="calibre41" /></div>
<div class="cdpaligncenter">Figure 6: The true labels of the images that are randomly selected</div>
<p class="calibre2">Finally, we can print the dataset statistics:</p>
<pre class="calibre20">print("Size of:") 
print("  - Training-set:tt{}".format(len(data.train.labels)))  
print("  - Test-set:tt{}".format(len(test_images))) 
print("  - Validation-set:t{}".format(len(data.valid.labels))) </pre>
<pre class="calibre20">&gt;&gt;&gt;<br class="title-page-name" />Reading training images<br class="title-page-name" /> Loading dogs files (Index: 0)<br class="title-page-name" /> Loading cats files (Index: 1)<br class="title-page-name" /> Reading test images<br class="title-page-name" /> Size of:<br class="title-page-name" /> - Training-set: 21000<br class="title-page-name" /> - Test-set: 12500<br class="title-page-name" /> - Validation-set: 4000</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-120">
        <section>

                            <header>
                    <h1 class="header-title">Step 3- Defining CNN hyperparameters</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now that we have the training and test set, it's time to define the hyperparameters for the CNN model before we start constructing. In the first and the second convolutional layers, we define the width and height of each filter, that is, <kbd class="calibre12">3</kbd>, where the number of filters is <kbd class="calibre12">32</kbd>:</p>
<pre class="calibre20">filter_size1 = 3  
num_filters1 = 32  
filter_size2 = 3  
num_filters2 = 32  </pre>
<p class="calibre2">The third convolutional layer has equal dimensions but twice the filters; that is, <kbd class="calibre12">64</kbd> filters:</p>
<pre class="calibre20">filter_size3 = 3<br class="title-page-name" />num_filters3 = 64  </pre>
<p class="calibre2"><span class="calibre10">The last two layers are fully connected layers, specifying the number of neurons:</span></p>
<pre class="calibre20">fc_size = 128     </pre>
<p class="calibre2">Now let's make the training slower for more intensive training by setting a lower value of the learning rate, as follows:</p>
<pre class="calibre20">learning_rate=1e-4  </pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-58">
        <section>

                            <header>
                    <h1 class="header-title">Step 4 &ndash; Constructing the CNN layers</h1>
                </header>
            
            <article>
                
<p class="calibre2">Once we have defined the CNN hyperparameters, the next task is to implement the CNN network. As you can guess, for our task, we will construct a CNN network having three convolutional layers, a flattened layer and two fully connected layers (refer to <kbd class="calibre12">LayersConstructor.py</kbd>). Moreover, we need to define the weight and the bias as well. Furthermore, we will have implicit max-pooling layers too. At first, let's define the weight. In the following, we have the <kbd class="calibre12">new_weights()</kbd> method that asks for the image shape and returns the truncated normal shapes:</p>
<pre class="calibre20">def new_weights(shape): 
    return tf.Variable(tf.truncated_normal(shape, stddev=0.05)) </pre>
<p class="calibre2">Then we define the biases using the <kbd class="calibre12">new_biases()</kbd> method:</p>
<pre class="calibre20">def new_biases(length): 
    return tf.Variable(tf.constant(0.05, shape=[length])) </pre>
<p class="calibre2">Now let's define a method,&nbsp;<kbd class="calibre12">new_conv_layer()</kbd>, for constructing a convolutional layer. The method takes the input batch, number of input channels, filter size, and number of filters and it also uses the max pooling (if true, we use a 2 x 2 max pooling) to construct a new convolutional layer. The workflow of the method is as follows:</p>
<ol class="calibre15">
<li class="calibre8">Define the shape of the filter weights for the convolution, which is determined by the TensorFlow API.</li>
<li class="calibre8">Create the new weights (that is, filters) with the given shape and new biases, one for each filter.</li>
<li class="calibre8">Create the TensorFlow operation for the convolution where the strides are set to 1 in all dimensions. The first and last stride must always be 1, because the first is for the image-number and the last is for the input channel. For example, strides= (1, 2, 2, 1) would mean that the filter is moved two pixels across the <em class="calibre29">x</em> axis and <em class="calibre29">y</em> axis of the image.</li>
<li class="calibre8">Add the biases to the results of the convolution. Then a bias-value is added to each filter-channel.</li>
<li class="calibre8">It then uses the pooling to downsample the image resolution. This is 2 x 2 max pooling, which means that we consider 2 x 2 windows and select the largest value in each window. Then we move two pixels to the next window.</li>
<li class="calibre8">ReLU is then used to calculate the <em class="calibre29">max(x, 0)</em> for each input pixel <em class="calibre29">x</em>. As stated earlier, a ReLU is normally executed before the pooling, but since <kbd class="calibre12">relu(max_pool(x)) == max_pool(relu(x))</kbd> we can save 75% of the relu-operations by max-pooling first.</li>
<li class="calibre8">Finally, it returns both the resulting layer and the filter-weights because we will plot the weights later.</li>
</ol>
<p class="calibre2">Now we define a function to construct the convolutional layer to be used:</p>
<pre class="calibre20">def new_conv_layer(input,  num_input_channels, filter_size, num_filters,                    use_pooling=True):   
    shape = [filter_size, filter_size, num_input_channels, num_filters] 
    weights = new_weights(shape=shape) 
    biases = new_biases(length=num_filters) 
    layer = tf.nn.conv2d(input=input, 
                         filter=weights, 
                         strides=[1, 1, 1, 1], 
                         padding='SAME') 
    layer += biases 
    if use_pooling: 
        layer = tf.nn.max_pool(value=layer, 
                               ksize=[1, 2, 2, 1], 
                               strides=[1, 2, 2, 1], 
                               padding='SAME') 
    layer = tf.nn.relu(layer) 
    return layer, weights </pre>
<p class="calibre2">The next task is to define the flattened layer:</p>
<ol class="calibre15">
<li class="calibre8">Get the shape of the input layer.</li>
<li class="calibre8">The number of features is <kbd class="calibre12">img_height * img_width * num_channels</kbd>. The <kbd class="calibre12">get_shape()</kbd> function TensorFlow is used to calculate this.</li>
<li class="calibre8">It will then reshape the layer to (<kbd class="calibre12">num_images</kbd><span><span>&nbsp;and</span></span>&nbsp;<kbd class="calibre12">num_features</kbd>). We just set the size of the second dimension to <kbd class="calibre12">num_features</kbd> and the size of the first dimension to -1, which means the size in that dimension is calculated so the total size of the tensor is unchanged from the reshaping.</li>
<li class="calibre8">Finally, it returns both the flattened layer and the number of features.</li>
</ol>
<p class="calibre2">The following code does exactly the same as described before <kbd class="calibre12">defflatten_layer(layer)</kbd>:</p>
<pre class="calibre20">    layer_shape = layer.get_shape() 
    num_features = layer_shape[1:4].num_elements() 
    layer_flat = tf.reshape(layer, [-1, num_features]) 
    return layer_flat, num_features </pre>
<p class="calibre2">Finally, we need to construct the fully connected layers. The following function, <kbd class="calibre12">new_fc_layer()</kbd>, takes the input batches, number of batches, and number of outputs (that is, predicted classes) and it uses the ReLU. It then creates the weights and biases based on the methods we define earlier in this step. Finally, it calculates the layer as the matrix multiplication of the input and weights, and then adds the bias values:</p>
<pre class="calibre20">def new_fc_layer(input, num_inputs, num_outputs, use_relu=True):  
    weights = new_weights(shape=[num_inputs, num_outputs]) 
    biases = new_biases(length=num_outputs) 
    layer = tf.matmul(input, weights) + biases 
    if use_relu: 
        layer = tf.nn.relu(layer) 
    return layer </pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-121">
        <section>

                            <header>
                    <h1 class="header-title">Step 5 &ndash; Preparing the TensorFlow graph</h1>
                </header>
            
            <article>
                
<p class="calibre2">We now create the placeholders for the TensorFlow graph:</p>
<pre class="calibre20">x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x') 
x_image = tf.reshape(x, [-1, img_size, img_size, num_channels]) 
y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true') 
y_true_cls = tf.argmax(y_true, axis=1) </pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-60">
        <section>

                            <header>
                    <h1 class="header-title">Step 6 &ndash; Creating a CNN model</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now we have the input; that is, <kbd class="calibre12">x_image</kbd> is ready to feed to the convolutional layer. We formally create a convolutional layer, followed by the max pooling:</p>
<pre class="calibre20">layer_conv1, weights_conv1 =  
    LayersConstructor.new_conv_layer(input=x_image, 
                   num_input_channels=num_channels, 
                   filter_size=filter_size1, 
                   num_filters=num_filters1, 
                   use_pooling=True) </pre>
<p class="calibre2">We must have the second convolutional layer, where the input is the first convolutional layer,&nbsp;<kbd class="calibre12">layer_conv1</kbd>, followed by the max pooling:</p>
<pre class="calibre20">layer_conv2, weights_conv2 =  
    LayersConstructor.new_conv_layer(input=layer_conv1, 
                   num_input_channels=num_filters1, 
                   filter_size=filter_size2, 
                   num_filters=num_filters2, 
                   use_pooling=True) </pre>
<p class="calibre2">We now have the third convolutional layer where the input is the output of the second convolutional layer, that is, <kbd class="calibre12">layer_conv2</kbd> followed by the max pooling:</p>
<pre class="calibre20">layer_conv3, weights_conv3 =  
    LayersConstructor.new_conv_layer(input=layer_conv2, 
                   num_input_channels=num_filters2, 
                   filter_size=filter_size3, 
                   num_filters=num_filters3, 
                   use_pooling=True) </pre>
<p class="calibre2">Once the third convolutional layer is instantiated, we then instantiate the flattened layer as follows:</p>
<pre class="calibre20">layer_flat, num_features = LayersConstructor.flatten_layer(layer_conv3) </pre>
<p class="calibre2">Once we have flattened the images, they are ready to be fed to the first fully connected layer. We use the ReLU:</p>
<pre class="calibre20">layer_fc1 = LayersConstructor.new_fc_layer(input=layer_flat, 
                         num_inputs=num_features, 
                         num_outputs=fc_size, 
                         use_relu=True) </pre>
<p class="calibre2">Finally, we have to have the second and the final fully connected layer where the input is the output of the first fully connected layer:</p>
<pre class="calibre20">layer_fc2 = LayersConstructor.new_fc_layer(input=layer_fc1, 
                         num_inputs=fc_size, 
                         num_outputs=num_classes, 
                         use_relu=False) </pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-122">
        <section>

                            <header>
                    <h1 class="header-title">Step 7 &ndash; Running the TensorFlow graph to train the CNN model</h1>
                </header>
            
            <article>
                
<p class="calibre2">The following steps are used to perform the training. The codes are self-explanatory, like the ones that we&nbsp;have already used in our previous examples. We use softmax to predict the classes by comparing them with true classes:</p>
<pre class="calibre20">y_pred = tf.nn.softmax(layer_fc2) 
y_pred_cls = tf.argmax(y_pred, axis=1) 
cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer_fc2,                                               labels=y_true) </pre>
<p class="calibre2">We define the <kbd class="calibre12">cost</kbd> function and then the optimizer (Adam optimizer in this case). Then we compute the accuracy:</p>
<pre class="calibre20">cost_op= tf.reduce_mean(cross_entropy) 
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_op) 
correct_prediction = tf.equal(y_pred_cls, y_true_cls) 
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) </pre>
<p class="calibre2">Then we initialize all the ops using the <kbd class="calibre12">global_variables_initializer()</kbd> function from TensorFlow:</p>
<pre class="calibre20">init_op = tf.global_variables_initializer() </pre>
<p class="calibre2">Then we create and run the TensorFlow session to carry the training across the tensors:</p>
<pre class="calibre20">session = tf.Session() 
session.run(init_op) </pre>
<p class="calibre2">We then feed out training data so that the batch size to 32 (see <em class="calibre19">Step 2</em>):</p>
<pre class="calibre20">train_batch_size = batch_size </pre>
<p class="calibre2">We maintain two lists to track the training and validation accuracy:</p>
<pre class="calibre20">acc_list = [] 
val_acc_list = [] </pre>
<p class="calibre2">We then count the total number of iterations performed so far and create an empty list to keep track of all the iterations:</p>
<pre class="calibre20">total_iterations = 0 
iter_list = [] </pre>
<p class="calibre2">We formally start the training by invoking the <kbd class="calibre12">optimize()</kbd> function, which takes a number of iterations. It needs two:</p>
<ul class="calibre7">
<li class="calibre8">The <kbd class="calibre12">x_batch</kbd> of training examples that holds a batch of images and</li>
<li class="calibre8"><kbd class="calibre12">y_true_batch</kbd>, the true labels for those images</li>
</ul>
<p class="calibre2">It then converts the shape of each image from (<kbd class="calibre12">num</kbd> examples, rows, columns, depth) to (<kbd class="calibre12">num</kbd> examples, flattened image shape). After that, we put the batch into a <kbd class="calibre12">dict</kbd> for placeholder variables in the TensorFlow graph. Later on, we run the optimizer on the batch of training data.</p>
<p class="calibre2">Then, TensorFlow assigns the variables in <kbd class="calibre12">feed_dict_train</kbd> to the placeholder variables. Optimizer is then executed to print the status at end of each epoch. Finally, it updates the total number of iterations that we performed:</p>
<pre class="calibre20">def optimize(num_iterations): 
    global total_iterations 
    best_val_loss = float("inf") 
    patience = 0 
    for i in range(total_iterations, total_iterations + num_iterations): 
        x_batch, y_true_batch, _, cls_batch = data.train.next_batch(train_batch_size) 
        x_valid_batch, y_valid_batch, _, valid_cls_batch = data.valid.next_batch(train_batch_size) 
        x_batch = x_batch.reshape(train_batch_size, img_size_flat) <br class="title-page-name" />        x_valid_batch = x_valid_batch.reshape(train_batch_size, img_size_flat) 
        feed_dict_train = {x: x_batch, y_true: y_true_batch}         
        feed_dict_validate = {x: x_valid_batch, y_true: y_valid_batch} 
        session.run(optimizer, feed_dict=feed_dict_train)         
 
        if i % int(data.train.num_examples/batch_size) == 0:  
            val_loss = session.run(cost, feed_dict=feed_dict_validate) 
            epoch = int(i / int(data.train.num_examples/batch_size)) 
            acc, val_acc = print_progress(epoch, feed_dict_train, feed_dict_validate, val_loss) 
            acc_list.append(acc) 
            val_acc_list.append(val_acc) 
            iter_list.append(epoch+1) 
             
            if early_stopping:     
                if val_loss &lt; best_val_loss: 
                    best_val_loss = val_loss 
                    patience = 0 
                else: 
                    patience += 1 
                if patience == early_stopping: 
                    break 
    total_iterations += num_iterations </pre>
<p class="calibre2">We will show how our training went along in the next section.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-63">
        <section>

                            <header>
                    <h1 class="header-title">Step 8 &ndash; Model evaluation</h1>
                </header>
            
            <article>
                
<p class="calibre2">We have managed to finish the training. It is time to evaluate the model. Before, we start evaluating the model, let's implement some auxiliary functions for plotting the example errors and printing the validation accuracy. The <kbd class="calibre12">plot_example_errors()</kbd> takes two parameters. The first is <kbd class="calibre12">cls_pred</kbd>, which is an array of the predicted class-number for all images in the test set.</p>
<p class="calibre2">The second parameter, <kbd class="calibre12">correct</kbd>, is a <kbd class="calibre12">boolean</kbd> array to predict whether the predicted class is equal to&nbsp;<kbd class="calibre12">true</kbd> class for each image in the test set. At first, it gets the images from the test set that have been incorrectly classified. Then it gets the predicted and the true classes for those images, and finally it plots the first nine images with their classes (that is, predicted versus true labels):</p>
<pre class="calibre20">def plot_example_errors(cls_pred, correct): 
    incorrect = (correct == False)     
    images = data.valid.images[incorrect]     
    cls_pred = cls_pred[incorrect] 
    cls_true = data.valid.cls[incorrect]     
    plot_images(images=images[0:9], cls_true=cls_true[0:9], cls_pred=cls_pred[0:9]) </pre>
<p class="calibre2">The second auxiliary function is called <kbd class="calibre12">print_validation_accuracy()</kbd>; it prints the validation accuracy. It allocates an array for the predicted classes, which will be calculated in batches and filled into this array, and then it calculates the predicted classes for the batches:</p>
<pre class="calibre20">def print_validation_accuracy(show_example_errors=False, show_confusion_matrix=False): 
    num_test = len(data.valid.images) 
    cls_pred = np.zeros(shape=num_test, dtype=np.int) 
    i = 0 
    while i &lt; num_test: 
        # The ending index for the next batch is denoted j. 
        j = min(i + batch_size, num_test) 
        images = data.valid.images[i:j, :].reshape(batch_size, img_size_flat)    
        labels = data.valid.labels[i:j, :] 
        feed_dict = {x: images, y_true: labels} 
        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict) 
        i = j 
 
    cls_true = np.array(data.valid.cls) 
    cls_pred = np.array([classes[x] for x in cls_pred])  
    correct = (cls_true == cls_pred) 
    correct_sum = correct.sum() 
    acc = float(correct_sum) / num_test 
 
    msg = "Accuracy on Test-Set: {0:.1%} ({1} / {2})" 
    print(msg.format(acc, correct_sum, num_test)) 
 
    if show_example_errors: 
        print("Example errors:") 
        plot_example_errors(cls_pred=cls_pred, correct=correct)     </pre>
<p class="calibre2">Now that we have our auxiliary functions, we can start the optimization. At the first place, let's iterate the fine-tuning 10,000 times and see the performance:</p>
<pre class="calibre20"> optimize(num_iterations=1000) </pre>
<p class="calibre2">After 10,000 iterations, we observe the following result:</p>
<pre class="calibre20">Accuracy on Test-Set: 78.8% (3150 / 4000) 
Precision: 0.793378626929 
Recall: 0.7875 
F1-score: 0.786639298213 </pre>
<p class="calibre2">This means the accuracy on the test set is about 79%. Also, let's see how well our classifier performs on a sample image:</p>
<div class="mce-root"><img src="images/000094.png" class="calibre42" /></div>
<div class="cdpaligncenter">Figure 7: Random prediction on the test set (after 10,000 iterations)</div>
<p class="calibre2">After that, we further iterate the optimization up to 100,000 times and observe better accuracy:</p>
<div class="mce-root"><img src="images/000077.png" class="calibre43" /></div>
<div class="cdpaligncenter">Figure 8: Random prediction on the test set (after 100,000 iterations)</div>
<pre class="calibre20">&gt;&gt;&gt; 
Accuracy on Test-Set: 81.1% (3244 / 4000) 
Precision: 0.811057239265 
Recall: 0.811 
F1-score: 0.81098298755 </pre>
<p class="calibre2">So it did not improve that much but was a 2% increase on the overall accuracy. Now is the time to evaluate our model for a single image. For simplicity, we will take two random images of a dog and a cat and see the prediction power of our model:</p>
<div class="mce-root"><img src="images/000070.png" class="calibre44" /></div>
<div class="cdpaligncenter">Figure 9: Example image for the cat and dog to be classified</div>
<p class="calibre2">At first, we load these two images and prepare the test set accordingly, as we have seen in an earlier step in this example:</p>
<pre class="calibre20">test_cat = cv2.imread('Test_image/cat.jpg') 
test_cat = cv2.resize(test_cat, (img_size, img_size), cv2.INTER_LINEAR) / 255 
preview_cat = plt.imshow(test_cat.reshape(img_size, img_size, num_channels)) 
 
test_dog = cv2.imread('Test_image/dog.jpg') 
test_dog = cv2.resize(test_dog, (img_size, img_size), cv2.INTER_LINEAR) / 255 
preview_dog = plt.imshow(test_dog.reshape(img_size, img_size, num_channels)) </pre>
<p class="calibre2">Then we have the following function for making the prediction:</p>
<pre class="calibre20">def sample_prediction(test_im):     
    feed_dict_test = { 
        x: test_im.reshape(1, img_size_flat), 
        y_true: np.array([[1, 0]]) 
    } 
    test_pred = session.run(y_pred_cls, feed_dict=feed_dict_test) 
    return classes[test_pred[0]] 
print("Predicted class for test_cat: {}".format(sample_prediction(test_cat))) 
print("Predicted class for test_dog: {}".format(sample_prediction(test_dog))) 
 
&gt;&gt;&gt;  
Predicted class for test_cat: cats 
Predicted class for test_dog: dogs </pre>
<p class="calibre2">Finally, when we're done, we close the TensorFlow session by invoking the <kbd class="calibre12">close()</kbd> method:</p>
<pre class="calibre20">session.close() </pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-123">
        <section>

                            <header>
                    <h1 class="header-title">Model performance optimization</h1>
                </header>
            
            <article>
                
<p class="calibre2">Since&nbsp;<span class="calibre10">CNNs are different&nbsp;</span><span class="calibre10">from the layering architecture's perspective, they have different requirements as well as tuning criteria. How do you know what combination of hyperparameters is the best for your task? Of course, you can use a grid search with cross-validation to find the right hyperparameters for linear machine learning models.</span></p>
<p class="calibre2"><span class="calibre10">However, for CNNs, there are many hyperparameters to tune, and since training a neural network on a large dataset takes a lot of time, you will only be able to explore a tiny part of the hyperparameter space in a reasonable amount of time. Here are some insights that can be followed.</span></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-66">
        <section>

                            <header>
                    <h1 class="header-title">Number of hidden layers</h1>
                </header>
            
            <article>
                
<p class="calibre2">For many problems, you can just begin with a single hidden layer and you will get reasonable results. It has actually been shown that an MLP with just one hidden layer can model even the most complex functions provided it has enough neurons. For a long time, these facts convinced researchers that there was no need to investigate any deeper neural networks. However, they overlooked the fact that deep networks have a much higher parameter efficiency than shallow ones; they can model complex functions using exponentially fewer neurons than shallow nets, making them much faster to train.</p>
<p class="calibre2">It is to be noted that this might not be always the case. However, in summary, for many problems, you can start with just one or two hidden layers. It will work just fine using two hidden layers with the same total amount of neurons, in roughly the same amount of training time. For a more complex problem, you can gradually ramp up the number of hidden layers, until you start overfitting the training set. Very complex tasks, such as large image classification or speech recognition, typically require networks with dozens of layers and a huge amount of training data.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-124">
        <section>

                            <header>
                    <h1 class="header-title">Number of neurons per hidden layer</h1>
                </header>
            
            <article>
                
<p class="calibre2">Obviously, the number of neurons in the input and output layers is determined by the type of input and output your task requires. For example, if your dataset has the shape of 28 x 28 it should expect to have input neurons with size 784 and the output neurons should be equal to the number of classes to be predicted. As for the hidden layers, a common practice is to size them to form a funnel, with fewer and fewer neurons at each layer, the rationale being that many low-level features can coalesce into far fewer high-level features. However, this practice is not as common now, and you may simply use the same size for all hidden layers.</p>
<p class="calibre2">If there are four convolutional layers with 256 neurons, that's just one hyperparameter to tune instead of one per layer. Just like the number of layers, you can try increasing the number of neurons gradually until the network starts overfitting. Another important question is: when would you want to add a max pooling layer rather than a convolutional layer with the same stride? The thing is that a max-pooling layer has no parameters at all, whereas a convolutional layer has quite a few.</p>
<p class="calibre2">Sometimes, adding a local response normalization layer that makes the neurons that most strongly activate inhibit neurons at the same location but in neighboring feature maps, encourages different feature maps to specialize and pushes them apart, forcing them to explore a wider range of features. It is typically used in the lower layers to have a larger pool of low-level features that the upper layers can build upon.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-9">
        <section>

                            <header>
                    <h1 class="header-title">Batch normalization</h1>
                </header>
            
            <article>
                
<p class="chapter-content"><strong class="calibre13">Batch normalization</strong> (<strong class="calibre13">BN</strong>) is a method to reduce internal covariate shift while training regular DNNs. This can apply to CNNs too. Due to the normalization, BN further prevents smaller changes to the parameters to amplify and thereby allows higher learning rates, making the network even faster:</p>
<div class="mce-root"><img src="images/000027.png" class="calibre45" /></div>
<p class="chapter-content">The idea is placing an additional step between the layers, in which the output of the layer before is normalized. To be more specific, in the case of non-linear operations (for example, ReLU), BN transformation has to be applied to the non-linear operation. Typically, the overall process has the following workflow:</p>
<ul class="calibre7">
<li class="calibre8">Transforming the network into a BN network (see <em class="calibre29">Figure 1</em>)</li>
<li class="calibre8">Then training the new network</li>
<li class="calibre8">Transforming the batch statistic into a population statistic</li>
</ul>
<p class="calibre2">This way, BN can fully partake in the process of backpropagation. As shown in <em class="calibre19">Figure 1</em>, BN is performed before the other processes of the network in this layer are applied. However, any kind of gradient descent (for example, <strong class="calibre13">stochastic gradient descent</strong> (<strong class="calibre13">SGD</strong>) and its variants) can be applied to train the BN network.</p>
<div class="packt_tip">Interested readers can refer to the original paper to get to more information: Ioffe, Sergey, and Christian Szegedy. <em class="calibre29">Batch normalization: Accelerating deep network training by reducing internal covariate shift</em>. arXiv preprint arXiv:1502.03167 (2015).</div>
<p class="calibre2">Now a valid question would be: where to place the BN layer? Well, to know the answer, a quick evaluation of BatchNorm layer performance on ImageNet-2012 (<a href="https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md" class="calibre11">https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md</a>) shows the following benchmark:</p>
<div class="mce-root"><img src="images/000065.png" class="calibre46" /></div>
<p class="chapter-content">From the preceding table, it can be seen that placing BN after non-linearity would be the right way. The second question would be: what activation function should be used in a BN layer? Well, from the same benchmark, we can see the following result:</p>
<div class="mce-root"><img src="images/000103.png" class="calibre47" /></div>
<p class="calibre2">From the preceding table, we can assume that using ReLU or its variants would be a better idea. Now, another question would be how to use these using deep learning libraries. Well, in TensorFlow, it is:</p>
<pre class="calibre20">training = tf.placeholder(tf.bool)<br class="title-page-name" />x = tf.layers.dense(input_x, units=100)<br class="title-page-name" />x = tf.layers.batch_normalization(x, training=training)<br class="title-page-name" />x = tf.nn.relu(x)</pre>
<p class="calibre2">A general warning: set this to <kbd class="calibre12">True</kbd> for training and <kbd class="calibre12">False</kbd> for testing. However, the preceding addition introduces extra ops to be performed on the graph, which is updating its mean and variance variables in such a way that they will not be dependencies of your training op. To do it, we can just run the ops separately, as follows:</p>
<pre class="calibre20">extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)<br class="title-page-name" />sess.run([train_op, extra_update_ops], ...)</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-125">
        <section>

                            <header>
                    <h1 class="header-title">Advanced regularization and avoiding overfitting</h1>
                </header>
            
            <article>
                
<p class="calibre2">As mentioned in the previous chapter, one of the main disadvantages observed during the training of large neural networks is overfitting, that is, generating very good approximations for the training data but emitting noise for the zones between single points. There are a couple of ways to reduce or even prevent this issue, such as dropout, early stop, and limiting the number of parameters.</p>
<p class="calibre2">In the case of overfitting, the model is specifically adjusted to the training dataset, so it will not be used for generalization. Therefore, although it performs well on the training set, its performance on the test dataset and subsequent tests is poor because it lacks the generalization property:</p>
<div class="mce-root"><img src="images/000079.png" class="calibre48" /></div>
<div class="cdpaligncenter">Figure 10: Dropout versus without dropout</div>
<p class="calibre2">The main advantage of this method is that it avoids holding all the neurons in a layer to optimize their weights synchronously. This adaptation made in random groups prevents all the neurons from converging to the same goals, thus de-correlating the adapted weights. A second property found in the dropout application is that the activation of the hidden units becomes sparse, which is also a desirable characteristic.</p>
<p class="calibre2">In the preceding figure, we have a representation of an original fully connected multilayer neural network and the associated network with the dropout linked. As a result, approximately half of the input was zeroed (this example was chosen to show that probabilities will not always give the expected four zeroes). One factor that could have surprised you is the scale factor applied to the non-dropped elements.</p>
<p class="calibre2">This technique is used to maintain the same network, and restore it to the original architecture when training, using <kbd class="calibre12">dropout_keep_prob</kbd> as 1. A major drawback of using dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected. To address this issue, there are a few techniques can be applied, such as DropConnect and stochastic pooling:</p>
<ul class="calibre7">
<li class="calibre8">DropConnect is similar to dropout as it introduces dynamic sparsity within the model, but it differs in that the sparsity is on the weights, rather than the output vectors of a layer. The thing is that a fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.</li>
<li class="calibre8">In stochastic pooling, the conventional deterministic pooling operations are replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. The approach is hyperparameter free and can be combined with other regularization approaches, such as dropout and data augmentation.</li>
</ul>
<div class="packt_infobox"><strong class="calibre1">Stochastic pooling versus standard max pooling:</strong> Stochastic pooling is equivalent to standard max pooling but with many copies of an input image, each having small local deformations.</div>
<p class="calibre2">Secondly, one of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting gets a chance to occur. This comes with the disadvantage that the learning process is halted. Thirdly, limiting the number of parameters is <span class="calibre10">sometimes&nbsp;</span>helpful and helps avoid overfitting. When it comes to CNN training, the filter size also affects the number of parameters. Thus, limiting this type of parameter restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and that limits the amount of overfitting.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-126">
        <section>

                            <header>
                    <h1 class="header-title">Applying dropout operations with TensorFlow</h1>
                </header>
            
            <article>
                
<p class="calibre2">If we apply the dropout operation to a sample vector, it will work on transmitting the dropout to all the architecture-dependent units. In order to apply the dropout operation, TensorFlow implements the <kbd class="calibre12">tf.nn.dropout</kbd> method, which works as follows:</p>
<pre class="calibre20">tf.nn.dropout (x, keep_prob, noise_shape, seed, name)</pre>
<p class="calibre2">Where <kbd class="calibre12">x</kbd> is the original tensor. The <kbd class="calibre12">keep_prob</kbd> means the probability of keeping a neuron and the factor by which the remaining nodes are multiplied. The <kbd class="calibre12">noise_shape</kbd> signifies a four-element list that determines whether a dimension will apply zeroing independently or not. Let's have a look at this code segment:</p>
<pre class="calibre20">import tensorflow as tf X = [1.5, 0.5, 0.75, 1.0, 0.75, 0.6, 0.4, 0.9] <br class="title-page-name" />drop_out = tf.nn.dropout(X, 0.5) <br class="title-page-name" />sess = tf.Session() with sess.as_default(): <br class="title-page-name" />    print(drop_out.eval()) <br class="title-page-name" />sess.close() </pre>
<pre class="calibre20">[ 3. 0. 1.5 0. 0. 1.20000005 0. 1.79999995]</pre>
<p class="calibre2">In the preceding example, you can see the results of applying dropout to the <em class="calibre19">x</em> variable, with a 0.5 probability of zero; in the cases in which it didn't occur, the values were doubled (multiplied by 1/1.5, the dropout probability).</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-127">
        <section>

                            <header>
                    <h1 class="header-title">Which optimizer to use?</h1>
                </header>
            
            <article>
                
<p class="calibre2">When using a CNN, since one of the objective functions is to minimize the evaluated cost, we must define an optimizer. Using the most common optimizer , such as SGD, the learning rates must scale with <em class="calibre19">1/T</em> to get convergence, where <em class="calibre19">T</em> is the number of iterations. Adam or RMSProp try to overcome this limitation automatically by adjusting the step size so that the step is on the same scale as the gradients. In addition, in the previous example, we have used Adam optimizer, which performs well in most cases.</p>
<p class="calibre2">Nevertheless, if you are training a neural network but computing the gradients is mandatory, using the <kbd class="calibre12">RMSPropOptimizer</kbd> function (which implements the <kbd class="calibre12">RMSProp</kbd> algorithm) is a better idea since it would be the faster way of learning in a mini-batch setting. Researchers also recommend using the momentum optimizer, while training a deep CNN or DNN. Technically, <kbd class="calibre12">RMSPropOptimizer</kbd> is an advanced form of gradient descent that divides the learning rate by an exponentially decaying average of squared gradients. The suggested setting value of the decay parameter is 0.9, while a good default value for the learning rate is 0.001. For example, in TensorFlow, <kbd class="calibre12">tf.train.RMSPropOptimizer()</kbd> helps us to use this with ease:</p>
<pre class="calibre20">optimizer = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost_op)</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-37">
        <section>

                            <header>
                    <h1 class="header-title">Memory tuning</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we try to provide some insights. We start with an issue and its solution; convolutional layers require a huge amount of RAM, especially during training, because the reverse pass of backpropagation requires all the intermediate values computed during the forward pass. During inference (that is, when making a prediction for a new instance), the RAM occupied by one layer can be released as soon as the next layer has been computed, so you only need as much RAM as required by two consecutive layers.</p>
<p class="calibre2">Nevertheless, during training, everything computed during the forward pass needs to be preserved for the reverse pass, so the amount of RAM needed is (at least) the total amount of RAM required by all layers. If your GPU runs out of memory while training a CNN, here are five things you can try to solve the problem (other than purchasing a GPU with more RAM):</p>
<ul class="calibre7">
<li class="calibre8">Reduce the mini-batch size</li>
<li class="calibre8">Reduce dimensionality using a larger stride in one or more layers</li>
<li class="calibre8">Remove one or more layers</li>
<li class="calibre8">Use 16-bit floats instead of 32-bit</li>
<li class="calibre8">Distribute the CNN across multiple devices (see more at <a href="https://www.tensorflow.org/deploy/distributed" class="calibre11"><span>https://www.tensorflow.org/deploy/distributed</span></a>)</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-36">
        <section>

                            <header>
                    <h1 class="header-title">Appropriate layer placement</h1>
                </header>
            
            <article>
                
<p class="calibre2">Another important question would be: when do you want to add a max pooling layer rather than a convolutional layer with the same stride? The thing is that a max-pooling layer has no parameters at all, whereas a convolutional layer has quite a few.</p>
<p class="calibre2">Even adding a local response normalization layer <span class="calibre10">sometimes</span> <span class="calibre10">makes the neurons that most strongly activate inhibit neurons at the same location but in neighboring feature maps, which encourages different feature maps to specialize and pushes them apart, forcing them</span> <span class="calibre10">to explore a wider range of features. It is typically used in the lower layers to have a larger pool of low-level features that the upper layers can build upon.</span></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-128">
        <section>

                            <header>
                    <h1 class="header-title">Building the second CNN by putting everything together</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now we know how to optimize the layering structure in a&nbsp;CNN by adding dropout, BN, and biases initializers, such as Xavier. Let's try to apply these to a less complex CNN. Throughout this example, we will see how to solve a real-life classification problem. To be more specific, our CNN model will be able to classify the traffic sign from a bunch of images.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-38">
        <section>

                            <header>
                    <h1 class="header-title">Dataset description and preprocessing</h1>
                </header>
            
            <article>
                
<p class="chapter-content">For this we will be using the Belgian traffic dataset (BelgiumTS for Classification (cropped images)). This dataset can be download from <a href="http://btsd.ethz.ch/shareddata/" class="calibre11">http://btsd.ethz.ch/shareddata/</a>. Here are a quick glimpse about the traffic signs convention in Belgium:</p>
<ul class="calibre7">
<li class="calibre8">Belgian traffic signs are usually in Dutch and French. This is good to know, but for the dataset that you'll be working with, it's not too important!</li>
<li class="calibre8">There are six categories of traffic signs in Belgium: warning signs, priority signs, prohibitory signs, mandatory signs, signs related to parking and standing still on the road and, lastly, designatory signs.</li>
</ul>
<p class="chapter-content">Once we download the aforementioned dataset, we will see the following directory structure (training left, test right):</p>
<div class="mce-root"><img src="images/000056.png" class="calibre49" /></div>
<p class="chapter-content">The images are in <kbd class="calibre12">.ppm</kbd> format; otherwise we could've used TensorFlow built-in image loader (example, <kbd class="calibre12">tf.image.decode_png</kbd>). However, we can use the <kbd class="calibre12">skimage</kbd> Python package.</p>
<p class="chapter-content">In Python 3, execute <kbd class="calibre12">$ sudo pip3 install scikit-image</kbd> for <kbd class="calibre12">skimage</kbd> to install and use this package. So let's get started by showing the directory path as follows:</p>
<pre class="cdpalignleft1">Train_IMAGE_DIR = "&lt;path&gt;/BelgiumTSC_Training/"<br class="title-page-name" />Test_IMAGE_DIR = ""&lt;path&gt;/BelgiumTSC_Testing/"</pre>
<p class="chapter-content">Then let's write a function using the <kbd class="calibre12">skimage</kbd> library to read the images and returns two lists:</p>
<ul class="calibre7">
<li class="calibre8"><kbd class="calibre12">images</kbd>: A list of Numpy arrays, each representing an image</li>
<li class="calibre8"><kbd class="calibre12">labels</kbd>: A list of numbers that represent the images labels</li>
</ul>
<pre class="calibre20">def load_data(data_dir):<br class="title-page-name" />    # All subdirectories, where each folder represents a unique label<br class="title-page-name" />    directories = [d for d in os.listdir(data_dir)if os.path.isdir(os.path.join(data_dir, d))]<br class="title-page-name" /><br class="title-page-name" />    # Iterate label directories and collect data in two lists, labels and images.<br class="title-page-name" />    labels = []<br class="title-page-name" />    images = []<br class="title-page-name" />    for d in directories:label_dir = os.path.join(data_dir, d)<br class="title-page-name" />    file_names = [os.path.join(label_dir, f) <br class="title-page-name" />                for f in os.listdir(label_dir) if f.endswith(".ppm")]<br class="title-page-name" /><br class="title-page-name" />    # For each label, load it's images and add them to the images list.<br class="title-page-name" />    # And add the label number (i.e. directory name) to the labels list.<br class="title-page-name" />    for f in file_names:images.append(skimage.data.imread(f))<br class="title-page-name" />    labels.append(int(d))<br class="title-page-name" />    return images, labels</pre>
<p class="calibre2">The preceding code block is straightforward and contains inline comments. How about showing related statistics about images? However, before that, let's invoke the preceding function:</p>
<pre class="calibre20"># Load training and testing datasets.<br class="title-page-name" />train_data_dir = os.path.join(Train_IMAGE_DIR, "Training")<br class="title-page-name" />test_data_dir = os.path.join(Test_IMAGE_DIR, "Testing")<br class="title-page-name" /><br class="title-page-name" />images, labels = load_data(train_data_dir)</pre>
<p class="calibre2">Then let's see some statistics:</p>
<pre class="calibre20">print("Unique classes: {0} \nTotal Images: {1}".format(len(set(labels)), len(images)))</pre>
<pre class="calibre20">&gt;&gt;&gt;<br class="title-page-name" />Unique classes: 62<br class="title-page-name" />Total Images: 4575</pre>
<p class="calibre2">So we have 62 classes to be predicted (that is, a multiclass image classification problem) and we have many images too that should be sufficient to satisfy a smaller CNN.Now let's see the class distribution visually:</p>
<pre class="calibre20"># Make a histogram with 62 bins of the `labels` data and show the plot: <br class="title-page-name" />plt.hist(labels, 62)<br class="title-page-name" />plt.xlabel('Class')<br class="title-page-name" />plt.ylabel('Number of training examples')<br class="title-page-name" />plt.show()</pre>
<div class="mce-root"><img src="images/000036.png" class="calibre50" /></div>
<p class="calibre2">Therefore, from the preceding figure, we can see that classes are very imbalanced. However, to make it simpler, we won't take care of this but next, it would be great to visually inspect some files, say displaying the first image of each label:</p>
<pre class="calibre20">def display_images_and_labels(images, labels):<br class="title-page-name" />    unique_labels = set(labels)<br class="title-page-name" />    plt.figure(figsize=(15, 15))<br class="title-page-name" />    i = 1<br class="title-page-name" />    for label in unique_labels:<br class="title-page-name" />        # Pick the first image for each label.<br class="title-page-name" />        image = images[labels.index(label)]<br class="title-page-name" />        plt.subplot(8, 8, i) # A grid of 8 rows x 8 column<br class="title-page-name" />        splt.axis('off')<br class="title-page-name" />        plt.title("Label {0} ({1})".format(label, labels.count(label)))<br class="title-page-name" />        i += 1        <br class="title-page-name" />        _= plt.imshow(image)<br class="title-page-name" />        plt.show()<br class="title-page-name" />display_images_and_labels(images, labels)</pre>
<div class="mce-root"><img src="images/000004.png" class="calibre51" /></div>
<p class="calibre2">Now you can see from the preceding figure that the images come in different sizes and shapes. Moreover, we can see it using Python code, as follows:</p>
<pre class="calibre20">for img in images[:5]:<br class="title-page-name" />    print("shape: {0}, min: {1}, max: {2}".format(img.shape, img.min(), img.max()))</pre>
<pre class="calibre20">&gt;&gt;&gt;<br class="title-page-name" />shape: (87, 84, 3), min: 12, max: 255<br class="title-page-name" />shape: (289, 169, 3), min: 0, max: 255<br class="title-page-name" />shape: (205, 76, 3), min: 0, max: 255<br class="title-page-name" />shape: (72, 71, 3), min: 14, max: 185<br class="title-page-name" />shape: (231, 228, 3), min: 0, max: 255</pre>
<p class="calibre2">Therefore, we need to apply some pre-processing such as resizing, reshaping, and so on to each image. Let's say each image will have size of 32 x 32:</p>
<pre class="calibre20">images32 = [skimage.transform.resize(img, (32, 32), mode='constant') <br class="title-page-name" /><br class="title-page-name" />for img in images]for img in images32[:5]:<br class="title-page-name" />    print("shape: {0}, min: {1}, max: {2}".format(img.shape, img.min(), img.max()))</pre>
<pre class="calibre20">&gt;&gt;&gt;<br class="title-page-name" />shape: (32, 32, 3), min: 0.06642539828431372, max: 0.9704350490196079<br class="title-page-name" />shape: (32, 32, 3), min: 0.0, max: 1.0<br class="title-page-name" />shape: (32, 32, 3), min: 0.03172870710784261, max: 1.0<br class="title-page-name" />shape: (32, 32, 3), min: 0.059474571078431314, max: 0.7036305147058846<br class="title-page-name" />shape: (32, 32, 3), min: 0.01506204044117481, max: 1.0</pre>
<p class="calibre2">Now, all of our images have same size. The next task would be to convert labels and image features as a <kbd class="calibre12">numpy</kbd> array:</p>
<pre class="calibre20">labels_array = np.array(labels)<br class="title-page-name" />images_array = np.array(images32)<br class="title-page-name" />print("labels: ", labels_array.shape, "nimages: ", images_array.shape)</pre>
<pre class="calibre20">&gt;&gt;&gt;<br class="title-page-name" />labels: (4575,)<br class="title-page-name" />images: (4575, 32, 32, 3)</pre>
<p class="calibre2">Fantastic! The next task would be creating our second CNN, but this time we will be using TensorFlow <kbd class="calibre12">contrib</kbd> package, which is a high-level API that supports layering ops.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-129">
        <section>

                            <header>
                    <h1 class="header-title">Creating the CNN model</h1>
                </header>
            
            <article>
                
<p class="calibre2">We are going to construct a complex network. However, it has a straightforward architecture. At the beginning, we use Xavier as the network initializer. Once we initialize the network bias using the&nbsp;Xavier initializer. The input layer is followed by a convolutional layer (convolutional layer 1), which is again followed by a BN layer (that is, BN layer 1). Then there is a pooling layer with strides of two and a kernel size of two. Then another BN layer follows the second convolutional layer. Next, there is the second pooling layer with strides of two and kernel size of two. Well, then the max polling layer is followed by a flattening layer that flattens the input from (None, height, width, channels) to (None, height * width * channels) == (None, 3072).</p>
<p class="calibre2">Once the flattening is completed, the input is fed into the first fully connected layer 1. Then third BN is applied as a normalizer function. Then we will have a dropout layer before we feed the lighter network into the fully connected layer 2 that generates logits of size (None, 62). Too much of a mouthful? Don't worry; we will see it step by step. Let's start the coding by creating the computational graph, creating both features, and labeling placeholders:</p>
<pre class="calibre20">graph = tf.Graph()<br class="title-page-name" />with graph.as_default():<br class="title-page-name" />    # Placeholders for inputs and labels.<br class="title-page-name" />    images_X = tf.placeholder(tf.float32, [None, 32, 32, 3]) # each image's 32x32 size<br class="title-page-name" />    labels_X = tf.placeholder(tf.int32, [None])<br class="title-page-name" /><br class="title-page-name" />    # Initializer: Xavier<br class="title-page-name" />     biasInit = tf.contrib.layers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32)<br class="title-page-name" /><br class="title-page-name" />    # Convolution layer 1: number of neurons 128 and kernel size is 6x6.<br class="title-page-name" />     conv1 = tf.contrib.layers.conv2d(images_X, num_outputs=128, kernel_size=[6, 6],     <br class="title-page-name" />            biases_initializer=biasInit)<br class="title-page-name" /><br class="title-page-name" />    # Batch normalization layer 1: can be applied as a normalizer <br class="title-page-name" />    # function for conv2d and fully_connected<br class="title-page-name" />    bn1 = tf.contrib.layers.batch_norm(conv1, center=True, scale=True, is_training=True)<br class="title-page-name" /><br class="title-page-name" />    # Max Pooling (down sampling) with strides of 2 and kernel size of 2<br class="title-page-name" />    pool1 = tf.contrib.layers.max_pool2d(bn1, 2, 2)<br class="title-page-name" /><br class="title-page-name" />    # Convolution layer 2: number of neurons 256 and kernel size is 6x6.<br class="title-page-name" />    conv2 = tf.contrib.layers.conv2d(pool1, num_outputs=256, kernel_size=[4, 4], stride=2,     <br class="title-page-name" />        biases_initializer=biasInit)<br class="title-page-name" /><br class="title-page-name" />    # Batch normalization layer 2: <br class="title-page-name" />    bn2 = tf.contrib.layers.batch_norm(conv2, center=True, scale=True, is_training=True)<br class="title-page-name" /><br class="title-page-name" />    # Max Pooling (down-sampling) with strides of 2 and kernel size of 2<br class="title-page-name" />    pool2 = tf.contrib.layers.max_pool2d(bn2, 2, 2)<br class="title-page-name" /><br class="title-page-name" />    # Flatten the input from [None, height, width, channels] to <br class="title-page-name" />    # [None, height * width * channels] == [None, 3072]<br class="title-page-name" />    images_flat = tf.contrib.layers.flatten(pool2)<br class="title-page-name" /><br class="title-page-name" />    # Fully connected layer 1<br class="title-page-name" />    fc1 = tf.contrib.layers.fully_connected(images_flat, 512, tf.nn.relu)<br class="title-page-name" /><br class="title-page-name" />    # Batch normalization layer 3<br class="title-page-name" />    bn3 = tf.contrib.layers.batch_norm(fc1, center=True, scale=True, is_training=True)<br class="title-page-name" /><br class="title-page-name" />    # apply dropout, if is_training is False, dropout is not applied<br class="title-page-name" />    fc1 = tf.layers.dropout(bn3, rate=0.25, training=True)<br class="title-page-name" /><br class="title-page-name" />    # Fully connected layer 2 that generates logits of size [None, 62]. <br class="title-page-name" />    # Here 62 means number of classes to be predicted.<br class="title-page-name" />    logits = tf.contrib.layers.fully_connected(fc1, 62, tf.nn.relu)</pre>
<p class="calibre2">Up to this point, we have managed to generate the logits of size (<kbd class="calibre12">None, 62</kbd>). Then we need to convert the logits to label indexes (<kbd class="calibre12">int</kbd>) with the shape (<kbd class="calibre12">None</kbd>), which is a 1D vector of <kbd class="calibre12">length == batch_size:predicted_labels = tf.argmax(logits, axis=1)</kbd>. Then we define cross-entropy as the <kbd class="calibre12">loss</kbd> function, which is a good choice for classification:</p>
<pre class="calibre20">loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels_X))</pre>
<p class="calibre2">Now one of the most important parts is updating the ops and creating an optimizer (Adam in our case):</p>
<pre class="calibre20">update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)<br class="title-page-name" />with tf.control_dependencies(update_ops):<br class="title-page-name" />    # Create an optimizer, which acts as the training op.train =             <br class="title-page-name" />    tf.train.AdamOptimizer(learning_rate=0.10).minimize(loss_op)</pre>
<p class="calibre2">Finally, we initialize all the ops:</p>
<pre class="calibre20">init_op = tf.global_variables_initializer()</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-41">
        <section>

                            <header>
                    <h1 class="header-title">Training and evaluating the network</h1>
                </header>
            
            <article>
                
<p class="calibre2">We start by create a session to run the graph we created. Note that for faster training, we should use a GPU. However, if you do not have a GPU, just set <kbd class="calibre12">log_device_placement=False</kbd>:</p>
<pre class="calibre20">session = tf.Session(graph=graph, config=tf.ConfigProto(log_device_placement=True))<br class="title-page-name" />session.run(init_op)<br class="title-page-name" />for i in range(300):<br class="title-page-name" />    _, loss_value = session.run([train, loss_op], feed_dict={images_X: images_array, labels_X: <br class="title-page-name" />    labels_array})<br class="title-page-name" />    if i % 10 == 0:<br class="title-page-name" />        print("Loss: ", loss_value)</pre>
<pre class="calibre20">&gt;&gt;&gt;<br class="title-page-name" />Loss: 4.7910895<br class="title-page-name" />Loss: 4.3410876<br class="title-page-name" />Loss: 4.0275432<br class="title-page-name" />...<br class="title-page-name" />Loss: 0.523456</pre>
<p class="calibre2">Once the training is completed, let us pick 10 random images and see the predictive power of our model:</p>
<pre class="calibre20">random_indexes = random.sample(range(len(images32)), 10)<br class="title-page-name" />random_images = [images32[i]<br class="title-page-name" />for i in random_indexes]<br class="title-page-name" />    random_labels = [labels[i]<br class="title-page-name" />for i in random_indexes]</pre>
<p class="calibre2">Then let's run the <kbd class="calibre12">predicted_labels op</kbd>:</p>
<pre class="calibre20">predicted = session.run([predicted_labels], feed_dict={images_X: random_images})[0]<br class="title-page-name" />print(random_labels)<br class="title-page-name" />print(predicted)</pre>
<pre class="calibre20">&gt;&gt;&gt;<br class="title-page-name" />[38, 21, 19, 39, 22, 22, 45, 18, 22, 53]<br class="title-page-name" />[20  21  19  51  22  22  45  53  22  53]</pre>
<p class="calibre2">So we can see that some images were correctly classified and some wrongly. However, visual inspection would be more helpful. So let's display the predictions and the ground truth:</p>
<pre class="calibre20">fig = plt.figure(figsize=(5, 5))<br class="title-page-name" />for i in range(len(random_images)):<br class="title-page-name" />    truth = random_labels[i]<br class="title-page-name" />    prediction = predicted[i]<br class="title-page-name" />    plt.subplot(5, 2,1+i)<br class="title-page-name" />    plt.axis('off')color='green' <br class="title-page-name" />    if truth == prediction <br class="title-page-name" />    else<br class="title-page-name" />     'red'plt.text(40, 10, "Truth: {0}nPrediction: {1}".format(truth, prediction), fontsize=12,     <br class="title-page-name" />    color=color)<br class="title-page-name" />plt.imshow(random_images[i])<br class="title-page-name" /><q class="calibre52">&gt;&gt;&gt;<br class="title-page-name" /></q></pre>
<div class="mce-root"><img src="images/000088.png" class="calibre53" /></div>
<p class="calibre2">Finally, we can evaluate our model using the test set. To see the predictive power, we compute the accuracy:</p>
<pre class="calibre20"># Load the test dataset.<br class="title-page-name" />test_X, test_y = load_data(test_data_dir)<br class="title-page-name" /><br class="title-page-name" /># Transform the images, just as we did with the training set.<br class="title-page-name" />test_images32 = [skimage.transform.resize(img, (32, 32), mode='constant') <br class="title-page-name" />for img in test_X]<br class="title-page-name" />    display_images_and_labels(test_images32, test_y)<br class="title-page-name" /><br class="title-page-name" /># Run predictions against the test <br class="title-page-name" />setpredicted = session.run([predicted_labels], feed_dict={images_X: test_images32})[0]<br class="title-page-name" /><br class="title-page-name" /># Calculate how many matches<br class="title-page-name" />match_count = sum([int(y == y_) for y, y_ in zip(test_y, predicted)])<br class="title-page-name" />accuracy = match_count / len(test_y)print("Accuracy: {:.3f}".format(accuracy))</pre>
<pre class="calibre20">&gt;&gt;<br class="title-page-name" />Accuracy: 87.583 <q class="calibre52"><br class="title-page-name" /></q></pre>
<p class="calibre2">Not that bad in terms of accuracy. In addition to this, we can also compute other performance metrics such as precision, recall, f1 measure and also visualize the result in a confusion matrix to show the predicted versus actual labels count. Nevertheless, we can still improve the accuracy by tuning the network and hyperparameters. But I leave these up to the readers.</p>
<p class="calibre2">Finally, we are done, so let's close the TensorFlow session:</p>
<pre class="calibre20">session.close()</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-130">
        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we discussed how to use CNNs, which are a type of feed-forward artificial neural network in which the connectivity pattern between neurons is inspired by the organization of an animal's visual cortex. We saw how to cascade a set of layers to construct a CNN and perform different operations in each layer. Then we saw how to train a CNN. Later on, we discussed how to optimize the CNN hyperparameters and optimization.</p>
<p class="calibre2">Finally, we built another CNN, where we utilized all the optimization techniques. Our CNN models did not achieve outstanding accuracy since we iterated both of the CNNs a few times and did not <span class="calibre10">even&nbsp;</span>apply any grid searching techniques; that means we did not hunt for the best combinations of the hyperparameters. Therefore, the takeaway would be to apply more robust feature engineering in the raw images, iterate the training for more epochs with the best hyperparameters, and observe the performance.</p>
<p class="calibre2">I<span class="calibre10">n the next chapter, we will see how to use some deeper and</span> <span class="calibre10">popular CNN architectures, such as</span> ImageNet, AlexNet, VGG, GoogLeNet, and ResNet. We will see how to utilize these trained models for transfer learning.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-43">
        <section>

                            <header>
                    <h1 class="header-title">Popular CNN Model Architectures</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, will introduce the ImageNet image database and also cover the architectures of the following popular CNN models:</p>
<ul class="calibre7">
<li class="calibre8">LeNet</li>
<li class="calibre8">AlexNet</li>
<li class="calibre8">VGG</li>
<li class="calibre8">GoogLeNet</li>
<li class="calibre8">ResNet&nbsp;</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-131">
        <section>

                            <header>
                    <h1 class="header-title">Introduction to ImageNet</h1>
                </header>
            
            <article>
                
<p class="calibre2">ImageNet is a database of over 15 million&nbsp;<span class="calibre10">hand-labeled, high-resolution&nbsp;</span>images in roughly 22,000 categories. This database is organized just like the WordNet hierarchy, where each concept is also called a&nbsp;<strong class="calibre13">synset</strong> (that is, <strong class="calibre13">synonym set</strong>). Each synset is a node in the ImageNet hierarchy. Each node has more than 500 images.&nbsp;</p>
<p class="calibre2"><span class="calibre10">The <strong class="calibre13">ImageNet Large Scale Visual Recognition Challenge</strong> (<strong class="calibre13">ILSVRC</strong>)</span> was founded in 2010 to improve state-of-the-art technology for object detection and image classification on a&nbsp;large scale:</p>
<div class="mce-root"><img src="images/000034.png" class="calibre26" /></div>
<p class="calibre2">Following this overview of ImageNet, we will now take a look at various CNN model architectures.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-44">
        <section>

                            <header>
                    <h1 class="header-title">LeNet</h1>
                </header>
            
            <article>
                
<p class="calibre2">In 2010, a&nbsp;<span class="calibre10">challenge f</span>rom ImageNet (known as <strong class="calibre13">ILSVRC 2010</strong>) came out with a CNN architecture, LeNet 5, built by Yann Lecun. This network takes a 32 x 32 image as input, which goes to the convolution layers (<strong class="calibre13">C1</strong>) and then to the subsampling layer (<strong class="calibre13">S2</strong>). Today, the subsampling layer is replaced by a pooling layer. Then, there is another sequence of convolution layers (<strong class="calibre13">C3</strong>) followed by a pooling (that is, subsampling) layer (<strong class="calibre13">S4</strong>). Finally, there are three fully connected layers, including the&nbsp;<strong class="calibre13">OUTPUT</strong> layer at the end. This network was used for zip code recognition in post offices. Since then, every year various CNN architectures were introduced with the help of this competition:</p>
<div class="mce-root"><img src="images/000002.png" class="calibre26" /></div>
<div class="cdpaligncenter">LeNet 5&nbsp;&ndash; CNN architecture from Yann Lecun's article in 1998</div>
<p class="calibre2">Therefore, we can conclude the following points:</p>
<ul class="calibre7">
<li class="calibre8">The input to this network is a grayscale 32 x 32 image</li>
<li class="calibre8">The architecture implemented is a CONV layer, followed by POOL and a fully connected layer</li>
<li class="calibre8">CONV filters are 5 x 5, applied at a stride of 1</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-132">
        <section>

                            <header>
                    <h1 class="header-title">AlexNet architecture</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre10">The first breakthrough in the architecture of CNN came in the year 2012. This award-winning CNN architecture is called <strong class="calibre13">AlexNet</strong>. It was developed at the University of Toronto by Alex Krizhevsky and his professor, Jeffry Hinton.&nbsp;</span></p>
<p class="calibre2">In the first run, a ReLU activation function and a dropout of 0.5 were used in this network to fight overfitting. As we can see in the following image, there is a normalization layer used in the architecture, but this is not used in practice anymore as it used heavy data augmentation. AlexNet is still used today even though there are more accurate networks available, because of its relative simple structure and small depth. It is widely used in computer vision:</p>
<div class="mce-root"><img src="images/000098.png" class="calibre54" /></div>
<p class="calibre2"><span class="calibre10">AlexNet is trained on the ImageNet database using two separate&nbsp;GPUs, possibly due to processing limitations with inter-GPU connections at the time, as shown in the following figure:</span></p>
<div class="mce-root"><img src="images/000058.png" class="calibre55" /></div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-76">
        <section>

                            <header>
                    <h1 class="header-title">Traffic sign classifiers using AlexNet</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this example, we will use transfer learning for feature extraction and a German traffic sign dataset to develop a classifier. Used here is an AlexNet implementation by<strong class="calibre13">&nbsp;</strong>Michael Guerzhoy and Davi Frossard, and AlexNet weights are from the Berkeley vision and Learning center. The complete code and dataset can be downloaded from&nbsp;here.</p>
<p class="calibre2">AlexNet expects a 227 x 227 x 3 pixel image, whereas the traffic sign images are 32 x 32 x 3 pixels. In order to feed the traffic sign images into AlexNet, we'll need to resize the images to the dimensions that AlexNet expects, that is, 227 x 227 x 3:</p>
<pre class="calibre20">original_image = tf.placeholder(tf.float32, (<span>None</span>, <span>32</span>, <span>32</span>, <span>3</span>))
resized_image = tf.image.resize_images(<span>original_imag</span>, (<span>227</span>, <span>227</span>))</pre>
<p class="calibre2">We can do so with the help of the <span class="calibre10"><kbd class="calibre12">tf.image.resize_images</kbd>&nbsp;method by&nbsp;</span>TensorFlow. Another issue here is that AlexNet was trained on the ImageNet dataset, which has 1,000 classes of images. So, we will replace this layer with a 43-neuron classification layer. <span class="calibre10">To do this, figure out the size of the output from the last fully connected layer; since this is a fully connected layer and so is a 2D shape, the last element will be the size of the output.</span> <kbd class="calibre12">fc7.get_shape().as_list()[-1]</kbd><span class="calibre10">&nbsp;does the trick; combine this with the number of classes for the traffic sign dataset to get the shape of the final fully connected layer:&nbsp;</span><kbd class="calibre12">shape = (fc7.get_shape().as_list()[-1], 43)</kbd><span class="calibre10">. The rest of the code is just the standard way to define a fully connected layer in TensorFlow. Finally, calculate the probabilities with <kbd class="calibre12">softmax</kbd>:</span></p>
<pre class="calibre20"><span>#Refer AlexNet implementation code, returns last fully connected layer</span>
fc7 = AlexNet(resized, feature_extract=<span>True</span>)
shape = (fc7.get_shape().as_list()[-<span>1</span>], 43)
fc8_weight = tf.Variable(tf.truncated_normal(shape, stddev=<span>1e-2</span>))
fc8_b = tf.Variable(tf.zeros(43))
logits = tf.nn.xw_plus_b(fc7, fc8_weight, fc8_b)
probs = tf.nn.softmax(logits)</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-133">
        <section>

                            <header>
                    <h1 class="header-title">VGGNet architecture</h1>
                </header>
            
            <article>
                
<p class="calibre2">The runner-up in the 2014 ImageNet challenge was VGGNet from the visual geometric group at Oxford University. This convolutional neural network is a simple and elegant architecture with a 7.3% error rate. It has two versions: VGG16 and VGG19.</p>
<p class="calibre2">VGG16 is a 16-layer neural network, not counting the max pooling layer and the softmax layer. Hence, it is known as VGG16. VGG19 consists of 19 layers. A pre-trained model is available in Keras for both Theano and TensorFlow backends.</p>
<p class="calibre2">The key design consideration here is depth. Increases in the depth of the network were achieved by adding more convolution layers, and it was done due to the small 3 x 3 convolution filters in all the layers. The default input size of an image for this model is 224 x 224 x 3. The image is passed through a stack of convolution layers with a stride of 1 pixel and padding of 1. It uses 3 x 3 convolution throughout the network. Max pooling is done over a 2 x 2 pixel window with a stride of 2, then another stack of convolution layers followed by three fully connected layers. The first two fully connected layers have 4,096 neurons each, and the third fully connected layers are responsible for classification with 1,000 neurons. The final layer is a softmax layer. VGG16 uses a much smaller 3 x 3 convolution window, compared to AlexNet's much larger 11 x 11 convolution window. All hidden layers are built with the ReLU activation function. The architecture looks like this:</p>
<div class="mce-root"><img src="images/000066.png" class="calibre56" /></div>
<div class="cdpaligncenter">VGG16 network architecture</div>
<div class="packt_tip">Due to the small 3 x 3 convolution filter, the depth of VGGNet is increased. The number of parameters in this network is approximately 140 million, mostly from the first fully connected layer. In latter-day architectures, fully connected layers of VGGNet are replaced with <strong class="calibre1">global average pooling</strong> (<strong class="calibre1">GAP</strong>) layers in order to minimize the number of parameters.</div>
<p class="calibre2">Another observation is that the number of filters increases as the image size decreases.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-48">
        <section>

                            <header>
                    <h1 class="header-title">VGG16 image classification code example</h1>
                </header>
            
            <article>
                
<p class="calibre2">The Keras Applications module has pre-trained neural network models, along with its pre-trained weights trained on ImageNet. These models can be used&nbsp;<span class="calibre10">directly</span><span class="calibre10">&nbsp;</span>for <span class="calibre10">prediction, feature extraction, and fine-tuning:</span></p>
<pre class="calibre20">#import VGG16 network model and other necessary libraries <br class="title-page-name" /><br class="title-page-name" />from keras.applications.vgg16 import VGG16<br class="title-page-name" />from keras.preprocessing import image<br class="title-page-name" />from keras.applications.vgg16 import preprocess_input<br class="title-page-name" />import numpy as np<br class="title-page-name" /><br class="title-page-name" /><span>#Instantiate VGG16 and returns a vgg16 model instance <br class="title-page-name" />vgg16_model = VGG16(weights=</span><span>'imagenet'</span><span>, include_top=</span><span>False</span><span>) <br class="title-page-name" /></span>#include_top: whether to include the 3 fully-connected layers at the top of the network.<br class="title-page-name" />#This has to be True for classification and False for feature extraction. Returns a model instance<br class="title-page-name" />#weights:'imagenet' means model is pre-training on ImageNet data.<br class="title-page-name" />model = VGG16(weights='imagenet', include_top=True)<br class="title-page-name" />model.summary()<br class="title-page-name" /><br class="title-page-name" />#image file name to classify<br class="title-page-name" />image_path = 'jumping_dolphin.jpg'<br class="title-page-name" />#load the input image with keras helper utilities and resize the image. <br class="title-page-name" />#Default input size for this model is 224x224 pixels.<br class="title-page-name" />img = image.load_img(image_path, target_size=(224, 224))<br class="title-page-name" />#convert PIL (Python Image Library??) image to numpy array<br class="title-page-name" />x = image.img_to_array(img)<br class="title-page-name" />print (x.shape)<br class="title-page-name" /><br class="title-page-name" />#image is now represented by a NumPy array of shape (224, 224, 3),<br class="title-page-name" /># but we need to expand the dimensions to be (1, 224, 224, 3) so we can<br class="title-page-name" /># pass it through the network -- we'll also preprocess the image by<br class="title-page-name" /># subtracting the mean RGB pixel intensity from the ImageNet dataset<br class="title-page-name" />#Finally, we can load our Keras network and classify the image:<br class="title-page-name" /><br class="title-page-name" />x = np.expand_dims(x, axis=0)<br class="title-page-name" />print (x.shape)<br class="title-page-name" /><br class="title-page-name" />preprocessed_image = preprocess_input(x)<br class="title-page-name" /><br class="title-page-name" />preds = model.predict(preprocessed_image)<br class="title-page-name" />print('Prediction:', decode_predictions(preds, top=2)[0])</pre>
<p class="calibre2">The first time it executes the preceding script, Keras will&nbsp;automatically<span class="calibre10">&nbsp;</span>download and cache the architecture weights to disk in the<span class="calibre10">&nbsp;</span><kbd class="calibre12"><span><span><span>~</span><span>/</span><span>.</span><span>keras</span><span>/</span><span>models</span></span></span></kbd>&nbsp;directory. Subsequent runs will be faster.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-134">
        <section>

                            <header>
                    <h1 class="header-title">GoogLeNet architecture</h1>
                </header>
            
            <article>
                
<p class="calibre2">In 2014, ILSVRC, Google published its own network known as&nbsp;<strong class="calibre13">GoogLeNet</strong>.&nbsp;Its performance is a little better than VGGNet; GoogLeNet's performance is 6.7% compared to VGGNet's performance of 7.3%. The main attractive feature of GoogLeNet is that it runs very fast due to the introduction of a new concept called <strong class="calibre13">inception module</strong>, thus reducing the number of parameters to only 5 million; that's 12 times less than AlexNet. It has lower memory use and lower power use too.</p>
<p class="calibre2"><span class="calibre10">It has 22 layers, so it is a very deep network. Adding more layers increases the number of parameters and it is likely that the network overfits. There will be more computation, because a linear increase in filters results in a quadratic increase in computation. So, the designers use the inception module and GAP. The fully connected layer at the end of the network is replaced with a GAP layer because fully connected layers are generally prone to overfitting. GAP has no parameters to learn or optimize.&nbsp;</span></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-53">
        <section>

                            <header>
                    <h1 class="header-title">Architecture insights</h1>
                </header>
            
            <article>
                
<p class="calibre2">Instead of choosing a particular filter size as in the previous architectures, the GoogLeNet designers applied all the three filters of sizes 1 x 1, 3 x 3, and 5 x 5 on the same patch, with a 3 x 3 max pooling and concatenation into a single output vector.</p>
<p class="calibre2">The use of 1 x 1 convolutions decreases the dimensions wherever the computation is increased by the expensive 3 x 3 and 5 x 5 convolutions. 1 x 1 convolutions with the ReLU activation function are used before the expensive 3 x 3 and 5 x 5 convolutions.</p>
<p class="calibre2">In GoogLeNet, inception modules are stacked one over the other. This stacking allows us to modify each module without affecting the later layers. For example, you can increase or decrease the width of any layer:</p>
<div class="mce-root"><img src="images/000003.png" class="calibre57" /></div>
<div class="cdpaligncenter">GoogLeNet architecture</div>
<p class="calibre2"><span class="calibre10">Deep networks also suffer from the fear of what is known as the&nbsp;<strong class="calibre13">vanishing gradient</strong>&nbsp;problem during backpropagation. This is avoided by adding auxiliary classifiers to intermediate layers. Also, during training, the intermediate loss was added to the total loss with a discounted factor of 0.3.</span></p>
<p class="calibre2">Since fully connected layers are prone to overfitting, it is replaced with a GAP layer. Average pooling does not exclude use of dropout, a regularization method for overcoming overfitting in deep neural networks. GoogLeNet added a linear layer after 60, a&nbsp;<span class="calibre10">GAP layer to help others swipe for their own classifier using transfer learning techniques.</span></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-135">
        <section>

                            <header>
                    <h1 class="header-title">Inception module</h1>
                </header>
            
            <article>
                
<p class="calibre2">The following image is an example of an inception module:</p>
<div class="mce-root"><img src="images/000023.png" class="calibre58" /></div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-136">
        <section>

                            <header>
                    <h1 class="header-title">ResNet architecture</h1>
                </header>
            
            <article>
                
<p class="calibre2">After a certain depth, adding additional layers to feed-forward convNets results in a higher training error and higher validation error. When adding layers, performance increases only up to a certain depth, and then it rapidly decreases. In the <strong class="calibre13">ResNet</strong> (<strong class="calibre13"><span class="calibre10">Residual Network</span></strong><span class="calibre10">)</span> paper, the authors argued that this underfitting is unlikely due to the vanishing gradient problem, because this happens even when using the batch normalization technique.<span class="calibre10">&nbsp;Therefore,&nbsp;they have added a new concept called <strong class="calibre13">residual block</strong>.&nbsp;The ResNet team added connections that can skip layers:</span></p>
<div class="packt_tip">ResNet uses standard convNet and adds connections that skip a few convolution layers at a time. Each bypass gives a residual block.</div>
<div class="mce-root"><img src="images/000016.png" class="calibre59" /></div>
<div class="cdpaligncenter">Residual block</div>
<p class="calibre2">In the 2015 ImageNet ILSVRC&nbsp;<span class="calibre10">competition, the winner was ResNet from Microsoft, with an error rate of 3.57%. ResNet is a kind of VGG in the sense that the same structure is repeated again and again to make the network deeper. Unlike VGGNet, it has different depth variations, such as 34, 50, 101, and 152 layers. It has a whopping 152 layers compared&nbsp;to AlexNet 8, VGGNet's 19 layers, and GoogLeNet's 22 layers. The ResNet architecture is a stack of residual blocks. The main idea is to skip layers by adding connections to the neural network. Every residual block has 3 x 3 convolution layers. After the last conv layer, a GAP layer is added. There is only one fully connected layer to classify 1,000 classes. It has different&nbsp;depth varieties, such as 34, 50, 101, or 152 layers for the ImageNet dataset. For a deeper network, say more than 50 layers, it uses the&nbsp;<strong class="calibre13">bottleneck</strong>&nbsp;features concept&nbsp;to improve efficiency. No dropout is used in this network.</span></p>
<p class="calibre2">Other network architectures to be aware of include:</p>
<ul class="calibre7">
<li class="calibre8">Network&nbsp;in Network</li>
<li class="calibre8">Beyond ResNet</li>
<li class="calibre8">FractalNet, an ultra-deep neural network without residuals</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-137">
        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we learned about the different CNN architectures.&nbsp;These models are pre-trained existing models and differ in network architecture. Each of these networks is designed to solve a problem specific to its architecture. So, here we described their architectural differences.</p>
<p class="calibre2">We also understood how our own CNN architecture, as defined in the previous chapter, differs from these advanced ones.</p>
<p class="calibre2">In the next chapter, we will learn how these pre-trained models can be used for transfer learning.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-57">
        <section>

                            <header>
                    <h1 class="header-title">Transfer Learning</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the previous chapter, we learned that a CNN consists of several layers. We also studied different CNN architectures, tuned different hyperparameters, and identified values for stride, window size, and padding. Then we chose a correct loss function and optimized it. We trained this architecture with a large volume of images. So, the question here is, how do we make use of this knowledge with a different dataset? Instead of building a CNN architecture and training it from scratch, it is possible to take an existing pre-trained network and adapt it to a new and different dataset through a technique called <strong class="calibre13">transfer learning</strong>.&nbsp;<span class="calibre10">We can do so through feature extraction and fine tuning.</span></p>
<div class="packt_tip">Transfer learning is the process of copying knowledge from an already trained network to a new network to solve similar problems.&nbsp;</div>
<p class="calibre2">In this chapter, we will cover the following topics:</p>
<ul class="calibre7">
<li class="calibre8">Feature extraction approach</li>
<li class="calibre8">Transfer learning example</li>
<li class="calibre8">Multi-task learning</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-52">
        <section>

                            <header>
                    <h1 class="header-title">Feature extraction approach</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre10">In a feature extraction approach, we train only the top level of the network; the rest of the network remains fixed.&nbsp;</span>Consider a feature extraction approach when the new dataset is relatively small and similar to the original dataset. In such cases, the higher-level features learned from the original dataset should transfer well to the new dataset.</p>
<p class="calibre2">Consider a fine-tuning approach when the new dataset is large and similar to the original dataset. Altering the original weights should be safe because the network is unlikely to overfit the new, large dataset.</p>
<p class="calibre2">Let us consider a pre-trained convolutional neural network, as shown in the following diagram. Using this we can study how the transfer of knowledge can be used in different situations:</p>
<div class="mce-root"><img src="images/000080.png" class="calibre60" /></div>
<p class="calibre2">When should we use transfer learning? Transfer learning can be applied in the following situations, depending on:</p>
<ul class="calibre7">
<li class="calibre8">The size of the new (target) dataset</li>
<li class="calibre8">Similarity between the original and target datasets</li>
</ul>
<p class="calibre2">There are four main use cases:</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">Case 1</strong>: New (target) dataset is small and is similar to the original training dataset</li>
<li class="calibre8"><strong class="calibre1">Case 2</strong><span>:&nbsp;</span><span>New (target) dataset is small but is different from the original training dataset</span></li>
<li class="calibre8"><strong class="calibre1">Case 3</strong><span>:&nbsp;New (target) dataset is large and is similar to the original training dataset</span></li>
<li class="calibre8"><span><strong class="calibre1">Case 4</strong>:<strong class="calibre1">&nbsp;</strong>New (target) dataset is large and is different from the original training dataset</span></li>
</ul>
<p class="calibre2">Let us now walk through each case in detail in the following sections.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-59">
        <section>

                            <header>
                    <h1 class="header-title">Target dataset is small and is similar to the original training dataset</h1>
                </header>
            
            <article>
                
<p class="calibre2">If the target dataset is small and similar to the original dataset:</p>
<ul class="calibre7">
<li class="calibre8">In this case, replace the last fully connected layer with a new fully connected layer that matches with the number of classes of the target dataset</li>
<li class="calibre8">Initialize old weights with randomized weights</li>
<li class="calibre8">Train the network to update the weights of the new, fully connected layer:</li>
</ul>
<div class="mce-root"><img src="images/000005.png" class="calibre61" /></div>
<div class="packt_tip">Transfer learning can be used as a strategy to avoid overfitting, especially when there is a small dataset.</div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-138">
        <section>

                            <header>
                    <h1 class="header-title">Target dataset is small but different from the original training dataset</h1>
                </header>
            
            <article>
                
<p class="calibre2">If the target dataset is small but of a different type to the original &ndash; for example, the original dataset is dog images and the new (target) dataset is flower images &ndash; then do the following:</p>
<ul class="calibre7">
<li class="calibre8">Slice most of the initial layers of the network</li>
<li class="calibre8">Add to the remaining pre-trained layers a new fully connected layer that matches the number of classes of the target dataset</li>
<li class="calibre8">Randomize the weights of the new fully connected layer and freeze all the weights from the pre-trained network</li>
<li class="calibre8">Train the network to update the weights of the new fully connected layer&nbsp;&nbsp;</li>
</ul>
<p class="calibre2">Since the dataset is small, overfitting is still a concern here as well. To overcome this, we will keep the weights of the original pre-trained network the same and update only the weights of the new fully connected layer:</p>
<div class="mce-root"><img src="images/000087.png" class="calibre62" /></div>
<div class="packt_tip">Only fine tune the higher level portion of the network. This is because the beginning layers are designed to extract more generic features. In general, the first layer of a convolutional neural network is not specific to a dataset.&nbsp;</div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-61">
        <section>

                            <header>
                    <h1 class="header-title">Target dataset is large and similar to the original training dataset</h1>
                </header>
            
            <article>
                
<p class="calibre2">Here we do not have an overfitting concern, as the dataset is large. So, in this case, we can retrain the entire network:</p>
<ul class="calibre7">
<li class="calibre8">Remove the last fully connected layer and replace it with a fully connected layer that matches the number of classes in the target dataset</li>
<li class="calibre8">Randomly initialize the weights of this newly added, fully connected layer</li>
<li class="calibre8">Initialize the rest of the weights with pre-trained weights</li>
<li class="calibre8">Train the entire network:</li>
</ul>
<div class="mce-root"><img src="images/000061.png" class="calibre63" /></div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-139">
        <section>

                            <header>
                    <h1 class="header-title">Target dataset is large and different from the original training dataset</h1>
                </header>
            
            <article>
                
<p class="calibre2">If the target dataset is large and different from the original:</p>
<ul class="calibre7">
<li class="calibre8">Remove the last fully connected layer and replace it with a fully connected layer that matches the number of classes in the target dataset</li>
<li class="calibre8"><span>Train the entire network from scratch&nbsp;with randomly initialized weights:</span></li>
</ul>
<div class="mce-root"><img src="images/000089.png" class="calibre64" /></div>
<div class="packt_tip">The <kbd class="calibre12">Caffe</kbd> library has ModelZoo, where one can share network weights.</div>
<p class="calibre2">Consider training from scratch when the dataset is large and completely different from the original dataset. In this case, we have enough data to train from scratch without the fear of overfitting. However, even in this case, it might be beneficial to initialize the entire network with pre-trained weights and fine tune it on the new dataset.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-65">
        <section>

                            <header>
                    <h1 class="header-title">Transfer learning example</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this example, we will take a pre-trained VGGNet and use transfer learning to train a CNN classifier that predicts dog breeds, given a dog image. Keras contains many pre-trained models, along with the code that loads and visualizes them. Another is a flower dataset that can be downloaded here. The Dog breed dataset has 133 dog breed categories and 8,351 dog images. Download the Dog breed dataset here and copy it to your folder. VGGNet has 16 convolutional with pooling layers from beginning to end and&nbsp;three fully connected layers followed by a <kbd class="calibre12">softmax</kbd> function. Its main objective was to show how the depth of the network gives the best performance. It came from <strong class="calibre13">Visual Geometric Group</strong> (<strong class="calibre13">VGG</strong>) at Oxford. Their best performing network is VGG16. The Dog breed dataset is relatively small and has a little overlap with the <kbd class="calibre12">imageNet</kbd> dataset. So, we can remove the last fully connected layer after the convolutional layer and replace it with our own. The weights of the convolutional layer are kept constant. An input image is passed through the convolutional layer and stops at the 16th layer:</p>
<div class="mce-root"><img src="images/000030.png" class="calibre65" /></div>
<div class="cdpaligncenter">VGGNet Architecture</div>
<p class="calibre2">We will use the bottleneck features of a pre-trained VGG16 network &ndash; such a network has already learned features from the&nbsp;<kbd class="calibre12">imageNet</kbd> dataset. Because the&nbsp;<kbd class="calibre12">imageNet</kbd> dataset already contains a few images of dogs, the VGG16 network model has already learned key features for classification. Similarly, other pre-trained CNN architectures can also be considered as an exercise to solve other image classification tasks.</p>
<p class="calibre2">Download the&nbsp;<kbd class="calibre12">bottleneck_features</kbd> of VGG16 here,&nbsp;copy it to your own folder, and load it:</p>
<pre class="calibre20">bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')<br class="title-page-name" />train_vgg16 = bottleneck_features['train']<br class="title-page-name" />valid_vgg16 = bottleneck_features['valid']<br class="title-page-name" />test_vgg16 = bottleneck_features['test']</pre>
<p class="calibre2">Now define the model architecture:</p>
<pre class="calibre20">from keras.layers import GlobalAveragePooling2D<br class="title-page-name" /><br class="title-page-name" />model = Sequential()<br class="title-page-name" />model.add(GlobalAveragePooling2D(input_shape=(7, 7, 512)))<br class="title-page-name" />model.add(Dense(133, activation='softmax'))<br class="title-page-name" />model.summary()</pre>
<pre class="calibre20">Layer (type)                     Output Shape          Param #     Connected to                     
=================================================================================================
globalaveragepooling2d_1 (Global (None, 512)           0           globalaveragepooling2d_input_1[0]
_________________________________________________________________________________________________
dense_2 (Dense)                  (None, 133)           68229       globalaveragepooling2d_1[0][0]   
=================================================================================================
Total params: 68,229
Trainable params: 68,229
Non-trainable params: 0
_________________________________________________________________________________________________</pre>
<p class="calibre2">Compile the model and train it:</p>
<pre class="calibre20">model.compile(loss='categorical_crossentropy', optimizer='rmsprop', <br class="title-page-name" />                  metrics=['accuracy'])<br class="title-page-name" />from keras.callbacks import ModelCheckpoint <br class="title-page-name" /><br class="title-page-name" /># train the model<br class="title-page-name" />checkpointer = ModelCheckpoint(filepath='dogvgg16.weights.best.hdf5', verbose=1, <br class="title-page-name" />                               save_best_only=True)<br class="title-page-name" />model.fit(train_vgg16, train_targets, nb_epoch=20, validation_data=(valid_vgg16, valid_targets), <br class="title-page-name" />          callbacks=[checkpointer], verbose=1, shuffle=True)</pre>
<p class="calibre2">Load the model and calculate the classification accuracy on the test set:</p>
<pre class="calibre20"># load the weights that yielded the best validation accuracy<br class="title-page-name" />model.load_weights('dogvgg16.weights.best.hdf5')<br class="title-page-name" /># get index of predicted dog breed for each image in test set<br class="title-page-name" />vgg16_predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) <br class="title-page-name" />                     for feature in test_vgg16]<br class="title-page-name" /><br class="title-page-name" /># report test accuracy<br class="title-page-name" />test_accuracy = 100*np.sum(np.array(vgg16_predictions)==<br class="title-page-name" />                           np.argmax(test_targets, axis=1))/len(vgg16_predictions)<br class="title-page-name" />print('\nTest accuracy: %.4f%%' % test_accuracy)</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-140">
        <section>

                            <header>
                    <h1 class="header-title">Multi-task learning</h1>
                </header>
            
            <article>
                
<p class="calibre2">In multi-task learning, transfer learning happens to be from one pre-trained model to many tasks simultaneously. For example, in self-driving cars, the deep neural network detects traffic signs, pedestrians, and other cars in front at the same time. Speech recognition also benefits from multi-task learning.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-141">
        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">In a few specific cases, convolutional neural network architectures trained on images allow us to reuse learned features in a new network. The performance benefits of transferring features decrease the more dissimilar the base task and target task are.&nbsp;It is surprising to know that initializing a convolutional neural network with transferred features from almost any number of layers can produce a boost to generalization performance after fine-tuning to a new dataset.</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2">&nbsp;</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-142">
        <section>

                            <header>
                    <h1 class="header-title">Autoencoders for CNN</h1>
                </header>
            
            <article>
                
<p class="calibre2">&nbsp;In this chapter, we will cover the following topics:</p>
<ul class="calibre7">
<li class="calibre8">Introducing to Autoencoders</li>
<li class="calibre8">Convolutional Autoencoder</li>
<li class="calibre8">Applications of Autoencoders</li>
<li class="calibre8">An example of compression</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-70">
        <section>

                            <header>
                    <h1 class="header-title">Introducing to autoencoders</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre10">An</span> autoencoder <span class="calibre10">is a regular neural network, an unsupervised learning model that takes an input and produces the same input in the output layer. So, there is no associated label in the training data. Generally, an autoencoder consists of two parts:</span></p>
<ul class="calibre7">
<li class="calibre8"><span>Encoder network</span></li>
<li class="calibre8"><span>Decoder network</span></li>
</ul>
<p class="calibre2"><span class="calibre10">It learns all the required features from unlabeled training data, which is known as lower dimensional feature representation. In the following figure, the input data (<em class="calibre19">x</em>) is passed through an encoder that produces a compressed representation of&nbsp;the input data. Mathematically, in the equation,&nbsp;</span><em class="calibre19">z = h(x)</em>,<em class="calibre19">&nbsp;</em><em class="calibre19">z</em> is a feature vector, and is usually a smaller dimension than <em class="calibre19">x</em>.</p>
<p class="calibre2">Then, we take these produced features from the input data and pass them through a decoder network to reconstruct the original data.&nbsp;</p>
<p class="calibre2">An encoder can be a fully connected neural network or a&nbsp;<strong class="calibre13">convolutional neural network</strong> (<strong class="calibre13">CNN</strong>). A&nbsp;<span class="calibre10">decoder also uses the same kind of network as an encoder.&nbsp;</span>Here, we've explained and implemented the encoder and decoder function using ConvNet:</p>
<div class="mce-root"><img src="images/000045.png" class="calibre66" /></div>
<p class="calibre2">Loss function: <em class="calibre19">||x - x||<sup class="calibre31">2</sup></em></p>
<div class="packt_tip">In this network, the size of the input and the output layers is the same.</div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-143">
        <section>

                            <header>
                    <h1 class="header-title">Convolutional autoencoder</h1>
                </header>
            
            <article>
                
<p class="calibre2">A&nbsp;convolutional&nbsp;autoencoder is a neural network (a special case of an unsupervised learning model) that is trained to reproduce its input image in the output layer. An image is passed through an encoder, which is a ConvNet that produces a low-dimensional representation of the image. The decoder, which is another sample ConvNet, takes this compressed image and reconstructs the original image.</p>
<p class="calibre2">The encoder is used to compress the data and the decoder is used to reproduce the original image. Therefore, autoencoders may be used for data, compression. Compression logic is data-specific, meaning it is learned from data rather than predefined compression algorithms such as JPEG, MP3, and so on. Other applications of autoencoders can be image denoising (producing a cleaner image from a corrupted image), dimensionality reduction, and image search:</p>
<div class="mce-root"><img src="images/000018.png" class="calibre67" /></div>
<p class="calibre2">This differs from regular ConvNets or neural nets in the sense that the input size and the target size must be the same.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-72">
        <section>

                            <header>
                    <h1 class="header-title">Applications</h1>
                </header>
            
            <article>
                
<p class="calibre2">Autoencoders are used for dimensionality reduction, or data compression, and image denoising. Dimensionality reduction, in turn, helps in improving runtime performance and consumes less memory. An image search can become highly efficient in low-dimension spaces.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-144">
        <section>

                            <header>
                    <h1 class="header-title">An example of compression</h1>
                </header>
            
            <article>
                
<p class="calibre2">The Network architecture comprises of an<span class="calibre10">&nbsp;encoder network, which is a typical convolutional pyramid. Each convolutional layer is followed by a max-pooling layer; this reduces the dimensions of the layers.&nbsp;</span></p>
<p class="calibre2"><span class="calibre10">The decoder converts the input from a sparse representation to a wide reconstructed image. A schematic of the network is shown here:</span></p>
<div class="mce-root"><img src="images/000095.png" class="calibre68" /></div>
<p class="calibre2">The encoder layer output image size is 4 x 4 x 8 = 128. The original image size was 28 x 28 x 1 = 784, so the compressed image vector is roughly 16% of the size of the original image.&nbsp;</p>
<p class="calibre2"><span class="calibre10">&nbsp;Usually, you'll see&nbsp;</span>transposed convolution<span class="calibre10">&nbsp;layers used to increase the width and height of the layers. They work almost exactly the same as convolutional layers but in reverse. A stride in the input layer results in a larger stride in the transposed convolution layer. For example, if you have a 3 x 3 kernel, a 3 x 3 patch in the input layer will be reduced to one unit in a convolutional layer. Comparatively, one unit in the input layer will be expanded into a 3 x 3 path in a transposed convolution layer. The TensorFlow API provides us with an easy way to create the layers:&nbsp;</span><kbd class="calibre12">tf.nn.conv2d_transpose</kbd><span class="calibre10">, click here,&nbsp;<a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose" target="_blank" class="calibre11">https://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose</a>.</span></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-69">
        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">We began this chapter with a short introduction to autoencoders, and we implemented the encoder and decoder function with the help of ConvNets.&nbsp;</p>
<p class="calibre2">We then moved to convolutional autoencoders and learned how they are different from regular ConvNets and neural nets.</p>
<p class="calibre2">We walked through the different applications of autoencoders, with an example, and saw how an autoencoder enhances the efficiency of image searches in low-dimension spaces.&nbsp;</p>
<p class="calibre2">In the next chapter, we will study object detection with CNNs and learn the difference between object detection and object classification.</p>
<p class="calibre2"></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-31">
        <section>

                            <header>
                    <h1 class="header-title">Object Detection and Instance Segmentation with CNN</h1>
                </header>
            
            <article>
                
<p class="calibre2">Until now, in this book, we have been mostly using <strong class="calibre13">convolutional neural networks</strong> (<strong class="calibre13">CNNs</strong>) for&nbsp;classification. Classification classifies the whole image into one of the classes with respect to the entity having the maximum probability of detection in the image. But what if there is not one, but multiple entities of interest and we want to have the image associated with all of them? One way to do this is to use tags instead of classes, where these tags are all classes of the penultimate Softmax classification layer with probability above a given threshold. However, the probability of detection here varies widely by size and placement of entity, and from the following image, we can actually say, <em class="calibre19">How confident is the model that the identified entity is the one that is claimed?</em>&nbsp;What if we are very confident that there is an entity, say a dog, in the image, but its scale and position in the image is not as prominent as that of its owner, a <em class="calibre19">Person</em>&nbsp;entity? So, a <em class="calibre19">Multi-Class Tag</em>&nbsp;is a valid way but not the best for this purpose:</p>
<div class="mce-root"><img src="images/00069.jpeg" class="calibre69" /></div>
<p class="calibre2">In this chapter, we will cover the following topics:</p>
<ul class="calibre7">
<li class="calibre8"><span>The differences between object detection and image classification</span></li>
<li class="calibre8">Traditional, non-CNN approaches for object detection</li>
<li class="calibre8">Region-based CNN and its features</li>
<li class="calibre8">Fast R-CNN</li>
<li class="calibre8">Faster R-CNN</li>
<li class="calibre8">Mask R-CNN</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-145">
        <section>

                            <header>
                    <h1 class="header-title">The differences between object detection and image classification</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let's take another example. You are watching the movie <em class="calibre19">101 Dalmatians</em>, and you want to know how many Dalmatians you can actually count in a given movie scene from that movie. Image Classification could, at best, tell you that there is at least one dog or one <em class="calibre19">Dalmatian</em> (depending upon which level you have trained your classifier for), but not exactly how many of them there are.</p>
<p class="calibre2"><span class="calibre10">Another issue with classification-based models is that they do not tell you where&nbsp;</span><span class="calibre10">the identified entity&nbsp;</span><span class="calibre10">in the image is. Many times, this is very important. Say, for example, you saw your neighbor's dog<em class="calibre19">&nbsp;</em>playing with him (<em class="calibre19">Person</em>) and his cat. You took a snap of them and wanted to extract the image of the dog from there to search on the web for its breed or similar dogs like it. The only problem here is that searching the whole image might not work, and without identifying individual objects from the image, you have to do the cut-extract-search job manually for this task, as shown in the following image:</span></p>
<div class="mce-root"><img src="images/00051.jpeg" class="calibre70" /></div>
<p class="calibre2">So, you essentially need a technique that not only identifies the entities in an image but also tells&nbsp;you&nbsp;their placement in the image. This is what is called <strong class="calibre13">object detection</strong>. Object detection gives you bounding boxes and class labels (along with the probability of detection) of all the entities identified in an image. The output of this system can be used to empower multiple advanced use cases that work on the specific class of the objects detected.</p>
<p class="calibre2">Take, for example, the Facial Recognition feature that you have in Facebook, Google Photos, and many other similar apps. In it, before you identify <em class="calibre19">who is</em>&nbsp;there in an image taken in at a party, you need to detect all the faces in that image; then you can pass these faces through your face recognition/classification module to get/classify their names. So, the Object nomenclature in object detection is not limited to linguistic entities but includes anything that has specific boundaries and enough data to train the system, as shown in the following image:</p>
<div class="mce-root"><img src="images/00024.jpeg" class="calibre71" /></div>
<p class="calibre2">Now, if you want to find out how many <span class="calibre10">of the guests present at your party&nbsp;</span>were actually <strong class="calibre13">enjoying</strong> it, you can even run an object detection for <strong class="calibre13">Smiling Faces</strong> or a <strong class="calibre13">Smile Detector</strong>. There are very powerful and efficient trained models of object detectors available for most of the detectable human body parts (eye, face, upper body, and so on), popular human expressions (such as a smile), and many other general objects as well. So, the next time you use the <strong class="calibre13">Smile Shutter</strong> on your smartphone (a feature made to automatically click the image when most of the faces in <span class="calibre10">the scene&nbsp;</span>are detected as smiling), you know what is powering this feature.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-146">
        <section>

                            <header>
                    <h1 class="header-title">Why is object detection much more challenging than image classification?</h1>
                </header>
            
            <article>
                
<p class="calibre2">From our understanding of CNN and image classification so far, let's try to understand how we can approach the object detection problem, and that should logically lead us to the discovery of the underlying complexity and challenges. Assume we are dealing with monochromatic images for simplicity.</p>
<p class="calibre2">Any object detection at a high level may be considered a combination of two tasks (we will refute this later):</p>
<ul class="calibre7">
<li class="calibre8">Getting the right bounding boxes (or as many of them to filter later)</li>
<li class="calibre8">Classifying the object in that bounding box (while returning the classification effectiveness for filtering)</li>
</ul>
<p class="calibre2">So, object detection not only has to cater to all the challenges of image classification (second objective), but also faces new challenges of finding the right, or as many as possible, bounding boxes. As we already know how to use CNNs for the purpose of image classification, and the associated challenges, we can now concentrate on our first task and explore how effective (classification accuracy) and efficient (computational complexity) our approach&nbsp;is&mdash;or rather how challenging this task is going to be.</p>
<p class="calibre2">So, we start with randomly generating bounding boxes from the image. Even if we do not worry about the computational load of generating so many candidate boxes, technically termed as <strong class="calibre13">Region Proposals</strong>&nbsp;(regions&nbsp;that we send as proposals for classifying objects), we still need to have some mechanism for finding the best values for the following parameters:</p>
<ul class="calibre7">
<li class="calibre8">Starting (or center) coordinates to extract/draw the candidate&nbsp;<span>bounding</span> box</li>
<li class="calibre8">Length of the candidate bounding box</li>
<li class="calibre8">Width of the&nbsp;<span>candidate bounding box</span></li>
<li class="calibre8">Stride across each axis (distance from one starting location to another in the <em class="calibre29">x</em>-horizontal axis and&nbsp;<span><em class="calibre29">y</em>-vertical</span>&nbsp;axis)</li>
</ul>
<p class="calibre2">Let's assume that we can generate such an algorithm that can give us the most optimal value of these parameters. Still, will one value for these parameters work in most of the cases, or in fact, in some general cases? From our experience, we know that each object will have a different scale, so we know that one fixed value for <em class="calibre19">L</em> and <em class="calibre19">W</em> for these boxes will not work. Also, we can understand that the same object, say Dog, may be present in varying proportions/scales and positions in different images, as in some of our earlier examples. So this confirms our belief that we need boxes of not only different scales but also different sizes.&nbsp;</p>
<p class="calibre2">Let's assume that, correcting from the previous analogy, we want to extract <em class="calibre19">N</em> number of candidate boxes per starting coordinate in the image, where <em class="calibre19">N</em> encompasses most of the sizes/scales that may fit our classification problem. Although that seems to be a rather challenging job in itself, let's assume we have that magic number and it is far from a combination of <em class="calibre19">L[1,l-image] x W[1,w-image]</em> (all combinations of <em class="calibre19">L</em> and <em class="calibre19">W</em> where length is a set of all integers between 1 and the length of the actual image and breadth is from 1 to the breadth of the image); that will lead us to <em class="calibre19">l*w</em> boxes per coordinate:</p>
<div class="mce-root"><img src="images/00104.jpeg" class="calibre72" /></div>
<p class="calibre2">Then, another question is about how many starting coordinates we need to visit in our image from where we will extract these <em class="calibre19">N</em> boxes each, or the Stride. Using a very big stride will lead us to extract sub-images in themselves, instead of a single homogeneous object that can be classified effectively and used for the purpose of achieving some of the objectives in our earlier examples. Conversely, too short a stride (say, 1 pixel in each direction) may mean a lot of candidate boxes.</p>
<p class="calibre2">From the preceding illustration, we can understand that even after hypothetically relaxing most of the constraints, we are nowhere close to making a system that we can fit in our Smartphones to detect smiling selfies or even bright faces in real time (even after an hour in fact). Nor can it have our robots and self-driving cars identify <span class="calibre10">objects as they move&nbsp;</span>(and navigate their way by avoiding them). This intuition should help us appreciate the advancements in the field of object detection and why it is such an impactful area of work.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-147">
        <section>

                            <header>
                    <h1 class="header-title">Traditional, nonCNN approaches to object detection</h1>
                </header>
            
            <article>
                
<p class="calibre2">Libraries such as OpenCV and some others saw rapid inclusion in the software bundles for Smartphones, Robotic projects, and many others, to provide detection capabilities of specific objects (face, smile, and so on), and Computer Vision like benefits, though with some constraints even before the prolific adoption of CNN.</p>
<p class="calibre2">CNN-based research in this area of object detection and Instance Segmentation provided many advancements and performance enhancements to this field, not only enabling large-scale deployment of these systems but also opening avenues for many new solutions. But before we plan to jump into CNN based advancements, it will be a good idea to understand how the challenges cited in the earlier section were answered to make object detection possible in the first place (even with all the constraints), and then we will logically start our discussion about the different researchers and the application of CNN to solve other problems that still persist with the use of traditional approaches.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-148">
        <section>

                            <header>
                    <h1 class="header-title">Haar features, cascading classifiers, and the Viola-Jones algorithm</h1>
                </header>
            
            <article>
                
<p class="calibre2">Unlike CNN, or the deepest learning for that matter, which is known for its capability of generating higher conceptual features automatically, which in-turn gives a major boost to the classifier, in case of traditional machine learning applications, such features need to be hand crafted by SMEs.</p>
<p class="calibre2">As we may also understand from our experience working on CPU-based machine learning classifiers, their performance is affected by high dimensionality in data and the availability of too many features to apply to the model, especially with some of the very popular and sophisticated classifiers such as&nbsp;<strong class="calibre13">Support Vector Machines</strong> (<strong class="calibre13">SVM</strong>), which used to be considered state-of-the-art until some time ago.&nbsp;</p>
<p class="calibre2">In this section, we will understand some of the innovative ideas <span class="calibre10">drawing inspirations from different fields of science and mathematics&nbsp;</span>that led to the resolution of some of the cited challenges above, to fructify the concept of real-time object detection in non-CNN systems.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-149">
        <section>

                            <header>
                    <h1 class="header-title">Haar Features</h1>
                </header>
            
            <article>
                
<p class="calibre2">Haar or Haar-like features are formations of rectangles with varying pixel density. Haar features sum up the pixel intensity in the adjacent rectangular regions at specific locations in the detection region. Based on the difference between the sums of pixel intensities across regions, they categorize the different subsections of the image.</p>
<div class="packt_infobox">Haar-like features have their name attributed to the mathematics term of Haar wavelet, which<span>&nbsp;is a sequence of rescaled square-shaped functions that together form a wavelet&nbsp;</span><span>family or basis.&nbsp;</span></div>
<div class="packt_tip">Because Haar-like features work on the difference between pixel intensities across regions, they work best with monochrome images. This is also the reason the images used earlier and in also this section are monochrome for better intuition.</div>
<p class="calibre2">These categories can be grouped into three major groups, as follows:</p>
<ul class="calibre7">
<li class="calibre8">Two rectangle features</li>
<li class="calibre8">Three rectangle features</li>
<li class="calibre8">Four rectangle features</li>
</ul>
<div class="mce-root"><img src="images/000082.png" class="calibre73" /></div>
<div class="cdpaligncenter">Haar-like Features</div>
<p class="calibre2">With some easy tricks, the computation of varying intensities across the image becomes very efficient and can be processed at a very high rate in real time.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-0">
        <section>

                            <header>
                    <h1 class="header-title">Cascading classifiers</h1>
                </header>
            
            <article>
                
<p class="calibre2">Even if we can extract Haar features from a particular region very quickly, it does not solve the problem of extracting such features from a lot of different places in the image; this is where the concept of cascading features comes in to help. It was observed that only 1 in 10,000 sub-regions turns positive for faces in classification, but we have to extract all features and run the whole classifier across all regions. Further, it was observed that <span class="calibre10">by using&nbsp;</span>just a few of the features (two in the first layer of the cascade), the classifier could eliminate a very high proportion of the regions (50% in the first region of the cascade). Also, if the sample consists of <span class="calibre10">just&nbsp;</span>these reduced region samples, then only slightly more features (10 features in the second layer of the cascade) are required for a classifier that can weed out a lot more cases, and so on. So we do classification in layers, starting with a classifier that requires very low computational power to weed out most of the subregions, gradually increasing the computation load required for the remaining subset, and so on.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-150">
        <section>

                            <header>
                    <h1 class="header-title">The Viola-Jones algorithm</h1>
                </header>
            
            <article>
                
<p class="calibre2">In 2001,&nbsp;<span class="calibre10">Paul Viola and Michael Jones proposed a solution that could work well to answer some of the preceding challenges, but with some constraints. Though it is an almost two decades old algorithm, some of the most popular computer vision software to date, or at least till recently, used to embed it in some form or another. This fact makes it very important to understand this very simple, yet powerful, algorithm before we move on to CNN-based approaches for Region Proposal.</span></p>
<div class="packt_infobox">OpenCV, one of the most popular software libraries for computer vision, uses cascading classifiers as the predominant mode for object detection, and Haar-featuring-like Cascade classifier is very popular with OpenCV. A lot of pretrained Haar classifiers are available for this for multiple types of general objects.</div>
<p class="calibre2"><span class="calibre10">This algorithm is not only capable of delivering detections with high <strong class="calibre13">TPRs</strong> (<strong class="calibre13">True Positive Rates</strong>) and low <strong class="calibre13">FPRs</strong> (<strong class="calibre13">False Positive Rates</strong>), it can also work in real time (process at least two frames per second).</span></p>
<div class="packt_tip">High TPR combined with Low FPR is a very important criterion for determining the robustness of an algorithm.</div>
<p class="calibre2"><span class="calibre10">The constraints of their proposed algorithm were the following:</span></p>
<ul class="calibre7">
<li class="calibre8">It could work only for detecting, not recognizing faces (they proposed the algorithm for faces, though the same could be used for many other objects).</li>
<li class="calibre8">The faces had to be present in the image as a frontal view. No other view could be detected.</li>
</ul>
<p class="calibre2">At the heart of this algorithm are the Haar (like) Features and Cascading Classifiers. Haar Features are described later in a subsection. The Viola-Jones algorithm uses a subset of Haar features to determine general features on a face such as:</p>
<ul class="calibre7">
<li class="calibre8">Eyes (determined by a two-rectangle feature (<span>horizontal)</span>, with a dark horizontal rectangle above the eye forming the brow, followed by a lighter rectangle below)</li>
<li class="calibre8">Nose (three-rectangle feature <span>(vertical)</span>, with the nose as the center light rectangle and one darker rectangle on either side on the nose, forming the temple), and so on</li>
</ul>
<p class="calibre2">These fast-to-extract features can then be used to make a classifier to detect (distinguish) faces (from non-faces).&nbsp;</p>
<div class="packt_tip"><span>Haar features, with some tricks, are very fast to compute.</span></div>
<div class="mce-root"><img src="images/000057.jpg" class="calibre74" /></div>
<div class="cdpaligncenter">Viola-Jones algorithm and Haar-like Features for detecting faces</div>
<p class="calibre2"><span class="calibre10">These Haar-like features are then used in the cascading classifiers to expedite the detection problem without losing the robustness of detection.</span></p>
<p class="calibre2"><span class="calibre10">The Haar Features and cascading classifiers thus led to some of the very robust, effective, and fast individual object detectors of the previous generation. But still, the training of these cascades for a new object was very time consuming, and they had a lot of constraints, as mentioned before. That is where the new generation CNN-based object detectors come to the rescue.</span></p>
<div class="packt_tip">In this chapter, we have covered only the basis of Haar-Cascades or Haar features (in the non-CNN category) as they remained predominant for a long time and were the basis of many new types. Readers are encouraged to also explore some of the later and much effective SIFT and HOG-based features/cascades (associated papers are given in the <em class="calibre29">References</em> section).&nbsp;</div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-2">
        <section>

                            <header>
                    <h1 class="header-title">R-CNN&nbsp;&ndash;&nbsp;Regions with CNN features</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the <span class="calibre10">'Why is object detection much more challenging than image classification?'</span> section, we used a non-CNN method to draw region proposals and CNN for classification, and we realized that this is not going to work well because the regions generated and fed into CNN were not optimal. R-CNN or regions with CNN features, as the name suggests, flips that example completely and use CNN to generate features that are classified using a (non-CNN) technique called <strong class="calibre13">SVM</strong> (<strong class="calibre13">Support Vector Machines</strong>)</p>
<p class="calibre2">R-CNN uses the sliding window method (much like we discussed earlier, taking some <em class="calibre19">L x W</em> and stride) to generate around 2,000 regions of interest, and then it converts them into features for classification using CNN. Remember what we discussed <span class="calibre10">in the transfer learning chapter&mdash;</span>the last flattened layer (<span class="calibre10">before the classification or softmax layer</span>) can be extracted to transfer learning from models trained on generalistic data, and further train them (often requiring much less data as compared to a model with similar performance that has been trained from scratch using domain-specific data) to model domain-specific models. R-CNNs also use a similar mechanism to improve their effectiveness on specific object detection:</p>
<div class="mce-root"><img src="images/000037.png" class="calibre75" /></div>
<div class="cdpaligncenter">R-CNN&nbsp;&ndash; Working&nbsp;</div>
<div class="packt_infobox">The original paper on R-CNN claims that on a PASCAL VOC 2012 dataset, it has improved the <strong class="calibre1">mean average precision</strong> (<strong class="calibre1">mAP</strong>) by more than 30% relative to the previous best result on that data while achieving a mAP of 53.3%.</div>
<div class="packt_tip">We saw very high precision figures for&nbsp;the&nbsp;image classification exercise (using CNN) over the ImageNet data. Do not use that figure with the comparison statistics given here, as not only are <span>the datasets used&nbsp;</span>different (and hence not comparable), but also the tasks in hand (classification versus object detection) are quite different, and object detection is much more challenging a task than image classification.</div>
<div class="packt_infobox">PASCAL <strong class="calibre1">VOC</strong> (<strong class="calibre1">Visual Object Challenge</strong>): Every area of research requires some sort of standardized dataset and standard KPIs to compare results across different studies and algorithms. Imagenet, the dataset we used for image classification, cannot be used as a standardized dataset for object detection, as object-detection requires (train, test, and validation set) data labeled with not only the object class but also its position. ImageNet does not provide this. Therefore, in most object detection studies, we may see the use of a standardized object-detection dataset, such as PASCAL VOC. The PASCAL VOC dataset has 4 variants so far, VOC2007, VOC2009, VOC2010, and VOC2012. VOC2012 is the latest (and richest) of them all.</div>
<p class="calibre2">Another place we stumbled at was the differing scales (and location) of the regions of interest, <em class="calibre19">recognition using region</em>. This is what is called the <strong class="calibre13">localization</strong> challenge; it is solved in R-CNN by using a varying range of receptive fields, starting from as high a region with 195 x 195 pixels and 32 x 32 strides, to lesser downwards.</p>
<div class="packt_tip">This approach is called <span><strong class="calibre1">recognition using region</strong>.</span></div>
<p class="calibre2">Wait a minute! Does that ring a bell? We said that we will use CNN to generate features from this region, but CNN uses a constant-size input to produce a fixed-size flattened layer. We do require fixed-size features (flattened vector size) as input to our SVMs, but here the input region size is changing. So how does that work? R-CNN uses a popular technique called <strong class="calibre13">Affine Image Warping</strong>&nbsp;to compute a fixed-size CNN input from each region proposal, regardless of the region's shape.</p>
<div class="packt_tip">In geometry, an&nbsp;affine transformation is the name given to a transformation function between affine spaces that preserves points, straight lines,&nbsp;<span>and&nbsp;</span>planes. Affine spaces are structures<span>&nbsp;that generalize the properties of Euclidian spaces while preserving&nbsp;</span><span>only the properties related to parallelism</span><span>&nbsp;and respective scale</span><span>.</span></div>
<p class="calibre2">Besides the challenges that we have covered, there exists another challenge that is worth mentioning. The candidate regions that we generated in the first step (on which we performed classification in the second step) were not very accurate, or they were lacking tight boundaries around the object identified. So we include a third stage in this method, which improves the accuracy of the bounding boxes by running a regression function (called <strong class="calibre13">bounding-box regressors</strong>) to identify the boundaries of separation.</p>
<p class="calibre2">R-CNN proved to be very successful when compared to the earlier end-to-end non-CNN approaches. But it uses CNN only for converting regions to features. As we understand, CNNs are very powerful for image classifications as well, but because our CNN will work only on input region images and not on flattened region features, we cannot use it here directly. In the next section, we will see how to overcome this obstacle.&nbsp;</p>
<div class="packt_infobox">R-CNN is very important to cover from the perspective of understanding the background use of CNN in object detection as it has been a giant leap from <span>all&nbsp;</span>non-CNN-based approaches. But because of further improvements in CNN-based object detection, as we will discuss next, R-CNN is not actively worked&nbsp;upon now and the code is not maintained any longer.&nbsp;</div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-151">
        <section>

                            <header>
                    <h1 class="header-title">Fast R-CNN&nbsp;&ndash;&nbsp;fast region-based CNN</h1>
                </header>
            
            <article>
                
<p class="calibre2">Fast R-CNN, or&nbsp;Fast Region-based CNN method, is an improvement over the previously covered R-CNN. To be precise about the improvement statistics, as compared to R-CNN, it is:</p>
<ul class="calibre7">
<li class="calibre8"><span>9x faster in training</span></li>
<li class="calibre8"><span>213x faster at scoring/servicing/testing (0.3s per image processing), ignoring the time spent on region proposals</span></li>
<li class="calibre8"><span>Has higher mAP of 66% on the PASCAL VOC 2012 dataset</span></li>
</ul>
<p class="calibre2">Where R-CNN uses a smaller (five-layer) CNN, Fast R-CNN uses the deeper VGG16 network, which accounts for its improved accuracy. Also, R-CNN is slow because it performs a ConvNet forward pass for each object proposal without sharing computation:</p>
<div class="mce-root"><img src="images/000006.png" class="calibre76" /></div>
<div class="cdpaligncenter">Fast R-CNN: Working</div>
<p class="calibre2">In Fast R-CNN, the deep VGG16 CNN provides essential computations for all the stages, namely:</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">Region of Interest</strong> (<strong class="calibre1">RoI</strong>) computation</li>
<li class="calibre8">Classification Objects (or background) for the region contents</li>
<li class="calibre8">Regression for enhancing the bounding box</li>
</ul>
<p class="calibre2">The input to the CNN, in this case, is not raw (candidate) regions from the image, but the (complete) actual image itself; the output is not the last flattened layer but the convolution (map) layer before that. From the so-generated convolution map, a the RoI pooling layer (a variant of max-pooling) is used to generate the flattened fixed-length RoI corresponding to each object proposal are generated, which are then passed through some <strong class="calibre13">fully connected</strong> (<strong class="calibre13">FC</strong>) layers.</p>
<div class="packt_infobox"><span>The RoI&nbsp;</span>pooling<span>&nbsp;is a variant of max&nbsp;</span>pooling (that we used in our initial chapters in this book)<span>, in which output size is fixed and input rectangle is a parameter.</span></div>
<div class="packt_tip">The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent.</div>
<p class="calibre2">The output from the penultimate FC layer is then used for both:</p>
<ul class="calibre7">
<li class="calibre8">Classification (SoftMax layer) with as many classes as object proposals, +1 additional class for the background (none of the classes found in the region)</li>
<li class="calibre8">Sets of regressors that produce the four numbers (two numbers denoting the x, y coordinates of the upper-left corner for the box for that object, and the next two numbers corresponding to the height and width of that object found in that region) for each object-proposal that is required to make bounding boxes precise for that particular object</li>
</ul>
<p class="calibre2">The result achieved with Fast R-CNN is great. What is even greater is the use of a powerful CNN network to provide very effective features for all three challenges that we need to overcome. But there are still some drawbacks, and there is scope for further improvements as we will understand in our next section on Faster R-CNN.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-6">
        <section>

                            <header>
                    <h1 class="header-title">Faster R-CNN&nbsp;&ndash; faster region proposal network-based CNN</h1>
                </header>
            
            <article>
                
<p class="calibre2">We saw in the earlier section that Fast R-CNN brought down the time required for scoring (testing) images drastically, but the reduction ignored the time required for generating Region Proposals, which use a separate mechanism (though pulling from the convolution map from CNN) and continue proving a bottleneck. Also, we observed that though all three challenges were resolved using the common features from convolution-map in Fast R-CNN, they were using different mechanisms/models.</p>
<p class="calibre2">Faster R-CNN improves upon these drawbacks and proposes the concept of <strong class="calibre13">Region Proposal Networks</strong> (<strong class="calibre13">RPNs</strong>), bringing down the scoring (testing) time to 0.2 seconds per image, even including time for Region Proposals.</p>
<div class="packt_tip"><span>Fast R-CNN was doing the scoring (testing) in 0.3 seconds per image, that too excluding the time required for the process equivalent to Region Proposal.</span></div>
<div class="mce-root"><img src="images/000090.png" class="calibre77" /></div>
<div class="cdpaligncenter">Faster R-CNN: Working - The Region Proposal Networking acting as Attention Mechanism</div>
<div class="title-page-name">
<p class="calibre2">As shown in the earlier figure, a VGG16 (or another) CNN works directly on the image, producing a convolutional map (similar to what was done in Fast R-CNN). Things differ from here, where now there are two branches, one feeding into the RPN and the other into the detection Network. This is&nbsp;<span class="calibre10">again</span><span class="calibre10">&nbsp;</span><span class="calibre10">an extension of the same CNN for prediction, leading to a <strong class="calibre13">Fully Convolutional Network</strong> (<strong class="calibre13">FCN</strong>). The&nbsp;</span>RPN<span class="calibre10">&nbsp;acts as an Attention Mechanism and also shares full-image convolutional features with the detection network.&nbsp;</span><span class="calibre10">Also, now because all the parts in the network can use efficient GPU-based computation, it thus reduces the overall time required:</span></p>
</div>
<div class="mce-root"><img src="images/000064.jpg" class="calibre78" /></div>
<div class="cdpaligncenter"><span>Faster R-CNN: Working - The Region Proposal Networking acting as Attention Mechanism</span></div>
<div class="packt_tip"><span>For a&nbsp;greater understanding of the Attention Mechanism, refer to the chapter on Attention Mechanisms for CNN in this book.</span></div>
<p class="calibre2">The RPN works in a sliding window mechanism, where a window slides (much like CNN filters) across the last convolution map from the shared convolutional layer. With each slide, the sliding window produces <em class="calibre19">k (k=N<sub class="calibre40">Scale</sub>&nbsp;Ã N<sub class="calibre40">Size</sub>)</em> number of Anchor Boxes (similar to Candidate Boxes), where <em class="calibre19">N<sub class="calibre40">Scale</sub></em>&nbsp;is the number of (pyramid like) scales per <em class="calibre19">size</em> of the <em class="calibre19">N<sub class="calibre40">Size&nbsp;</sub></em>sized (aspect ratio) box extracted from the center of the sliding window, much like the following figure.</p>
<p class="calibre2">The RPN leads into a flattened, FC layer. This, in turn, leads into two networks, one for predicting the four numbers for each of the <em class="calibre19">k</em> boxes (determining the coordinates, length and width of the box as in Fast R-CNN), and another into a binomial classification model that determines the objectness or probability of finding any of the given objects in that box. The output from the RPN leads into the detection network, which detects which particular class of object is in each of the k boxes given the position of the box and its objectness.&nbsp;</p>
<div class="title-page-name">
<div class="mce-root"><img src="images/000046.png" class="calibre26" /></div>
<div class="cdpaligncenter">Faster R-CNN: Working - extracting different scales and sizes</div>
</div>
<p class="calibre2">One problem in this architecture is the training of the two networks, namely the Region Proposal and detection network. We learned that CNN is trained using&nbsp;backpropagating across all layers while reducing the losses layers with every iteration. But because of the split into two different networks, we could at a time backpropagate across only one network. To resolve this issue, the training is done iteratively across each network, while keeping the weights of the other network constant. This helps in converging both the networks quickly.</p>
<p class="calibre2">An important feature of the RPN architecture is that it has <span class="calibre10">translation invariance with respect to both the functions, one that is producing the anchors, and another that is producing the attributes (its coordinate and objectness) for the anchors. Because of translation invariance, a reverse operation, or producing the portion of the image given a vector map of an anchor map is feasible.</span></p>
<div class="packt_tip"><span>Owing to Translational Invariance, we can move in either direction in a CNN, that is from image to (region) proposals, and from the proposals to the&nbsp;corresponding portion of the image.</span></div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-152">
        <section>

                            <header>
                    <h1 class="header-title">Mask R-CNN&nbsp;&ndash; Instance segmentation with CNN</h1>
                </header>
            
            <article>
                
<p class="calibre2">Faster R-CNN is state-of-the-art stuff in object detection today. But there are problems overlapping the area of object detection that Faster R-CNN cannot solve effectively, which is where Mask R-CNN, an evolution of Faster R-CNN can help.</p>
<p class="calibre2">This section introduces the concept of instance segmentation, which is a combination of the standard object detection problem as described in this chapter, and the challenge of s<span class="calibre10">emantic segmentation</span>.</p>
<div class="packt_tip"><span>In semantic segmentation, as applied to images, the goal is to classify each pixel into a fixed set of categories without differentiating object instances.</span></div>
<p class="calibre2">Remember our example of counting the number of dogs in the image in the intuition section? We were able to count the number of dogs easily, because they were very much apart, with no overlap, so essentially just counting the number of objects did the job. Now, take the following image, for instance, and count the number of tomatoes using object detection. It will be a daunting task because the Bounding Boxes will have so much of an overlap that it will be difficult to distinguish the Instances of tomatoes from the boxes.</p>
<p class="calibre2">So, essentially, we need to go further, beyond bounding boxes and into pixels to get that level separation and identification. Like we use to classify bounding boxes with object names in object detection, in Instance Segment, we segment/ classify, each pixel with not only the specific object name but also the object-instance.</p>
<p class="calibre2"><span class="calibre10">The object detection and Instance Segmentation could be treated as two different tasks, one logically leading to another, much like we discovered the tasks of finding Region Proposals and Classification in the case of object detection. But as in the case of object detection, and especially with techniques like Fast/Faster R-CNN, we discovered that it would be much effective if we have a mechanism to do them simultaneously, while also utilizing much of the computation and network to do so, to make the tasks seamless.&nbsp;</span></p>
<div class="mce-root"><img src="images/00014.jpeg" class="calibre79" /></div>
<div class="cdpaligncenter">Instance Segmentation&nbsp;&ndash; Intuition</div>
<p class="calibre2">Mask R-CNN is an extension of Faster R-CNN covered in the earlier network, and uses all the techniques used in Faster R-CNN, with one addition&mdash;an additional path in the network to generate a Segmentation Mask (or Object Mask) for each detected Object Instance in parallel. Also, because of this approach of using most of the existing network, it adds only a minimal overhead to the entire processing and has a scoring (test) time almost equivalent to that of Faster R-CNN. It has one of the best accuracies across all single-model solutions as applied to the COCO2016 challenge (using the COCO2015 dataset).</p>
<div class="packt_infobox">Like, PASCAL VOC, COCO is another large-scale standard (<span>series of</span>) dataset (from Microsoft). Besides object detection,&nbsp;<span>COCO is also used for segmentation and captioning. COCO is more extensive than many other datasets and much of the recent comparison on object detection is done on this for comparison purposes. The COCO dataset comes in three variants, namely COCO 2014, COCO 2015, and COCO 2017.</span></div>
<p class="calibre2">In Mask R-CNN, besides having the two branches that generate the objectness and localization for each anchor box or RoI, there also exists a third FCN that takes in the RoI and predicts a segmentation mask in a pixel-to-pixel manner for the given anchor box.</p>
<p class="calibre2">But there still remain some challenges. Though Faster R-CNN does demonstrate transformational invariance (that is, we could trace from the convolutional map of the RPN to the pixel map of the actual image), the convolutional map has a different structure from that of the actual image pixels. So, there is no pixel-to-pixel alignment between network inputs and outputs, which is important for our purpose of providing pixel-to-pixel masking using this network. To solve this challenge, Mask R-CNN uses a quantization-free layer (named RoIAlign in the original paper) that helps align the exact spatial locations. This layer not only provides exact alignment but also helps in improving the accuracy to a great extent, because of which Mask R-CNN is able to outperform many other networks:</p>
<div class="mce-root"><img src="images/000096.jpg" class="calibre80" /></div>
<div class="cdpaligncenter">Mask R-CNN&nbsp;&ndash; Instance Segmentation Mask (illustrative output)</div>
<div class="title-page-name">
<p class="calibre2">The concept of instance segmentation is very powerful and can lead to realizing a lot of very impactful use cases that were not possible with object detection alone.</p>
<div class="packt_tip"><span>We can even use instance segmentation to estimate human poses in the same framework and eliminate them.</span></div>
</div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-11">
        <section>

                            <header>
                    <h1 class="header-title">Instance segmentation in code</h1>
                </header>
            
            <article>
                
<p class="calibre2">It's now time to put the things that we've learned into practice. We'll use the COCO dataset and its API for the data, and use Facebook Research's Detectron project (link in References), which provides the Python implementation of many of the previously discussed techniques under an Apache 2.0 license. The code works with Python2 and Caffe2, so we'll need a virtual environment with the given configuration.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-153">
        <section>

                            <header>
                    <h1 class="header-title">Creating the environment</h1>
                </header>
            
            <article>
                
<p class="calibre2">The virtual environment, with Caffe2 installation, can be created as per the <kbd class="calibre12">caffe2</kbd> installation instructions on the Caffe2 repository link in the <em class="calibre19">References</em> Section. Next, we will install the dependencies.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-14">
        <section>

                            <header>
                    <h1 class="header-title">Installing Python dependencies (Python2 environment)</h1>
                </header>
            
            <article>
                
<p class="calibre2">We can install the Python dependencies as shown in the following code block:</p>
<div class="packt_tip">Python 2X and Python 3X are two different flavors of Python (or more precisely CPython), and not a conventional upgrade of version, therefore the libraries for one variant might not be compatible with another. Use Python 2X for this section.</div>
<div class="packt_infobox"><em class="calibre29">When we refer to the (interpreted) programming language Python, we need to refer to it with the specific interpreter (since it is an interpreted language as opposed to a compiled one like Java). The interpreter that we implicitly refer to as the Python interpreter (like the one you download from Python.org or the one that comes bundled with Anaconda) is technically called CPython, on which is the default byte-code interpreter of Python, which is written in C. But there are other Python interpreters also like Jython (build on Java), PyPy (written in Python itself - not so intuitive, right?), IronPython (.NET implementation of Python).&nbsp;</em></div>
<pre class="calibre20">pip install numpy&gt;=1.13 pyyaml&gt;=3.12 matplotlib opencv-python&gt;=3.2 setuptools Cython mock scipy</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-5">
        <section>

                            <header>
                    <h1 class="header-title">Downloading and installing the COCO API and detectron library (OS shell commands)</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre10">We will then download and&nbsp; install the Python dependencies as shown in the following code block:</span></p>
<pre class="calibre20"># COCO API download and install<br class="title-page-name" /># COCOAPI=/path/to/clone/cocoapi<br class="title-page-name" />git clone https://github.com/cocodataset/cocoapi.git $COCOAPI<br class="title-page-name" />cd $COCOAPI/PythonAPI<br class="title-page-name" />make install<br class="title-page-name" /><br class="title-page-name" /># Detectron library download and install<br class="title-page-name" /># DETECTRON=/path/to/clone/detectron
git clone https://github.com/facebookresearch/detectron $DETECTRON<br class="title-page-name" /><span>cd $DETECTRON/lib &amp;&amp; make</span></pre>
<p class="calibre2">Alternatively, we can download and use the Docker image of the environment (requires Nvidia GPU support):</p>
<pre class="calibre20"><span># DOCKER image build</span><br class="title-page-name" /><span>cd $DETECTRON/docker docker build -t detectron:c2-cuda9-cudnn7.</span><br class="title-page-name" /><span>nvidia-docker run --rm -it detectron:c2-cuda9-cudnn7 python2 tests/test_batch_permutation_op.py</span></pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-17">
        <section>

                            <header>
                    <h1 class="header-title">Preparing the COCO dataset folder structure</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now we will see the code to prepare the COCO dataset folder structure as follows:</p>
<pre class="calibre20"># We need the following Folder structure: coco [coco_train2014, coco_val2014, annotations]<br class="title-page-name" />mkdir -p $DETECTRON/lib/datasets/data/coco<br class="title-page-name" />ln -s /path/to/coco_train2014 $DETECTRON/lib/datasets/data/coco/<br class="title-page-name" />ln -s /path/to/coco_val2014 $DETECTRON/lib/datasets/data/coco/<br class="title-page-name" />ln -s /path/to/json/annotations $DETECTRON/lib/datasets/data/coco/annotations</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-154">
        <section>

                            <header>
                    <h1 class="header-title">Running the pre-trained model on the COCO dataset</h1>
                </header>
            
            <article>
                
<p class="calibre2">We can now implement the pre-trained model on the COCO dataset as shown in the following code snippet:</p>
<pre class="calibre20">python2 tools/test_net.py \
    --cfg configs/12_2017_baselines/e2e_mask_rcnn_R-101-FPN_2x.yaml \
    TEST.WEIGHTS https://s3-us-west-2.amazonaws.com/detectron/35861858/12_2017_baselines/e2e_mask_rcnn_R-101-             FPN_2x.yaml.02_32_51.SgT4y1cO/output/train/coco_2014_train:coco_2014_valminusminival/generalized_rcnn/model_final.pkl \
    NUM_GPUS 1</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-19">
        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ol class="calibre15">
<li class="calibre8">
<p class="calibre9"><span class="calibre10">Paul Viola and Michael Jones,&nbsp;</span><span class="calibre10">Rapid object detection using a boosted cascade of simple features,&nbsp;<em class="calibre19">Conference on Computer Vision and Pattern Recognition</em>,&nbsp;</span><span class="calibre10">2001.</span></p>
</li>
<li class="calibre8">
<p class="calibre9"><span class="calibre10">Paul Viola and Michael Jones,&nbsp;</span><span class="calibre10">Robust Real-time object detection,&nbsp;</span><span class="calibre10"><em class="calibre19">International Journal of Computer Vision</em>,&nbsp;</span><span class="calibre10">2001.</span></p>
</li>
<li class="calibre8">
<p class="calibre9">Itseez2015opencv, OpenCV,<span class="calibre10">&nbsp;</span><em class="calibre19">Open Source Computer Vision Library</em>, Itseez, 2015.</p>
</li>
<li class="calibre8">
<p class="calibre9">Ross B. Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik,<span class="calibre10">&nbsp;</span><em class="calibre19">Rich feature hierarchies for accurate object detection and semantic segmentation</em>, CoRR, arXiv:1311.2524, 2013.</p>
</li>
<li class="calibre8">
<p class="calibre9">Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik,<span class="calibre10">&nbsp;</span><em class="calibre19">Rich feature hierarchies for accurate object detection and semantic segmentation</em>, Computer Vision and Pattern Recognition, 2014.</p>
</li>
<li class="calibre8">
<p class="calibre9">M. Everingham, L. VanGool, C. K. I. Williams, J. Winn, A. Zisserman,<span class="calibre10">&nbsp;</span><em class="calibre19">The PASCAL Visual Object Classes Challenge 2012</em>, VOC2012, Results.</p>
</li>
<li class="calibre8">
<p class="calibre9">D. Lowe.<span class="calibre10">&nbsp;</span><em class="calibre19">Distinctive image features from scale-invariant keypoints</em>, IJCV, 2004.</p>
</li>
<li class="calibre8">
<p class="calibre9">N. Dalal and B. Triggs.<span class="calibre10">&nbsp;</span><em class="calibre19">Histograms of oriented gradients for human detection</em>. In CVPR, 2005.</p>
</li>
<li class="calibre8">
<p class="calibre9">Ross B. Girshick, Fast R-CNN, CoRR, arXiv:1504.08083, 2015.</p>
</li>
<li class="calibre8">
<p class="calibre9">Rbgirshick, fast-rcnn, GitHub,<span class="calibre10">&nbsp;</span><a href="https://github.com/rbgirshick/fast-rcnn" class="calibre11">https://github.com/rbgirshick/fast-rcnn</a>, Feb-2018.</p>
</li>
<li class="calibre8">
<p class="calibre9">Shaoqing Ren, Kaiming He, Ross B. Girshick, Jian Sun, Faster R-CNN:<span class="calibre10">&nbsp;</span><em class="calibre19">Towards Real-Time Object Detection with Region Proposal Networks</em>, CoRR, arXiv:1506.01497, 2015.</p>
</li>
<li class="calibre8">
<p class="calibre9">Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun, Faster R-CNN:<span class="calibre10">&nbsp;</span><em class="calibre19">Towards Real-Time Object Detection with Region Proposal Networks</em>, Advances in<span class="calibre10">&nbsp;</span><strong class="calibre13">Neural Information Processing Systems</strong><span class="calibre10">&nbsp;</span>(<strong class="calibre13">NIPS</strong>), 2015.</p>
</li>
<li class="calibre8">
<p class="calibre9">Rbgirshick, py-faster-rcnn, GitHub,<span class="calibre10">&nbsp;</span><a href="https://github.com/rbgirshick/py-faster-rcnn" class="calibre11">https://github.com/rbgirshick/py-faster-rcnn</a>, Feb-2018.</p>
</li>
<li class="calibre8">
<p class="calibre9">Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Dollar, Kaiming He,</p>
Detectron, GitHub,<span>&nbsp;</span><a href="https://github.com/facebookresearch/Detectron" class="calibre11">https://github.com/facebookresearch/Detectron</a>, Feb-2018.</li>
<li class="calibre8">
<p class="calibre9">Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, C. Lawrence Zitnick,<span class="calibre10">&nbsp;</span><em class="calibre19">Microsoft COCO: Common Objects in Context</em>, CoRR, arXiv:1405.0312, 2014.</p>
</li>
<li class="calibre8">
<p class="calibre9">Kaiming He, Georgia Gkioxari, Piotr Dollar, Ross B. Girshick, Mask R-CNN, CoRR, arXiv:1703.06870, 2017.</p>
</li>
<li class="calibre8">
<p class="calibre9">Liang-Chieh Chen, Alexander Hermans, George Papandreou, Florian Schroff, Peng Wang, Hartwig Adam, MaskLab:<span class="calibre10">&nbsp;</span><em class="calibre19">Instance Segmentation by Refining Object Detection with Semantic and Direction Features</em>, CoRR, arXiv:1712.04837, 2017.</p>
</li>
<li class="calibre8">
<p class="calibre9">Anurag Arnab, Philip H. S. Torr,<span class="calibre10">&nbsp;</span><em class="calibre19">Pixelwise Instance Segmentation with a Dynamically Instantiated Network</em>, CoRR, arXiv:1704.02386, 2017.</p>
</li>
<li class="calibre8">
<p class="calibre9">Matterport, Mask_RCNN, GitHub,<span class="calibre10">&nbsp;</span><a href="https://github.com/matterport/Mask_RCNN" class="calibre11">https://github.com/matterport/Mask_RCNN</a>, Feb-2018.</p>
</li>
<li class="calibre8">
<p class="calibre9">CharlesShang, FastMaskRCNN, GitHub,<span class="calibre10">&nbsp;</span><a href="https://github.com/CharlesShang/FastMaskRCNN" class="calibre11">https://github.com/CharlesShang/FastMaskRCNN</a>, Feb-2018.</p>
</li>
<li class="calibre8">
<p class="calibre9">Caffe2, Caffe2, GitHub,<span class="calibre10">&nbsp;</span><a href="https://github.com/caffe2/caffe2" class="calibre11">https://github.com/caffe2/caffe2</a>, Feb-2018.</p>
</li>
</ol>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-155">
        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we started from the very simple intuition behind the task of object detection and then proceeded to very advanced concepts, such as Instance Segmentation, which is a contemporary research area. Object detection is at the heart of a lot of innovation in the field of Retail, Media, Social Media, Mobility, and Security; there is a lot of potential for using these technologies to create very impactful and profitable features for both enterprise and social consumption.&nbsp;</p>
<p class="calibre2">From the Algorithms perspective, this chapter started with the legendary Viola-Jones algorithm and its underlying mechanisms, such as Haar Features and Cascading Classifiers. Using that intuition, we started exploring the world of CNN for object detection with algorithms, such as R-CNN, Fast R-CNN, up to the very state-of-the-art Faster R-CNN.</p>
<p class="calibre2">In this chapter, we also laid the foundations and introduced a very recent and impactful field of research called <strong class="calibre13">instance segmentation</strong>. We also covered some state-of-the-art Deep CNNs based on methods, such as Mask R-CNN, for easy and performant implementation of instance segmentation.</p>
<p class="calibre2">&nbsp;</p>
<p class="calibre2"></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-22">
        <section>

                            <header>
                    <h1 class="header-title">GAN: Generating New Images with CNN</h1>
                </header>
            
            <article>
                
<p class="calibre2">Generally, a neural network needs labeled examples to learn effectively. Unsupervised learning approaches to learn from unlabeled data have not worked very well. A&nbsp;<strong class="calibre13">generative adversarial network</strong>, or simply a&nbsp;<strong class="calibre13">GAN</strong>, is part of an unsupervised learning approach but based on differentiable generator networks. GANs were first invented by Ian Goodfellow and others in 2014. Since then they have become extremely popular. This is based on game theory and has two players or networks: a generator network and b) a discriminator network, both competing against each other. This dual network game theory-based approach vastly improved the process of learning from unlabeled data. The generator network produces fake data and passes it to a discriminator. The discriminator network also sees real data and predicts whether the data it receives is fake or real. So, the generator is trained so that it can easily produce data that is very close to real data in order to fool the discriminator network. The discrimin<span class="calibre10">ator network is trained to classify which data is real and which data is fake. So, eventually, a generator network learns to produce data that is very, very close to real data. GANs are going to be widely popular in the music and arts domains.</span></p>
<div class="packt_tip"><span>According to&nbsp;</span><span>Goodfellow, "<em class="calibre29">You can think of generative models as giving Artificial Intelligence a form of imagination</em>."</span></div>
<p class="calibre2"><span class="calibre10">The following are a couple of examples of GANs:</span></p>
<ul class="calibre7">
<li class="calibre8">Pix2pix</li>
<li class="calibre8">CycleGAN</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-156">
        <section>

                            <header>
                    <h1 class="header-title">Pix2pix - Image-to-Image translation GAN</h1>
                </header>
            
            <article>
                
<p class="calibre2">This network uses a&nbsp;<strong class="calibre13">conditional generative adversarial network</strong> (<strong class="calibre13">cGAN</strong>) to learn mapping from the input and output of an image. Some of the examples that can be done from the original paper are as follows:</p>
<div class="cdpaligncenter"><img src="images/00071.jpeg" class="calibre81" /></div>
<div class="cdpaligncenter">&nbsp;Pix2pix examples of cGANs&nbsp;</div>
<p class="calibre2">In the handbags example, the network learns how to color a black and white image. Here, the training dataset has the input image in black and white and the target image is the color version.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-25">
        <section>

                            <header>
                    <h1 class="header-title">CycleGAN&nbsp;</h1>
                </header>
            
            <article>
                
<p class="calibre2">CycleGAN is also an image-to-image translator but without input/output pairs. For example, to generate photos from paintings, convert a horse image into a zebra image:</p>
<div class="mce-root"><img src="images/00052.jpeg" class="calibre26" /></div>
<div class="packt_tip"><span>In a discriminator network, use of dropout is important. Otherwise, it may produce a poor result.</span></div>
<p class="calibre2">The generator network takes random noise as input and produces a realistic image as output. Running a generator network for different kinds of random noise produces different types of realistic images. The second network, which is known as the&nbsp;<strong class="calibre13">discriminator network</strong>,&nbsp;is very similar to a regular neural net classifier. This network is trained on real images, although training a GAN is quite different from a supervised training method. In supervised training, each image is labeled first before being shown to the model. For example, if the input is a dog image, we tell the model this is a dog. In case of a generative model, we show the model a lot of images and ask it to make more such similar images from the same probability distribution. Actually, the second discriminator network helps the generator network to achieve this.</p>
<p class="calibre2">The discriminator outputs the probability that the image is real or fake from the generator network. In other words, it tries to assign a probability close to 1 for a real image and a probability close to 0 for fake images. Meanwhile, the generator does the opposite. It is trained to output images that will have a probability close to 1 by the discriminator. Over time, the generator produces more realistic images and fools the discriminator:</p>
<div class="mce-root"><img src="images/000025.png" class="calibre82" /></div>
<p class="calibre2"></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-157">
        <section>

                            <header>
                    <h1 class="header-title">Training a GAN model</h1>
                </header>
            
            <article>
                
<p class="calibre2">Most machine learning models explained in earlier chapters are based on optimization, that is, we minimize the cost function over its parameter space. GANs are different because of two networks: the generator G and the discriminator D. Each has its own cost. An easy way to visualize GAN is the cost of the discriminator is the negative of the cost of the generator. In GAN, we can define a value function that the generator has to minimize and the discriminator has to maximize. The training process for a generative model is quite different from the supervised training method. GAN is sensitive to the initial weights. So we need to use batch normalization. B<span class="calibre10">atch normalization</span>&nbsp;makes the model stable, besides improving performance. Here, we train two models, the generative model and the discriminative model, simultaneously. Generative model G captures data distribution and discriminative model D estimates the probability of a sample that came from training data rather than G.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-28">
        <section>

                            <header>
                    <h1 class="header-title">GAN &ndash; code example</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the following example, we build and train a GAN model using an MNIST dataset and using TensorFlow. Here, we will use a special version of the ReLU activation function known as <span class="calibre10"><strong class="calibre13">Leaky ReLU</strong>. The output is a new type of handwritten digit:</span></p>
<div class="packt_tip"><span>Leaky ReLU is a variation of the ReLU activation function given by the formula&nbsp;<span><span><em class="calibre29"><span>f</span><span>(</span><span>x</span><span>)&nbsp;</span><span>=&nbsp;</span><span>m</span><span>a</span><span>x</span><span>(</span><span>Î±</span><span>â</span><span>x</span><span>,&nbsp;</span><span>x</span></em><span><em class="calibre29">)</em>. So the output for the negative value for <em class="calibre29">x</em>&nbsp;is <em class="calibre29">alpha * x&nbsp;</em>and the output for positive <em class="calibre29">x</em> is <em class="calibre29">x</em>.</span></span></span></span></div>
<pre class="calibre20">#import all necessary libraries and load data set<br class="title-page-name" />%matplotlib inline<br class="title-page-name" /><br class="title-page-name" />import pickle as pkl<br class="title-page-name" />import numpy as np<br class="title-page-name" />import tensorflow as tf<br class="title-page-name" />import matplotlib.pyplot as plt<br class="title-page-name" /><br class="title-page-name" />from tensorflow.examples.tutorials.mnist import input_data<br class="title-page-name" />mnist = input_data.read_data_sets('MNIST_data')</pre>
<p class="calibre2">In order to build this network, we need two inputs, one for the generator and one for the discriminator. In the following code, we create placeholders for <kbd class="calibre12">real_input</kbd> for the discriminator and&nbsp;<kbd class="calibre12">z_input</kbd> for the generator, with the input sizes as <kbd class="calibre12">dim_real</kbd> and <kbd class="calibre12">dim_z</kbd>, respectively:</p>
<pre class="calibre20">#place holder for model inputs <br class="title-page-name" />def model_inputs(dim_real, dim_z):<br class="title-page-name" />    real_input = tf.placeholder(tf.float32, name='dim_real')<br class="title-page-name" />    z_input = tf.placeholder(tf.float32, name='dim_z')<br class="title-page-name" />    <br class="title-page-name" />    return real_input, z_input</pre>
<p class="calibre2"><span class="calibre10">Here, input <kbd class="calibre12">z</kbd> is a random vector to the generator which turns this vector into an image. Then we add a hidden layer, which is a leaky ReLU layer, to allow gradients to flow backwards. Leaky ReLU is just like a normal ReLU (for negative values emitting zero) except that there is a small non-zero output for negative input values. The generator performs better with the <kbd class="calibre12">tanh</kbd><kbd class="calibre12">sigmoid</kbd> function. Generator output is <kbd class="calibre12">tanh</kbd> output. So, we'll have to rescale the MNIST images to be between -1 and 1, instead of 0 and 1. With this knowledge, we can build the generator network:</span></p>
<pre class="calibre20">#Following code builds Generator Network<br class="title-page-name" />def generator(z, out_dim, n_units=128, reuse=False, alpha=0.01):<br class="title-page-name" />    ''' Build the generator network.<br class="title-page-name" />    <br class="title-page-name" />        Arguments<br class="title-page-name" />        ---------<br class="title-page-name" />        z : Input tensor for the generator<br class="title-page-name" />        out_dim : Shape of the generator output<br class="title-page-name" />        n_units : Number of units in hidden layer<br class="title-page-name" />        reuse : Reuse the variables with tf.variable_scope<br class="title-page-name" />        alpha : leak parameter for leaky ReLU<br class="title-page-name" />        <br class="title-page-name" />        Returns<br class="title-page-name" />        -------<br class="title-page-name" />        out: <br class="title-page-name" />    '''<br class="title-page-name" />    with tf.variable_scope('generator', reuse=reuse) as generator_scope: # finish this<br class="title-page-name" />        # Hidden layer<br class="title-page-name" />        h1 = tf.layers.dense(z, n_units, activation=None )<br class="title-page-name" />        # Leaky ReLU<br class="title-page-name" />        h1 = tf.nn.leaky_relu(h1, alpha=alpha,name='leaky_generator')<br class="title-page-name" />        <br class="title-page-name" />        # Logits and tanh output<br class="title-page-name" />        logits = tf.layers.dense(h1, out_dim, activation=None)<br class="title-page-name" />        out = tf.tanh(logits)<br class="title-page-name" />        <br class="title-page-name" />        return out</pre>
<p class="calibre2">The discriminator network is the same as the generator except that output layer is a <kbd class="calibre12">sigmoid</kbd> function:</p>
<pre class="calibre20"><br class="title-page-name" />def discriminator(x, n_units=128, reuse=False, alpha=0.01):<br class="title-page-name" />    ''' Build the discriminator network.<br class="title-page-name" />    <br class="title-page-name" />        Arguments<br class="title-page-name" />        ---------<br class="title-page-name" />        x : Input tensor for the discriminator<br class="title-page-name" />        n_units: Number of units in hidden layer<br class="title-page-name" />        reuse : Reuse the variables with tf.variable_scope<br class="title-page-name" />        alpha : leak parameter for leaky ReLU<br class="title-page-name" />        <br class="title-page-name" />        Returns<br class="title-page-name" />        -------<br class="title-page-name" />        out, logits: <br class="title-page-name" />    '''<br class="title-page-name" />    with tf.variable_scope('discriminator', reuse=reuse) as discriminator_scope:# finish this<br class="title-page-name" />        # Hidden layer<br class="title-page-name" />        h1 = tf.layers.dense(x, n_units, activation=None )<br class="title-page-name" />        # Leaky ReLU<br class="title-page-name" />        h1 = tf.nn.leaky_relu(h1, alpha=alpha,name='leaky_discriminator')<br class="title-page-name" />        <br class="title-page-name" />        logits = tf.layers.dense(h1, 1, activation=None)<br class="title-page-name" />        out = tf.sigmoid(logits)<br class="title-page-name" />        <br class="title-page-name" />        return out, logits</pre>
<p class="calibre2"><span class="calibre10">To build the network, use the following code:</span></p>
<pre class="calibre20">#Hyperparameters<br class="title-page-name" /># Size of input image to discriminator<br class="title-page-name" />input_size = 784 # 28x28 MNIST images flattened<br class="title-page-name" /># Size of latent vector to generator<br class="title-page-name" />z_size = 100<br class="title-page-name" /># Sizes of hidden layers in generator and discriminator<br class="title-page-name" />g_hidden_size = 128<br class="title-page-name" />d_hidden_size = 128<br class="title-page-name" /># Leak factor for leaky ReLU<br class="title-page-name" />alpha = 0.01<br class="title-page-name" /># Label smoothing <br class="title-page-name" />smooth = 0.1</pre>
<p class="calibre2"><span class="calibre10">We want to share weights between real and fake data, so we need to reuse the variables:</span></p>
<pre class="calibre20">#Build the network<br class="title-page-name" />tf.reset_default_graph()<br class="title-page-name" /># Create our input placeholders<br class="title-page-name" />input_real, input_z = model_inputs(input_size, z_size)<br class="title-page-name" /><br class="title-page-name" /># Build the model<br class="title-page-name" />g_model = generator(input_z, input_size, n_units=g_hidden_size, alpha=alpha)<br class="title-page-name" /># g_model is the generator output<br class="title-page-name" /><br class="title-page-name" />d_model_real, d_logits_real = discriminator(input_real, n_units=d_hidden_size, alpha=alpha)<br class="title-page-name" />d_model_fake, d_logits_fake = discriminator(g_model, reuse=True, n_units=d_hidden_size, alpha=alpha)</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-158">
        <section>

                            <header>
                    <h1 class="header-title">Calculating loss&nbsp;</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre10">For the discriminator, the total loss is the sum of the losses for real and fake images. The losses will be sigmoid cross-entropyies, which we can get using the TensorFlow <kbd class="calibre12">tf.nn.sigmoid_cross_entropy_with_logits</kbd>. Then we compute the mean for all the images in the batch.&nbsp;So the losses will look like this:</span></p>
<pre class="calibre20">tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))</pre>
<p class="calibre2">To help the discriminator generalize better, the <kbd class="calibre12">labels</kbd> can be reduced a bit from 1.0 to 0.9, by for example, using the parameter <kbd class="calibre12">smooth</kbd><em class="calibre19">.&nbsp;</em>This is known as <strong class="calibre13">label smoothing</strong>, and is typically used with classifiers to improve performance.&nbsp;<span class="calibre10">The discriminator loss for the fake data is similar. The <kbd class="calibre12">logits</kbd> are&nbsp;<kbd class="calibre12">d_logits_fake</kbd></span><span class="calibre10">, which we got from passing the generator output to the discriminator. These fake <kbd class="calibre12">logits</kbd> are used with <kbd class="calibre12">labels</kbd> of all zeros. Remember that we want the discriminator to output 1 for real images and 0 for fake images, so we need to set up the losses to reflect that.</span></p>
<p class="calibre2"><span class="calibre10">Finally, the generator losses are using&nbsp;<kbd class="calibre12">d_logits_fake</kbd><em class="calibre19">,&nbsp;</em>the fake image <kbd class="calibre12">logits</kbd>. But now the <kbd class="calibre12">labels</kbd> are all 1s. The generator is trying to fool the discriminator, so it wants the discriminator to output ones for fake images:</span></p>
<pre class="calibre20"># Calculate losses<br class="title-page-name" />d_loss_real = tf.reduce_mean(<br class="title-page-name" />                  tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, <br class="title-page-name" />                                                          labels=tf.ones_like(d_logits_real) * (1 - smooth)))<br class="title-page-name" />d_loss_fake = tf.reduce_mean(<br class="title-page-name" />                  tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, <br class="title-page-name" />                                                          labels=tf.zeros_like(d_logits_real)))<br class="title-page-name" />d_loss = d_loss_real + d_loss_fake<br class="title-page-name" /><br class="title-page-name" />g_loss = tf.reduce_mean(<br class="title-page-name" />             tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,<br class="title-page-name" />                                                     labels=tf.ones_like(d_logits_fake)))</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-33">
        <section>

                            <header>
                    <h1 class="header-title">Adding the optimizer</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre10">We need to update the generator and discriminator variables separately. So, first get all the variables of the graph and then, as we explained earlier, we can get only generator variables from the generator scope and, similarly, discriminator variables from the discriminator scope:</span></p>
<pre class="calibre20"># Optimizers<br class="title-page-name" />learning_rate = 0.002<br class="title-page-name" /><br class="title-page-name" /># Get the trainable_variables, split into G and D parts<br class="title-page-name" />t_vars = tf.trainable_variables()<br class="title-page-name" />g_vars = [var for var in t_vars if var.name.startswith('generator')]<br class="title-page-name" />d_vars = [var for var in t_vars if var.name.startswith('discriminator')]<br class="title-page-name" /><br class="title-page-name" />d_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)<br class="title-page-name" />g_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)</pre>
<p class="calibre2"><span class="calibre10">To train the network, use:</span></p>
<pre class="calibre20">batch_size = 100<br class="title-page-name" />epochs = 100<br class="title-page-name" />samples = []<br class="title-page-name" />losses = []<br class="title-page-name" /># Only save generator variables<br class="title-page-name" />saver = tf.train.Saver(var_list=g_vars)<br class="title-page-name" />with tf.Session() as sess:<br class="title-page-name" />    sess.run(tf.global_variables_initializer())<br class="title-page-name" />    for e in range(epochs):<br class="title-page-name" />        for ii in range(mnist.train.num_examples//batch_size):<br class="title-page-name" />            batch = mnist.train.next_batch(batch_size)<br class="title-page-name" />            <br class="title-page-name" />            # Get images, reshape and rescale to pass to D<br class="title-page-name" />            batch_images = batch[0].reshape((batch_size, 784))<br class="title-page-name" />            batch_images = batch_images*2 - 1<br class="title-page-name" />            <br class="title-page-name" />            # Sample random noise for G<br class="title-page-name" />            batch_z = np.random.uniform(-1, 1, size=(batch_size, z_size))<br class="title-page-name" />            <br class="title-page-name" />            # Run optimizers<br class="title-page-name" />            _ = sess.run(d_train_opt, feed_dict={input_real: batch_images, input_z: batch_z})<br class="title-page-name" />            _ = sess.run(g_train_opt, feed_dict={input_z: batch_z})<br class="title-page-name" />        <br class="title-page-name" />        # At the end of each epoch, get the losses and print them out<br class="title-page-name" />        train_loss_d = sess.run(d_loss, {input_z: batch_z, input_real: batch_images})<br class="title-page-name" />        train_loss_g = g_loss.eval({input_z: batch_z})<br class="title-page-name" />            <br class="title-page-name" />        print("Epoch {}/{}...".format(e+1, epochs),<br class="title-page-name" />              "Discriminator Loss: {:.4f}...".format(train_loss_d),<br class="title-page-name" />              "Generator Loss: {:.4f}".format(train_loss_g)) <br class="title-page-name" />        # Save losses to view after training<br class="title-page-name" />        losses.append((train_loss_d, train_loss_g))<br class="title-page-name" />        <br class="title-page-name" />        # Sample from generator as we're training for viewing afterwards<br class="title-page-name" />        sample_z = np.random.uniform(-1, 1, size=(16, z_size))<br class="title-page-name" />        gen_samples = sess.run(<br class="title-page-name" />                       generator(input_z, input_size, n_units=g_hidden_size, reuse=True, alpha=alpha),<br class="title-page-name" />                       feed_dict={input_z: sample_z})<br class="title-page-name" />        samples.append(gen_samples)<br class="title-page-name" />        saver.save(sess, './checkpoints/generator.ckpt')<br class="title-page-name" /><br class="title-page-name" /># Save training generator samples<br class="title-page-name" />with open('train_samples.pkl', 'wb') as f:<br class="title-page-name" />    pkl.dump(samples, f)</pre>
<p class="calibre2">Once the model is trained and saved, you can visualize the generated digits (the code is not here, but it can be downloaded).&nbsp;</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-159">
        <section>

                            <header>
                    <h1 class="header-title">Semi-supervised learning and GAN</h1>
                </header>
            
            <article>
                
<p class="calibre2">So for, we have seen how GAN can be used to generate realistic images. In this section, we will see how GAN can be used for classification tasks where we have less labeled data but still <span class="calibre10">want to improve the accuracy</span> of the classifier. Here we will also use the same <strong class="calibre13">Street View House Number</strong> or <strong class="calibre13">SVHN</strong> dataset to classify images. As previously, here we also have two networks, the generator G and discriminator D. In this case, the discriminator is trained to become a classifier. Another change is that the output of the discriminator goes to a softmax function instead of a <kbd class="calibre12">sigmoid</kbd> function, as&nbsp;seen earlier. The softmax function returns the probability distribution over labels:</p>
<div class="mce-root"><img src="images/000105.png" class="calibre83" /></div>
<p class="calibre2">Now we model the network as:</p>
<p class="mce-root1"><em class="calibre19">total cost = cost of labeled data + cost of unlabeled data</em></p>
<p class="calibre2">To get the cost of labeled data, we can use the <kbd class="calibre12">cross_entropy</kbd> function:</p>
<pre class="calibre20"><span>cost of labeled data  = cross_entropy ( logits, labels)<br class="title-page-name" /></span><span>cost of unlabeled data = </span>  <span>cross_entropy ( logits, real)</span></pre>
<p class="calibre2">Then we can <span class="calibre10">calculate the sum&nbsp;</span>of all classes:</p>
<pre class="calibre20">real prob = sum (softmax(real_classes))</pre>
<p class="calibre2">Normal classifiers work on labeled data. However, semi-supervised GAN-based classifiers work on labeled data, real unlabeled data, and fake images. This works very well, that is, there are less classification errors even though we have less labeled data in the training process.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-35">
        <section>

                            <header>
                    <h1 class="header-title">Feature matching</h1>
                </header>
            
            <article>
                
<p class="calibre2">The idea of feature matching is to add an extra variable to the cost function of the generator in order to penalize the difference between absolute errors in the test data and training data.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-160">
        <section>

                            <header>
                    <h1 class="header-title">Semi-supervised classification using a GAN example</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we explain how to use GAN to build a classifier with the semi-supervised learning approach.</p>
<p class="calibre2">In supervised learning, we have a training set of inputs<span class="calibre10">&nbsp;<kbd class="calibre12">X</kbd></span><span class="calibre10">&nbsp;</span>and class labels<span class="calibre10">&nbsp;</span><kbd class="calibre12"><span><span>y</span></span></kbd>. We train a model that takes<span class="calibre10">&nbsp;<kbd class="calibre12">X</kbd></span>&nbsp;as input and gives<span class="calibre10">&nbsp;</span><kbd class="calibre12"><span><span>y</span></span></kbd><span class="calibre10">&nbsp;</span>as output.</p>
<p class="calibre2">In semi-supervised learning, our goal is still to train a model that takes<span class="calibre10">&nbsp;<kbd class="calibre12">X</kbd>&nbsp;</span>as input and generates<span class="calibre10">&nbsp;</span><kbd class="calibre12"><span><span>y</span></span></kbd><span class="calibre10">&nbsp;</span>as output. However, not all of our training examples have a label<span class="calibre10">&nbsp;</span><kbd class="calibre12"><span><span><span><span><span>y</span></span></span></span></span></kbd>.&nbsp;</p>
<p class="calibre2">We use the SVHN dataset. We'll turn the GAN discriminator into an 11 class <span class="calibre10">discriminator</span> (0 to 9 and one label for the fake image). It will recognize the 10 different classes of real SVHN digits, as well as an eleventh class of fake images that come from the generator. The discriminator will get to train on real labeled images, real unlabeled images, and fake images. By drawing on three sources of data instead of just one, it will generalize to the test set much better than a traditional classifier trained on only one source of data:</p>
<pre class="calibre20">def model_inputs(real_dim, z_dim):<br class="title-page-name" />    inputs_real = tf.placeholder(tf.float32, (None, *real_dim), name='input_real')<br class="title-page-name" />    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')<br class="title-page-name" />    y = tf.placeholder(tf.int32, (None), name='y')<br class="title-page-name" />    label_mask = tf.placeholder(tf.int32, (None), name='label_mask')<br class="title-page-name" />    <br class="title-page-name" />    return inputs_real, inputs_z, y, label_mask</pre>
<p class="calibre2">Add the generator:</p>
<pre class="calibre20">def generator(z, output_dim, reuse=False, alpha=0.2, training=True, size_mult=128):<br class="title-page-name" />    with tf.variable_scope('generator', reuse=reuse):<br class="title-page-name" />        # First fully connected layer<br class="title-page-name" />        x1 = tf.layers.dense(z, 4 * 4 * size_mult * 4)<br class="title-page-name" />        # Reshape it to start the convolutional stack<br class="title-page-name" />        x1 = tf.reshape(x1, (-1, 4, 4, size_mult * 4))<br class="title-page-name" />        x1 = tf.layers.batch_normalization(x1, training=training)<br class="title-page-name" />        x1 = tf.maximum(alpha * x1, x1)<br class="title-page-name" />        <br class="title-page-name" />        x2 = tf.layers.conv2d_transpose(x1, size_mult * 2, 5, strides=2, padding='same')<br class="title-page-name" />        x2 = tf.layers.batch_normalization(x2, training=training)<br class="title-page-name" />        x2 = tf.maximum(alpha * x2, x2)<br class="title-page-name" />        <br class="title-page-name" />        x3 = tf.layers.conv2d_transpose(x2, size_mult, 5, strides=2, padding='same')<br class="title-page-name" />        x3 = tf.layers.batch_normalization(x3, training=training)<br class="title-page-name" />        x3 = tf.maximum(alpha * x3, x3)<br class="title-page-name" />        <br class="title-page-name" />        # Output layer<br class="title-page-name" />        logits = tf.layers.conv2d_transpose(x3, output_dim, 5, strides=2, padding='same')<br class="title-page-name" />        <br class="title-page-name" />        out = tf.tanh(logits)<br class="title-page-name" />        <br class="title-page-name" />        return out</pre>
<p class="calibre2">Add the discriminator:</p>
<pre class="calibre20">def discriminator(x, reuse=False, alpha=0.2, drop_rate=0., num_classes=10, size_mult=64):<br class="title-page-name" />    with tf.variable_scope('discriminator', reuse=reuse):<br class="title-page-name" />        x = tf.layers.dropout(x, rate=drop_rate/2.5)<br class="title-page-name" />        <br class="title-page-name" />        # Input layer is 32x32x3<br class="title-page-name" />        x1 = tf.layers.conv2d(x, size_mult, 3, strides=2, padding='same')<br class="title-page-name" />        relu1 = tf.maximum(alpha * x1, x1)<br class="title-page-name" />        relu1 = tf.layers.dropout(relu1, rate=drop_rate)<br class="title-page-name" />        <br class="title-page-name" />        x2 = tf.layers.conv2d(relu1, size_mult, 3, strides=2, padding='same')<br class="title-page-name" />        bn2 = tf.layers.batch_normalization(x2, training=True)<br class="title-page-name" />        relu2 = tf.maximum(alpha * x2, x2)<br class="title-page-name" />        <br class="title-page-name" />        <br class="title-page-name" />        x3 = tf.layers.conv2d(relu2, size_mult, 3, strides=2, padding='same')<br class="title-page-name" />        bn3 = tf.layers.batch_normalization(x3, training=True)<br class="title-page-name" />        relu3 = tf.maximum(alpha * bn3, bn3)<br class="title-page-name" />        relu3 = tf.layers.dropout(relu3, rate=drop_rate)<br class="title-page-name" />        <br class="title-page-name" />        x4 = tf.layers.conv2d(relu3, 2 * size_mult, 3, strides=1, padding='same')<br class="title-page-name" />        bn4 = tf.layers.batch_normalization(x4, training=True)<br class="title-page-name" />        relu4 = tf.maximum(alpha * bn4, bn4)<br class="title-page-name" />        <br class="title-page-name" />        x5 = tf.layers.conv2d(relu4, 2 * size_mult, 3, strides=1, padding='same')<br class="title-page-name" />        bn5 = tf.layers.batch_normalization(x5, training=True)<br class="title-page-name" />        relu5 = tf.maximum(alpha * bn5, bn5)<br class="title-page-name" />        <br class="title-page-name" />        x6 = tf.layers.conv2d(relu5, 2 * size_mult, 3, strides=2, padding='same')<br class="title-page-name" />        bn6 = tf.layers.batch_normalization(x6, training=True)<br class="title-page-name" />        relu6 = tf.maximum(alpha * bn6, bn6)<br class="title-page-name" />        relu6 = tf.layers.dropout(relu6, rate=drop_rate)<br class="title-page-name" />        <br class="title-page-name" />        x7 = tf.layers.conv2d(relu5, 2 * size_mult, 3, strides=1, padding='valid')<br class="title-page-name" />        # Don't use bn on this layer, because bn would set the mean of each feature<br class="title-page-name" />        # to the bn mu parameter.<br class="title-page-name" />        # This layer is used for the feature matching loss, which only works if<br class="title-page-name" />        # the means can be different when the discriminator is run on the data than<br class="title-page-name" />        # when the discriminator is run on the generator samples.<br class="title-page-name" />        relu7 = tf.maximum(alpha * x7, x7)<br class="title-page-name" />        <br class="title-page-name" />        # Flatten it by global average pooling<br class="title-page-name" />        features = raise NotImplementedError()<br class="title-page-name" />        <br class="title-page-name" />        # Set class_logits to be the inputs to a softmax distribution over the different classes<br class="title-page-name" />        raise NotImplementedError()<br class="title-page-name" />        <br class="title-page-name" />        <br class="title-page-name" />        # Set gan_logits such that P(input is real | input) = sigmoid(gan_logits).<br class="title-page-name" />        # Keep in mind that class_logits gives you the probability distribution over all the real<br class="title-page-name" />        # classes and the fake class. You need to work out how to transform this multiclass softmax<br class="title-page-name" />        # distribution into a binary real-vs-fake decision that can be described with a sigmoid.<br class="title-page-name" />        # Numerical stability is very important.<br class="title-page-name" />        # You'll probably need to use this numerical stability trick:<br class="title-page-name" />        # log sum_i exp a_i = m + log sum_i exp(a_i - m).<br class="title-page-name" />        # This is numerically stable when m = max_i a_i.<br class="title-page-name" />        # (It helps to think about what goes wrong when...<br class="title-page-name" />        # 1. One value of a_i is very large<br class="title-page-name" />        # 2. All the values of a_i are very negative<br class="title-page-name" />        # This trick and this value of m fix both those cases, but the naive implementation and<br class="title-page-name" />        # other values of m encounter various problems)<br class="title-page-name" />        raise NotImplementedError()<br class="title-page-name" />        <br class="title-page-name" />        return out, class_logits, gan_logits, features</pre>
<p class="calibre2"><span class="calibre10">Calculate the loss:</span></p>
<pre class="calibre20">def model_loss(input_real, input_z, output_dim, y, num_classes, label_mask, alpha=0.2, drop_rate=0.):<br class="title-page-name" />    """<br class="title-page-name" />    Get the loss for the discriminator and generator<br class="title-page-name" />    :param input_real: Images from the real dataset<br class="title-page-name" />    :param input_z: Z input<br class="title-page-name" />    :param output_dim: The number of channels in the output image<br class="title-page-name" />    :param y: Integer class labels<br class="title-page-name" />    :param num_classes: The number of classes<br class="title-page-name" />    :param alpha: The slope of the left half of leaky ReLU activation<br class="title-page-name" />    :param drop_rate: The probability of dropping a hidden unit<br class="title-page-name" />    :return: A tuple of (discriminator loss, generator loss)<br class="title-page-name" />    """<br class="title-page-name" />    <br class="title-page-name" />    <br class="title-page-name" />    # These numbers multiply the size of each layer of the generator and the discriminator,<br class="title-page-name" />    # respectively. You can reduce them to run your code faster for debugging purposes.<br class="title-page-name" />    g_size_mult = 32<br class="title-page-name" />    d_size_mult = 64<br class="title-page-name" />    <br class="title-page-name" />    # Here we run the generator and the discriminator<br class="title-page-name" />    g_model = generator(input_z, output_dim, alpha=alpha, size_mult=g_size_mult)<br class="title-page-name" />    d_on_data = discriminator(input_real, alpha=alpha, drop_rate=drop_rate, size_mult=d_size_mult)<br class="title-page-name" />    d_model_real, class_logits_on_data, gan_logits_on_data, data_features = d_on_data<br class="title-page-name" />    d_on_samples = discriminator(g_model, reuse=True, alpha=alpha, drop_rate=drop_rate, size_mult=d_size_mult)<br class="title-page-name" />    d_model_fake, class_logits_on_samples, gan_logits_on_samples, sample_features = d_on_samples<br class="title-page-name" />    <br class="title-page-name" />    <br class="title-page-name" />    # Here we compute `d_loss`, the loss for the discriminator.<br class="title-page-name" />    # This should combine two different losses:<br class="title-page-name" />    # 1. The loss for the GAN problem, where we minimize the cross-entropy for the binary<br class="title-page-name" />    # real-vs-fake classification problem.<br class="title-page-name" />    # 2. The loss for the SVHN digit classification problem, where we minimize the cross-entropy<br class="title-page-name" />    # for the multi-class softmax. For this one we use the labels. Don't forget to ignore<br class="title-page-name" />    # use `label_mask` to ignore the examples that we are pretending are unlabeled for the<br class="title-page-name" />    # semi-supervised learning problem.<br class="title-page-name" />    raise NotImplementedError()<br class="title-page-name" />    <br class="title-page-name" />    # Here we set `g_loss` to the "feature matching" loss invented by Tim Salimans at OpenAI.<br class="title-page-name" />    # This loss consists of minimizing the absolute difference between the expected features<br class="title-page-name" />    # on the data and the expected features on the generated samples.<br class="title-page-name" />    # This loss works better for semi-supervised learning than the tradition GAN losses.<br class="title-page-name" />    raise NotImplementedError()<br class="title-page-name" /><br class="title-page-name" />    pred_class = tf.cast(tf.argmax(class_logits_on_data, 1), tf.int32)<br class="title-page-name" />    eq = tf.equal(tf.squeeze(y), pred_class)<br class="title-page-name" />    correct = tf.reduce_sum(tf.to_float(eq))<br class="title-page-name" />    masked_correct = tf.reduce_sum(label_mask * tf.to_float(eq))<br class="title-page-name" />    <br class="title-page-name" />    return d_loss, g_loss, correct, masked_correct, g_model</pre>
<p class="calibre2">Add the optimizers:</p>
<pre class="calibre20">def model_opt(d_loss, g_loss, learning_rate, beta1):<br class="title-page-name" />    """<br class="title-page-name" />    Get optimization operations<br class="title-page-name" />    :param d_loss: Discriminator loss Tensor<br class="title-page-name" />    :param g_loss: Generator loss Tensor<br class="title-page-name" />    :param learning_rate: Learning Rate Placeholder<br class="title-page-name" />    :param beta1: The exponential decay rate for the 1st moment in the optimizer<br class="title-page-name" />    :return: A tuple of (discriminator training operation, generator training operation)<br class="title-page-name" />    """<br class="title-page-name" />    # Get weights and biases to update. Get them separately for the discriminator and the generator<br class="title-page-name" />    raise NotImplementedError()<br class="title-page-name" /><br class="title-page-name" />    # Minimize both players' costs simultaneously<br class="title-page-name" />    raise NotImplementedError()<br class="title-page-name" />    shrink_lr = tf.assign(learning_rate, learning_rate * 0.9)<br class="title-page-name" />    <br class="title-page-name" />    return d_train_opt, g_train_opt, shrink_lr</pre>
<p class="calibre2"><span class="calibre10">Build the network model:</span></p>
<pre class="calibre20">class GAN:<br class="title-page-name" />    """<br class="title-page-name" />    A GAN model.<br class="title-page-name" />    :param real_size: The shape of the real data.<br class="title-page-name" />    :param z_size: The number of entries in the z code vector.<br class="title-page-name" />    :param learnin_rate: The learning rate to use for Adam.<br class="title-page-name" />    :param num_classes: The number of classes to recognize.<br class="title-page-name" />    :param alpha: The slope of the left half of the leaky ReLU activation<br class="title-page-name" />    :param beta1: The beta1 parameter for Adam.<br class="title-page-name" />    """<br class="title-page-name" />    def __init__(self, real_size, z_size, learning_rate, num_classes=10, alpha=0.2, beta1=0.5):<br class="title-page-name" />        tf.reset_default_graph()<br class="title-page-name" />        <br class="title-page-name" />        self.learning_rate = tf.Variable(learning_rate, trainable=False)<br class="title-page-name" />        inputs = model_inputs(real_size, z_size)<br class="title-page-name" />        self.input_real, self.input_z, self.y, self.label_mask = inputs<br class="title-page-name" />        self.drop_rate = tf.placeholder_with_default(.5, (), "drop_rate")<br class="title-page-name" />        <br class="title-page-name" />        loss_results = model_loss(self.input_real, self.input_z,<br class="title-page-name" />                                  real_size[2], self.y, num_classes,<br class="title-page-name" />                                  label_mask=self.label_mask,<br class="title-page-name" />                                  alpha=0.2,<br class="title-page-name" />                                  drop_rate=self.drop_rate)<br class="title-page-name" />        self.d_loss, self.g_loss, self.correct, self.masked_correct, self.samples = loss_results<br class="title-page-name" />        <br class="title-page-name" />        self.d_opt, self.g_opt, self.shrink_lr = model_opt(self.d_loss, self.g_loss, self.learning_rate, beta1)</pre>
<p class="calibre2"><span class="calibre10">Train and persist the model:</span></p>
<pre class="calibre20">def train(net, dataset, epochs, batch_size, figsize=(5,5)):<br class="title-page-name" />    <br class="title-page-name" />    saver = tf.train.Saver()<br class="title-page-name" />    sample_z = np.random.normal(0, 1, size=(50, z_size))<br class="title-page-name" /><br class="title-page-name" />    samples, train_accuracies, test_accuracies = [], [], []<br class="title-page-name" />    steps = 0<br class="title-page-name" /><br class="title-page-name" />    with tf.Session() as sess:<br class="title-page-name" />        sess.run(tf.global_variables_initializer())<br class="title-page-name" />        for e in range(epochs):<br class="title-page-name" />            print("Epoch",e)<br class="title-page-name" />            <br class="title-page-name" />            t1e = time.time()<br class="title-page-name" />            num_examples = 0<br class="title-page-name" />            num_correct = 0<br class="title-page-name" />            for x, y, label_mask in dataset.batches(batch_size):<br class="title-page-name" />                assert 'int' in str(y.dtype)<br class="title-page-name" />                steps += 1<br class="title-page-name" />                num_examples += label_mask.sum()<br class="title-page-name" /><br class="title-page-name" />                # Sample random noise for G<br class="title-page-name" />                batch_z = np.random.normal(0, 1, size=(batch_size, z_size))<br class="title-page-name" /><br class="title-page-name" />                # Run optimizers<br class="title-page-name" />                t1 = time.time()<br class="title-page-name" />                _, _, correct = sess.run([net.d_opt, net.g_opt, net.masked_correct],<br class="title-page-name" />                                         feed_dict={net.input_real: x, net.input_z: batch_z,<br class="title-page-name" />                                                    net.y : y, net.label_mask : label_mask})<br class="title-page-name" />                t2 = time.time()<br class="title-page-name" />                num_correct += correct<br class="title-page-name" /><br class="title-page-name" />            sess.run([net.shrink_lr])<br class="title-page-name" />            <br class="title-page-name" />            <br class="title-page-name" />            train_accuracy = num_correct / float(num_examples)<br class="title-page-name" />            <br class="title-page-name" />            print("\t\tClassifier train accuracy: ", train_accuracy)<br class="title-page-name" />            <br class="title-page-name" />            num_examples = 0<br class="title-page-name" />            num_correct = 0<br class="title-page-name" />            for x, y in dataset.batches(batch_size, which_set="test"):<br class="title-page-name" />                assert 'int' in str(y.dtype)<br class="title-page-name" />                num_examples += x.shape[0]<br class="title-page-name" /><br class="title-page-name" />                correct, = sess.run([net.correct], feed_dict={net.input_real: x,<br class="title-page-name" />                                                   net.y : y,<br class="title-page-name" />                                                   net.drop_rate: 0.})<br class="title-page-name" />                num_correct += correct<br class="title-page-name" />            <br class="title-page-name" />            test_accuracy = num_correct / float(num_examples)<br class="title-page-name" />            print("\t\tClassifier test accuracy", test_accuracy)<br class="title-page-name" />            print("\t\tStep time: ", t2 - t1)<br class="title-page-name" />            t2e = time.time()<br class="title-page-name" />            print("\t\tEpoch time: ", t2e - t1e)<br class="title-page-name" />            <br class="title-page-name" />            <br class="title-page-name" />            gen_samples = sess.run(<br class="title-page-name" />                                   net.samples,<br class="title-page-name" />                                   feed_dict={net.input_z: sample_z})<br class="title-page-name" />            samples.append(gen_samples)<br class="title-page-name" />            _ = view_samples(-1, samples, 5, 10, figsize=figsize)<br class="title-page-name" />            plt.show()<br class="title-page-name" />            <br class="title-page-name" />            <br class="title-page-name" />            # Save history of accuracies to view after training<br class="title-page-name" />            train_accuracies.append(train_accuracy)<br class="title-page-name" />            test_accuracies.append(test_accuracy)<br class="title-page-name" />            <br class="title-page-name" /><br class="title-page-name" />        saver.save(sess, './checkpoints/generator.ckpt')<br class="title-page-name" /><br class="title-page-name" />    with open('samples.pkl', 'wb') as f:<br class="title-page-name" />        pkl.dump(samples, f)<br class="title-page-name" />    <br class="title-page-name" />    return train_accuracies, test_accuracies, samples</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-39">
        <section>

                            <header>
                    <h1 class="header-title">Deep convolutional GAN</h1>
                </header>
            
            <article>
                
<p class="calibre2"><strong class="calibre13">Deep convolutional GAN</strong>, also called <strong class="calibre13">DCGAN</strong>, is used to generate color images. Here we use a convolutional layer<span class="calibre10">&nbsp;in the generator and discriminator. We'll also need to use batch normalization to get the GAN to train appropriately. We will discuss batch normalization&nbsp;in detail in the performance improvement of deep neural networks chapter. We'll be training GAN on the SVHN dataset; a small example is shown in the following figure. After training, the generator will be able to create images that are nearly identical to these images. You can download the code for this example:</span></p>
<div class="mce-root"><img src="images/000007.png" class="calibre26" /></div>
<div class="cdpaligncenter">Google Street View house numbers view</div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-161">
        <section>

                            <header>
                    <h1 class="header-title">Batch normalization</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre10">Batch normalization is a technique for improving the performance and stability of neural networks. The idea is to normalize the layer inputs so that they have a mean of zero and variance of 1. Batch normalization was introduced in Sergey Ioffe's and Christian Szegedy's 2015 paper,&nbsp;<em class="calibre19">Batch Normalization is Necessary to Make DCGANs Work</em>.&nbsp;The idea is that instead of just normalizing the inputs to the network, we normalize the inputs to&nbsp;layers within&nbsp;the network. It's called <strong class="calibre13">batch</strong>&nbsp;<strong class="calibre13">normalization</strong> because during training, we normalize each layer's input by using the mean and variance of the values in the current mini-batch.</span></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-49">
        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we have seen how the GAN model truly displays the power of CNN. We learned how to train our own generative model and saw a practical example of GAN that <span class="calibre10">can generate photos from paintings and turn horses into zebras</span>.</p>
<p class="calibre2">We understood how GAN differs from other discriminative models and learned why generative models are preferred.</p>
<p class="calibre2"><span class="calibre10">In the next chapter, we will learn about deep learning software comparison from scratch.</span></p>
<p class="calibre2"></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-162">
        <section>

                            <header>
                    <h1 class="header-title">Attention Mechanism for CNN and Visual Models</h1>
                </header>
            
            <article>
                
<p class="calibre2">Not everything in an image or text&mdash;or in general, any data&mdash;is equally relevant from the perspective of insights that we need to draw from it. For example, consider a task where we are trying to predict the next word in a sequence of a verbose statement like <em class="calibre19">Alice and Alya are friends. Alice lives in France and works in Paris. Alya is British and works in London. Alice prefers to buy books written in French, whereas Alya prefers books in _____.</em></p>
<p class="calibre2">When this example is given to a human, even a child with decent language proficiency can very well predict the next word will most probably be <em class="calibre19">English</em>. Mathematically, and in the context of deep learning, this can similarly be ascertained by creating a vector embedding of these words and then computing the results using vector mathematics, as follows:</p>
<div class="mce-root"><img class="fm-editor-equation6" src="images/000097.png" /></div>
<p class="calibre2">Here, <em class="calibre19">V(Word)</em> is the vector embedding for the required word; similarly, <em class="calibre19">V(French)</em>, <em class="calibre19">V(Paris)</em>, and <em class="calibre19">V(London)</em> are the required vector embeddings for the words <em class="calibre19">French</em>, <em class="calibre19">Paris</em>, and <em class="calibre19">London</em>, respectively.</p>
<div class="packt_infobox">Embeddings are (often) lower dimensional and dense (numerical) vector representations of inputs or indexes of inputs (for non-numerical data); in this case, text.</div>
<div class="packt_tip">Algorithms such as <kbd class="calibre12">Word2Vec</kbd> and <kbd class="calibre12">glove</kbd> can be used to get word embeddings. Pretrained variants of these models for general texts are available in popular Python-based NLP libraries, such as SpaCy, Gensim&nbsp;and&nbsp;others can also be trained using most deep learning libraries, such as Keras, TensorFlow, and so on.</div>
<div class="packt_infobox">The concept of embeddings is as much relevant to vision and images as it is to text.</div>
<p class="calibre2">There may not be an existing vector exactly matching the vector we obtained just now in the form of <img class="fm-editor-equation7" src="images/000083.png" />;&nbsp;but if we try to find the one closest to the so obtained&nbsp;<img class="fm-editor-equation8" src="images/000017.png" /> that exists and find the representative word using reverse indexing, that word would most likely be the same as what we as humans thought of earlier, that is, <em class="calibre19">English</em>.</p>
<div class="packt_tip">Algorithms such as cosine similarity can be used to get the vector closest to the computed one.</div>
<div class="packt_infobox">For implementation, a computationally more efficient way of finding the closest vector would be <strong class="calibre1">approximate nearest neighbor</strong> (<strong class="calibre1">ANN</strong>), as available in Python's <kbd class="calibre12">annoy</kbd> library.</div>
<p class="calibre2">Though we have helped get the same results, both cognitively and through deep learning approaches, the input in&nbsp;both the cases was not the same. To humans, we had given the exact sentence as to the computer, but for deep learning applications, we had carefully picked the correct words (<em class="calibre19">French</em>, <em class="calibre19">Paris</em>, and <em class="calibre19">London</em>) and their right position in the equation to get the results. Imagine how we can very easily&nbsp;<span class="calibre10">realize</span><span class="calibre10">&nbsp;</span><span class="calibre10">the right words to pay attention to in order to understand the correct context, and hence we have the results; but in the current form, it was not possible for our deep learning approach to do the same.</span></p>
<p class="calibre2">Now there are quite sophisticated algorithms in language modeling using different variants and architectures of RNN, such as LSTM and Seq2Seq, respectively. These could have solved this problem and got the right solution, but they are most effective in shorter and more direct sentences, such as <em class="calibre19">Paris is to French what London is to _____</em>. In order to correctly understand a long sentence and generate the correct result, it is important to have a mechanism to teach the architecture whereby specific words need to be paid more attention to in a long sequence of words. This is called the <strong class="calibre13">attention mechanism</strong> in deep learning, and it is applicable to many types of deep learning applications but in slightly different ways.</p>
<div class="packt_infobox"><strong class="calibre1">RNN</strong> stands for <strong class="calibre1">recurrent neural networks</strong> and is used to depict a temporal sequence of data in deep learning. Due to the vanishing gradient problem, RNN is seldom used directly; instead, its variants, such as <strong class="calibre1">LSTM</strong> (<strong class="calibre1">Long-Short Term Memory</strong>) and&nbsp;<strong class="calibre1">GRU</strong> (<strong class="calibre1">Gated Recurrent Unit</strong>) are more popular in actual implementations.</div>
<div class="packt_infobox"><strong class="calibre1">Seq2Seq</strong> stands for <strong class="calibre1">Sequence-to-Sequence</strong> models and comprises two RNN (or variant) networks (hence it is called <strong class="calibre1">Seq2Seq</strong>, where each RNN network represents a sequence); one acts as an encoder and the other as a&nbsp;decoder. The two RNN networks can be multi-layer or stacked RNN networks, and they are connected via a thought or context vector. Additionally, Seq2Seq models may use the attention&nbsp;mechanism to improve performance, especially for longer sequences.&nbsp;</div>
<p class="calibre2"><span class="calibre10">In fact, to be more precise, even we had to process the preceding information in layers, first understanding that the last sentence is about Alya. Then we can identify and extract Alya's city, then that for Alice, and so on. Such a layered way of human thinking is analogous to stacking in deep learning, and hence in similar applications, stacked architectures are quite common.</span></p>
<div class="packt_tip">To know more about how stacking works in deep learning, especially with sequence-based architectures, explore topics such as stacked RNN and stacked attention networks.</div>
<p class="calibre2">In this chapter, we will cover the following topics:</p>
<ul class="calibre7">
<li class="calibre8">Attention mechanism for image captioning</li>
<li class="calibre8">Types of attention (Hard, and Soft Attentions)</li>
<li class="calibre8">Using attention to improve visual models
<ul class="calibre84">
<li class="calibre8">Recurrent models of visual attention</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-62">
        <section>

                            <header>
                    <h1 class="header-title">Attention mechanism for image captioning</h1>
                </header>
            
            <article>
                
<p class="calibre2">From the introduction, so far, it must be clear to you that the attention mechanism works on a sequence of objects, assigning each element in the sequence a weight for a specific iteration of a required output. With every next step, not only the sequence but also the weights in the attention mechanism <span class="calibre10">can change</span>. So, attention-based architectures are <span class="calibre10">essentially</span> sequence networks, best implemented in deep learning using RNNs (or their variants).</p>
<p class="calibre2">The question now is: how do we implement a sequence-based attention on a static image, especially the one represented in a <strong class="calibre13">convolutional neural network</strong> (<strong class="calibre13">CNN</strong>)? Well, let's take an example that sits right in between a text and image to understand this. Assume that we need to caption an image with respect to its contents.</p>
<p class="calibre2">We have some images with captions provided by humans as training data and using this, we need to create a system that can provide a decent caption for any new image not seen earlier by the model. As seen earlier, let's take an example and see how we, as humans, will perceive this task and the analogous process to it that needs to be implemented in deep learning and CNN. Let's consider the following image and conceive some plausible captions for it. We'll also rank them heuristically using human judgment:</p>
<div class="mce-root"><img src="images/00053.jpeg" class="calibre85" /></div>
<p class="calibre2">Some probable captions (in order of most likely to least likely) are:</p>
<ul class="calibre7">
<li class="calibre8">Woman seeing dog in snow forest</li>
<li class="calibre8">Brown dog in snow</li>
<li class="calibre8">A person wearing cap in woods and white land</li>
<li class="calibre8">Dog, tree, snow, person, and sunshine</li>
</ul>
<p class="calibre2">An important thing to note here is that, despite the fact that the woman is central to the image and the dog is not the biggest object in the image, the caption we sought probable focused on them and then their surroundings here. This is because we consider them as important entities here (given no prior context). So as humans, how we reached this conclusion is as follows: we first glanced the whole image, and then we focused towards the woman, in high resolution, while putting everything in the background (assume a <strong class="calibre13">Bokeh</strong>&nbsp;effect in a dual-camera phone).&nbsp;<span class="calibre10">We identified the caption part for that,</span>&nbsp;and then the dog in high resolution while putting everything else in low resolution; and we appended the caption part. Finally, we did the same for the surroundings and caption part for that.&nbsp;</p>
<p class="calibre2">So essentially, we saw it in this sequence to reach to the first caption:</p>
<div class="mce-root"><img src="images/000039.jpg" class="calibre86" />&nbsp;</div>
<div class="cdpaligncenter"><span>Image 1: Glance the image first</span></div>
<div class="mce-root"><img src="images/000015.jpg" class="calibre87" />&nbsp;</div>
<div class="cdpaligncenter"><span>Image 2: Focus on woman</span></div>
<div class="mce-root">&nbsp;<img src="images/000107.jpg" class="calibre88" /></div>
<div class="cdpaligncenter"><span>Image 3:&nbsp;</span>Focus on dog</div>
<div class="cdpaligncenter1"><img src="images/000091.jpg" class="calibre85" /></div>
<div class="cdpaligncenter"><span>Image 4: Focus on snow</span></div>
<div class="cdpaligncenter1"><img src="images/000072.jpg" class="calibre89" /></div>
<div class="cdpaligncenter"><span>Image 5:&nbsp;</span>Focus on forest</div>
<p class="calibre2">In terms of weight of attention or focus, after glancing the image, we focus on the first most important object: the woman here. This is analogous to creating a mental frame in which we put the part of the image with the woman in high-resolution and the remaining part of the image in low-resolution.</p>
<p class="calibre2">In a deep learning reference, the attention sequence will have the highest weight for the vector (embedding) representing the concept of the woman for this part of the sequence. In the next step of the output/sequence, the weight will shift more towards the vector representation for the dog and so on.</p>
<p class="calibre2">To understand this intuitively, we convert the image represented in the form of CNN into a flattened vector or some other similar structure; then we create different splices of the image or sequences with different parts in varying resolutions. Also, as we understand now from our discussion in <a href="#calibre_link-31" target="_blank" class="calibre11">Chapter 7</a>, <em class="calibre19">Object-Detection &amp; Instance-Segmentation with CNN</em>, we must have the relevant portions that we need to detect in varying scales as well for effective detection. The same concept applies here too, and besides resolution, we also vary the scale; but for now, we will keep it simple and ignore the scale part for intuitive understanding.</p>
<p class="calibre2">These splices or sequences of images now act as a sequence of words, as in our earlier example, and hence they can be treated inside an RNN/LSTM or similar sequence-based architecture for the purpose of attention. This is done to get the best-suited word as the output in every iteration. So the first iteration of the sequence leads to woman (from the weights of a sequence representing an object represented as a <em class="calibre19">Woman</em> in <em class="calibre19">Image 2</em>) â then the next iteration as â&nbsp;<em class="calibre19">seeing</em> (from a sequence identifying the back of the <em class="calibre19">Woman</em> as in <em class="calibre19">Image 2</em>)&nbsp;â <em class="calibre19">Dog</em> (sequence as in <em class="calibre19">Image 3</em>)&nbsp;â <span class="calibre10"><em class="calibre19">in</em> (from a sequence where everything is blurred generating <em class="calibre19">filler</em> words transitioning from entities to surroundings)&nbsp;â <em class="calibre19">Snow</em> (sequence as in <em class="calibre19">Image 4</em>)&nbsp;â <em class="calibre19">Forest</em> (sequence as in <em class="calibre19">Image 5</em>).</span></p>
<p class="calibre2">Filler words such as&nbsp;<em class="calibre19">in</em>&nbsp;and action words such as&nbsp;<em class="calibre19">seeing</em>&nbsp;can also be automatically learned when the best image splice/sequence mapping to human-generated captions is done across several images. But for the simpler version, a caption such as&nbsp;<em class="calibre19">Woman</em>, <em class="calibre19">Dog</em>, <em class="calibre19">Snow</em>, and <em class="calibre19">Forest</em>&nbsp;can also be a good depiction of entities and surroundings in the image.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-163">
        <section>

                            <header>
                    <h1 class="header-title">Types of Attention</h1>
                </header>
            
            <article>
                
<p class="calibre2">There are two types attention mechanisms. They are as follows:</p>
<ul class="calibre7">
<li class="calibre8">Hard attention</li>
<li class="calibre8">Soft attention</li>
</ul>
<p class="calibre2">Let's now take a look at each one in detail in the following sections.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-75">
        <section>

                            <header>
                    <h1 class="header-title">Hard Attention</h1>
                </header>
            
            <article>
                
<p class="calibre2">In reality, in our recent image caption example, several more pictures would be selected, but due to our training with the handwritten captions, those would never be weighted higher. However, the essential thing to understand is how the system would understand what all pixels (or more precisely, the CNN representations of them) the system focuses on to draw these high-resolution images of different aspects and then how to choose the next pixel to repeat the process.</p>
<p class="calibre2">In the preceding example, the points are chosen at random from a distribution and the process is repeated. Also, which pixels around this point get a higher resolution is decided inside the attention network. This type of attention is known as <strong class="calibre13">hard attention</strong>.</p>
<p class="calibre2">Hard attention has something called the <strong class="calibre13">differentiability problem</strong>. Let's spend some time understanding this. We know that in deep learning the networks have to be trained and to train them we iterate across training batches in order to minimize the loss function. We can minimize the loss function by changing the weights in the direction of the gradient of the minima, which in turn is arrived at after differentiating the loss function<em class="calibre19">.</em></p>
<div class="packt_infobox">This process of minimizing losses across layers of a deep network, starting from the last layer to the first, is known as <strong class="calibre1">back-propagation</strong>.</div>
<div class="packt_tip"><span>Examples of some differentiable loss functions used in deep learning and machine learning are the log-likelihood loss function, squared-error loss function, binomial and multinominal cross-entropy, and so on.</span></div>
<p class="calibre2">However, since the points are chosen <span class="calibre10">randomly&nbsp;</span>in each iteration in hard attention&mdash;and since such a random pixel choosing mechanism is not a differentiable function<span class="calibre10">&mdash;</span>we <span class="calibre10">essentially&nbsp;</span>cannot train this attention mechanism, as explained. This problem is overcome either by using <strong class="calibre13">Reinforcement Learning</strong> (<strong class="calibre13">RL</strong>) or by switching to soft attention.</p>
<div class="packt_infobox"><span>RL involves mechanisms of solving two problems, either separately or in combination. The first is called the <strong class="calibre1">control problem</strong>, which determines the most optimal action that the agent should take in each step given its state, and the second is the <strong class="calibre1">prediction problem</strong>, which determines the optimal <em class="calibre29">value</em> of the state.</span></div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-164">
        <section>

                            <header>
                    <h1 class="header-title">Soft Attention</h1>
                </header>
            
            <article>
                
<p class="calibre2">As introduced in the preceding sub-section on hard attention, soft attention uses RL to progressively train and determine where to seek next (<span class="calibre10">con</span><span class="calibre10">trol problem</span><span class="calibre10">)</span>.</p>
<p class="calibre2">There exist two major problems with using the combination of hard attention and RL to achieve the required objective:</p>
<ul class="calibre7">
<li class="calibre8">It becomes slightly complicated to involve RL and train an RL agent and an RNN/deep network based on it separately.</li>
<li class="calibre8">The variance in the gradient of the policy function is not only high (as in <strong class="calibre1">A3C</strong> model), but also has a computational complexity of <em class="calibre29">O(N)</em>, where <em class="calibre29">N</em> is the number of units in the network. This increases the computation load for such approaches massively. Also, given that the attention mechanism adds more value in overly long sequences (of words or image embedding splices)&mdash;and to train networks involving longer sequences requires larger memory, and hence much deeper networks<span>&mdash;</span>this approach is computationally not very efficient.</li>
</ul>
<div class="packt_infobox">The <strong class="calibre1">Policy Function&nbsp;</strong>in RL, determined as <em class="calibre29">Q(a,s)</em>, is the function used to determine the optimal policy or the action <em class="calibre29">(a)</em> that should be taken in any given state <em class="calibre29">(s)</em> to maximize the rewards.</div>
<p class="calibre2">So what is the alternative? As we discussed, the problem arose because the mechanism that we were choosing for attention led to a non-differentiable function, because of which we had to go with RL. So let's take a different approach here. Taking an analogy of our l<span class="calibre10">anguage modeling</span> problem example (as in the A<em class="calibre19">ttention Mechanism - Intuition</em>&nbsp;section) earlier, we assume that we have the vector of the tokens for the objects/ words present in the attention network. Also, in same vector space <span class="calibre10">(say in the embedding hyperspace)&nbsp;</span>we bring the tokens for the object/ words in the required query&nbsp;of the particular sequence step. On taking this approach, finding the right attention weights for the tokens in the attention network with the respect to the tokens in query space is as easy as computing the vector similarity between them; for example, a cosine distance. Fortunately, most vector distance and similarity functions are differentiable; hence the loss function derived by using such vector distance/similarity functions in such space is also differentiable, and our back-propagation can work in this scenario.</p>
<div class="packt_tip">The cosine distance between two vectors, say <img class="fm-editor-equation9" src="images/000059.png" />, and <img class="fm-editor-equation10" src="images/000020.png" />, in a multi-dimensional (three in this example) vector space is given as:<img class="fm-editor-equation11" src="images/000028.png" /></div>
<p class="calibre2">This approach of using a differentiable loss function for training an attention network is known as <strong class="calibre13">soft attention</strong>.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-30">
        <section>

                            <header>
                    <h1 class="header-title">Using attention to improve visual models</h1>
                </header>
            
            <article>
                
<p class="calibre2">As we discovered in the NLP example covered in the earlier section on Attention Mechanism - Intuition, Attention did help us a lot in both achieving new use-cases, not optimally feasible with conventional NLP, and vastly improving the performance of the existing NLP mechanism. Similar is the usage of Attention in CNN and Visual Models as well</p>
<p class="calibre2">In the earlier chapter<span class="calibre10">&nbsp;</span><a href="#calibre_link-31" target="_blank" class="calibre11">Chapter 7</a><span class="calibre10">,</span> <em class="calibre19">Object-Detection &amp;<span class="calibre10">&nbsp;</span>Instance-Segmentation with CNN</em><span class="calibre10">, we discovered how Attention (like) mechanism are used as Region Proposal Networks for networks like Faster R-CNN and Mask R-CNN, to greatly enhance and optimize the proposed regions, and enable the generation of segment masks. This corresponds to the first part of the discussion. In this section, we will cover the second part of the discussion, where we will use 'Attention' mechanism to improve the performance of our CNNs, even under extreme conditions.</span></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-26">
        <section>

                            <header>
                    <h1 class="header-title">Reasons for sub-optimal performance of visual CNN models</h1>
                </header>
            
            <article>
                
<p class="calibre2">The performance of a CNN network can be improved to a certain extent by adopting proper tuning and setup mechanisms such as: data pre-processing,&nbsp;<span class="calibre10">batch normalization</span>, optimal pre-initialization of weights; choosing the correct activation function; using techniques such as regularization to avoid overfitting; using an optimal optimization function; and training with plenty of (quality) data.</p>
<p class="calibre2">Beyond these training and architecture-related decisions, there are image-related nuances because of which the performance of visual models may be impacted. Even after controlling the aforementioned training and architectural factors, the conventional CNN-based image classifier does not work well under some of the following conditions related to the underlying images:</p>
<ul class="calibre7">
<li class="calibre8">Very big images</li>
<li class="calibre8">Highly cluttered images with a number of classification entities</li>
<li class="calibre8">Very noisy images</li>
</ul>
<p class="calibre2">Let's try to understand the reasons behind the sub-optimal performance under these conditions, and then we will logically understand what may fix the problem.</p>
<p class="calibre2">In conventional CNN-based models, even after a downsizing across layers, the computational complexity is quite high. In fact, the complexity is of the order of <img class="fm-editor-equation12" src="images/000073.png" />, where <em class="calibre19">L</em> and <em class="calibre19">W</em> are the&nbsp;length and width of the image in inches, and <em class="calibre19">PPI</em> is pixels per inch (pixel density). This translates into a linear complexity with respect to the total number of pixels (<em class="calibre19">P</em>) in the image, or <em class="calibre19">O(P)</em>. This directly answers the first point of the challenge; for higher <em class="calibre19">L</em>, <em class="calibre19">W</em>, or <em class="calibre19">PPI</em>, we need much higher computational power and time to train the network.</p>
<div class="packt_tip">Operations such as max-pooling, average-pooling, and so on help downsize the computational load drastically vis-a-vis all the computations across all the layers performed on the actual image.</div>
<p class="calibre2">If we visualize the patterns formed in each of the layers of our CNN, we would understand the intuition behind the working of the CNN and why it needs to be deep. In each subsequent layer, the CNN trains higher conceptual features, which may progressively better help understand the objects in the image layer after layer. So, in the case of MNIST, the first layer may only identify boundaries, the second the diagonals and straight-line-based shapes of the boundaries, and so on:</p>
<div class="mce-root"><img src="images/000099.jpg" class="calibre26" /></div>
<div class="cdpaligncenter">Illustrative conceptual features formed in different (initial) layers of CNN for MNIST</div>
<div class="mce-root"><img src="images/000038.jpg" class="calibre26" /></div>
<p class="calibre2">MNIST is a simple dataset, whereas&nbsp;real-life images are quite complex; this requires higher conceptual features to distinguish them, and hence more complex and much deeper networks. Moreover, in MNIST, we are trying to distinguish between similar types of objects (all handwritten numbers). Whereas in real life, the objects might differ widely, and hence the different types of features that may be required to model all such objects will be very high:</p>
<div class="mce-root"><img src="images/00010.jpeg" class="calibre26" /></div>
<p class="calibre2">This brings us to our second challenge. A cluttered image with too many objects would require a very complex network to model all these objects. Also, since there are too many objects to identify, the image resolution needs to be good to correctly extract and map the features for each object, which in turn means that the image size and the number of pixels need to be high for an effective classification. This, in turn, increases the complexity <span class="calibre10">exponentially&nbsp;</span>by combining the first two challenges.&nbsp;</p>
<div class="packt_infobox">The number of layers, and hence the complexity of popular CNN architectures used in ImageNet challenges, have been increasing over the years. Some examples are&nbsp;VGG16 &ndash; Oxford (2014)&nbsp;<span>with&nbsp;</span><span>16 layers</span><span>,&nbsp;GoogLeNet (2014)</span> <span>with&nbsp;</span><span>19 layers</span><span>, and ResNet (2015) with&nbsp;</span><span>152 layers.</span></div>
<p class="calibre2">Not all images are perfect SLR quality. Often, because of low light, image processing, low resolution, lack of stabilization,&nbsp;and so on, there may be a lot of noise introduced in the image. This is just one form of noise, one that is easier to understand. From the perspective of CNN, another form of noise can be image transition, rotation, or transformation:</p>
<div class="mce-root"><img src="images/00075.jpeg" class="calibre90" />&nbsp;</div>
<div class="cdpaligncenter">Image without noise</div>
<div class="mce-root"><img src="images/000060.jpg" class="calibre91" />&nbsp;</div>
<div class="cdpaligncenter"><span>Same image with added noise</span></div>
<p class="calibre2">In the preceding images, try reading the newspaper title&nbsp;<em class="calibre19">Business</em>&nbsp;in the image without and with noise, or identify the mobile in both the images. Difficult to do that in the image with noise, right? Similar is the detection/classification challenge with our CNN in the case of noisy images.</p>
<p class="calibre2">Even with exhaustive training, perfect hyperparameter adjustment, and techniques such as dropouts and others, these real-life challenges continue to diminish the image recognition accuracy of CNN networks. Now that we've understood the causes and intuition behind the lack of accuracy and performance in our CNNs, let's explore some ways and architectures to alleviate these challenges using visual attention.</p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-64">
        <section>

                            <header>
                    <h1 class="header-title">Recurrent models of visual attention</h1>
                </header>
            
            <article>
                
<p class="calibre2"><em class="calibre19">Recurrent models of visual attention</em>&nbsp;can be used to answer some of the challenges we covered in the earlier section. These models use the hard attention&nbsp;method, as covered in an earlier (<em class="calibre19">Types of attention</em>) section.&nbsp;<span class="calibre10">Here we use</span><span class="calibre10">&nbsp;one of the popular variants of recurrent models of visual attention, the</span> <span class="calibre10"><strong class="calibre13">Recurrent Attention Model</strong></span><span class="calibre10">&nbsp;(<strong class="calibre13">RAM</strong>).&nbsp;</span></p>
<p class="calibre2">As covered earlier, <span class="calibre10">hard attention</span>&nbsp;problems are non-differentiable and have to use RL for the control problem. The RAM thus uses RL for this optimization.&nbsp;</p>
<p class="calibre2"><span class="calibre10">A recurrent model of visual attention does not process the entire image, or even a sliding-window-based bounding box, at once.&nbsp;</span>It mimics the human eye and works on the concept of&nbsp;<em class="calibre19">Fixation</em>&nbsp;of <em class="calibre19">Gaze</em>&nbsp;at different locations of an image; with each <em class="calibre19">Fixation</em>, it incrementally combines information of importance to dynamically build up an internal representation of scenes in the image. It uses an&nbsp;<span class="calibre10">RNN to do this in a sequential manner.&nbsp;</span></p>
<p class="calibre2">The model selects the next location to Fixate to based on the RL agents control policy to maximize the reward based on the current state. The current state, in turn, is a function of all the past information and the demands of the task. Thus, it finds the next coordinate for fixation so that it can maximize the reward (<span class="calibre10">demands of the task</span>), given the information collected until now across the previous gazes in the memory snapshot of the RNN and the previously visited coordinate.</p>
<div class="packt_tip">Most RL mechanisms use the <strong class="calibre1">Markov Decision Process</strong> (<strong class="calibre1">MDP</strong>), in which the next action is determined only by the current state, irrespective of the states visited earlier. By using RNN here, important information from previous <em class="calibre29">Fixations</em> can be combined in the present state itself.&nbsp;</div>
<p class="calibre2">The preceding mechanism solves the last two problems highlighted in CNN in the earlier section. Also, in the RAM, the&nbsp;<span class="calibre10">number of parameters and&nbsp;amount of computation it performs can be controlled independently of the size of the input image, thus solving the first problem as well.</span></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-15">
        <section>

                            <header>
                    <h1 class="header-title">Applying the RAM on a noisy MNIST sample</h1>
                </header>
            
            <article>
                
<p class="calibre2">To understand the working of the&nbsp;<span class="calibre10">RAM</span>&nbsp;in greater detail, let's try to create an MNIST sample incorporating some of the problems as highlighted in the earlier section:</p>
<div class="mce-root"><img src="images/000026.jpg" class="calibre92" /></div>
<div class="cdpaligncenter">Larger image of noisy and distorted MNIST</div>
<p class="calibre2">The preceding image represents a larger image/collage using an actual and slightly noisy sample of an MNIST image (of number <strong class="calibre13">2</strong>), and a lot of other distortions and snippets of other partial samples. Also, the actual digit <strong class="calibre13">2</strong> here is not centered. This example represents all the previously stated problems, yet it is simple enough to understand the working of the RAM.</p>
<p class="calibre2">The RAM uses the concept of a&nbsp;<strong class="calibre13">Glimpse Sensor</strong>. The RL agent fixes its gaze at a particular coordinate (<em class="calibre19">l</em>) and particular time (<em class="calibre19">t-1</em>). The coordinate at time t-1,&nbsp;<em class="calibre19">l<sub class="calibre40">t-1</sub></em>&nbsp;of the image <em class="calibre19">x<sub class="calibre40">t&nbsp;</sub></em>and uses the <strong class="calibre13">Glimpse Sensor</strong>&nbsp;to extract retina-like multiple-resolution patches of the image with&nbsp;<em class="calibre19">l<sub class="calibre40">t-1</sub></em>&nbsp;as the center. These representations, extracted at time <em class="calibre19">t-1</em>, are collectively called <em class="calibre19">p(x<sub class="calibre40">t</sub></em>, <em class="calibre19">l<sub class="calibre40">t-1</sub>)</em>:</p>
<div class="mce-root"><img src="images/000008.jpg" class="calibre93" /></div>
<div class="cdpaligncenter">The concept of the Glimpse Sensor</div>
<div class="mce-root"><img src="images/000078.jpg" class="calibre94" />&nbsp;&nbsp;;<img src="images/000067.jpg" class="calibre95" /></div>
<p class="calibre2">These images show the representations of our image across two fixations using the <strong class="calibre13">Glimpse Sensor</strong>.</p>
<p class="calibre2">The representations obtained from the <strong class="calibre13">Glimpse Sensor</strong>&nbsp;are passes through the 'Glimpse Network, which flattens the representation at two stages. In the first stage, the representations from the <strong class="calibre13">Glimpse Sensor</strong>&nbsp;and the <strong class="calibre13">Glimpse Network</strong> are flattened separately (<img class="fm-editor-equation13" src="images/000047.png" />), and then they are combined into a single flattened layer (<img class="fm-editor-equation14" src="images/000106.png" />) to generate the output representation <em class="calibre19">g<sub class="calibre40">t</sub>&nbsp;</em>for time <em class="calibre19">t</em>:</p>
<div class="mce-root"><img src="images/000081.jpg" class="calibre96" /></div>
<div class="cdpaligncenter">The concept of the Glimpse Network</div>
<p class="calibre2">These output representations are then passed through the RNN model architecture. The fixation for the next step in the iteration is determined by the RL agent to maximize the reward from this architecture:</p>
<div class="mce-root">&nbsp;<img src="images/000085.jpg" class="calibre97" /></div>
<div class="cdpaligncenter">Model architecture (RNN)</div>
<p class="calibre2">As can be intuitively understood, the Glimpse Sensor captures important information across fixations, which can help identify important concepts. For example, the multiple resolution (here 3) representations at the Fixation represented by our second sample image <span class="calibre10">have&nbsp;</span>three resolutions as marked (red, green, and blue in order of decreasing resolution). As can be seen, even if these are used directly, we have got a varying capability to detect the right digit represented by this noisy collage:</p>
<div class="mce-root"><img src="images/000067.jpg" class="calibre98" /></div>
<div class="mce-root"><img src="images/000033.jpg" class="calibre99" /></div>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-165">
        <section>

                            <header>
                    <h1 class="header-title">Glimpse Sensor in code</h1>
                </header>
            
            <article>
                
<p class="calibre2">As discussed in the earlier section, the Glimpse Sensor is a powerful concept. Combined with other concepts, such as RNN and RL, as discussed earlier, it is at the heart of improving the performance of visual models.</p>
<p class="calibre2">Let's see this in greater detail here. The code is commented at every line for easy understanding and is self-explanatory:</p>
<pre class="calibre20"><br class="title-page-name" /><span>import </span>tensorflow <span>as </span>tf<br class="title-page-name" /># the code is in tensorflow<br class="title-page-name" /><span>import </span>numpy <span>as </span>np<br class="title-page-name" /><br class="title-page-name" /><br class="title-page-name" /><span>def </span>glimpseSensor(image, fixationLocation):<br class="title-page-name" />    <span>'''<br class="title-page-name" /></span><span>    Glimpse Sensor for Recurrent Attention Model (RAM)<br class="title-page-name" /></span><span>    </span><span>:param</span><span> image: the image xt<br class="title-page-name" /></span><span>    </span><span>:type</span><span> image: numpy vector<br class="title-page-name" /></span><span>    </span><span>:param</span><span> fixationLocation: cordinates l for fixation center<br class="title-page-name" /></span><span>    </span><span>:type</span><span> fixationLocation: tuple<br class="title-page-name" /></span><span>    </span><span>:return</span><span>: Multi Resolution Representations from Glimpse Sensor<br class="title-page-name" /></span><span>    </span><span>:rtype</span><span>:<br class="title-page-name" /></span><span>    '''<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>    </span>img_size=np.asarray(image).shape[:<span>2</span>]<br class="title-page-name" />    <span># this can be set as default from the size of images in our dataset, leaving the third 'channel' dimension if any<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>    </span>channels=<span>1<br class="title-page-name" /></span><span>    </span><span># settings channels as 1 by default<br class="title-page-name" /></span><span>    </span><span>if </span>(np.asarray(img_size).shape[<span>0</span>]==<span>3</span>):<br class="title-page-name" />        channels=np.asarray(image).shape[-<span>1</span>]<br class="title-page-name" />    <span># re-setting the channel size if channels are present<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>    </span>batch_size=<span>32<br class="title-page-name" /></span><span>    </span><span># setting batch size<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>    </span>loc = tf.round(((fixationLocation + <span>1</span>) / <span>2.0</span>) * img_size)<br class="title-page-name" />    <span># fixationLocation coordinates are normalized between -1 and 1 wrt image center as 0,0<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>    </span>loc = tf.cast(loc, tf.int32)<br class="title-page-name" />    <span># converting number format compatible with tf<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>    </span>image = tf.reshape(image, (batch_size, img_size[<span>0</span>], img_size[<span>1</span>], channels))<br class="title-page-name" />    <span># changing img vector shape to fit tf<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>    </span>representaions = []<br class="title-page-name" />    <span># representations of image<br class="title-page-name" /></span><span>    </span>glimpse_images = []<br class="title-page-name" />    <span># to show in window<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>    </span>minRadius=img_size[<span>0</span>]/<span>10<br class="title-page-name" /></span><span>    </span><span># setting the side size of the smallest resolution image<br class="title-page-name" /></span><span>    </span>max_radius=minRadius*<span>2<br class="title-page-name" /></span><span>    </span>offset = <span>2 </span>* max_radius<br class="title-page-name" />    <span># setting the max side and offset for drawing representations<br class="title-page-name" /></span><span>    </span>depth = <span>3<br class="title-page-name" /></span><span>    </span><span># number of representations per fixation<br class="title-page-name" /></span><span>    </span>sensorBandwidth = <span>8<br class="title-page-name" /></span><span>    </span><span># sensor bandwidth for glimpse sensor<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>    # process each image individually<br class="title-page-name" /></span><span>    </span><span>for </span>k <span>in </span><span>range</span>(batch_size):<br class="title-page-name" />        imageRepresentations = []<br class="title-page-name" />        one_img = image[k,:,:,:]<br class="title-page-name" />        <span># selecting the required images to form a batch<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>        </span>one_img = tf.image.pad_to_bounding_box(one_img, offset, offset, max_radius * <span>4 </span>+ img_size, max_radius * <span>4 </span>+ img_size)<br class="title-page-name" />        <span># pad image with zeros for use in tf as we require consistent size<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>        </span><span>for </span>i <span>in </span><span>range</span>(depth):<br class="title-page-name" />            r = <span>int</span>(minRadius * (<span>2 </span>** (i)))<br class="title-page-name" />            <span># radius of draw<br class="title-page-name" /></span><span>            </span>d_raw = <span>2 </span>* r<br class="title-page-name" />            <span># diameter<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>            </span>d = tf.constant(d_raw, <span>shape</span>=[<span>1</span>])<br class="title-page-name" />            <span># tf constant for dia<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>            </span>d = tf.tile(d, [<span>2</span>])<br class="title-page-name" />            loc_k = loc[k,:]<br class="title-page-name" />            adjusted_loc = offset + loc_k - r<br class="title-page-name" />            <span># location wrt image adjusted wrt image transformation and pad<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>            </span>one_img2 = tf.reshape(one_img, (one_img.get_shape()[<span>0</span>].value, one_img.get_shape()[<span>1</span>].value))<br class="title-page-name" />            <span># reshaping image for tf<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>            </span>representations = tf.slice(one_img2, adjusted_loc, d)<br class="title-page-name" />            <span># crop image to (d x d) for representation<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>            </span>representations = tf.image.resize_bilinear(tf.reshape(representations, (<span>1</span>, d_raw, d_raw, <span>1</span>)), (sensorBandwidth, sensorBandwidth))<br class="title-page-name" />            <span># resize cropped image to (sensorBandwidth x sensorBandwidth)<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>            </span>representations = tf.reshape(representations, (sensorBandwidth, sensorBandwidth))<br class="title-page-name" />            <span># reshape for tf<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>            </span>imageRepresentations.append(representations)<br class="title-page-name" />            <span># appending the current representation to the set of representations for image<br class="title-page-name" /></span><span><br class="title-page-name" /></span><span>        </span>representaions.append(tf.stack(imageRepresentations))<br class="title-page-name" /><br class="title-page-name" />    representations = tf.stack(representations)<br class="title-page-name" /><br class="title-page-name" />    glimpse_images.append(representations)<br class="title-page-name" />    <span># return glimpse sensor output<br class="title-page-name" /></span><span>    </span><span>return </span>representations</pre>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-40">
        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ol class="calibre15">
<li class="calibre8">Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, Yoshua Bengio, Show, Attend and Tell:<span>&nbsp;</span><em class="calibre29">Neural Image Caption Generation with Visual Attention</em>, CoRR, arXiv:1502.03044, 2015.</li>
<li class="calibre8">Karl Moritz Hermann, Tom's Kocisk, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom,<span>&nbsp;</span><em class="calibre29">Teaching Machines to Read and Comprehend</em>, CoRR, arXiv:1506.03340, 2015.</li>
<li class="calibre8">Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu,<span>&nbsp;</span><em class="calibre29">Recurrent Models of Visual Attention</em>, CoRR, arXiv:1406.6247, 2014.</li>
<li class="calibre8">Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Tat-Seng Chua, SCA-CNN:<span>&nbsp;</span><em class="calibre29">Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning</em>, CoRR, arXiv:1611.05594, 2016.</li>
<li class="calibre8">Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, Ram Nevatia, ABC-CNN:<span>&nbsp;</span><em class="calibre29">An Attention Based Convolutional Neural Network for Visual Question Answering</em>, CoRR, arXiv:1511.05960, 2015.</li>
<li class="calibre8">Wenpeng Yin, Sebastian Ebert, Hinrich Schutze,<span>&nbsp;</span><em class="calibre29">Attention-Based Convolutional Neural Network for Machine Comprehension</em>, CoRR, arXiv:1602.04341, 2016.</li>
<li class="calibre8">Wenpeng Yin, Hinrich Schutze, Bing Xiang, Bowen Zhou, ABCNN:<span>&nbsp;</span><em class="calibre29">Attention-Based Convolutional Neural Network for Modeling Sentence Pairs</em>, CoRR, arXiv:1512.05193, 2015.</li>
<li class="calibre8">Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alexander J. Smola,<span>&nbsp;</span><em class="calibre29">Stacked Attention Networks for Image Question Answering</em>, CoRR, arXiv:1511.02274, 2015.</li>
</ol>
<ol start="9" class="calibre15">
<li class="calibre8"><span>Y. Chen, D. Zhao, L. Lv and C. Li,&nbsp;<em class="calibre29">A visual attention based convolutional neural network for image classification</em>,&nbsp;</span><em class="calibre29">2016 12th World Congress on Intelligent Control and Automation (WCICA)</em><span>, Guilin, 2016, pp. 764-769.</span></li>
<li class="calibre8"><span>H. Zheng, J. Fu, T. Mei and J. Luo,&nbsp;<em class="calibre29">Learning Multi-attention Convolutional Neural Network for Fine-Grained Image Recognition</em>,&nbsp;</span><em class="calibre29">2017 IEEE International Conference on Computer Vision (ICCV)</em><span>, Venice, 2017, pp. 5219-5227.</span></li>
<li class="calibre8">Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang, Yuxin Peng, Zheng Zhang,<span>&nbsp;</span><em class="calibre29">The Application of Two-level Attention Models in Deep Convolutional Neural Network for Fine-grained Image Classification</em>, CoRR, arXiv:1411.6447, 2014.</li>
<li class="calibre8">Jlindsey15,<span>&nbsp;</span><em class="calibre29">A TensorFlow implementation of the recurrent attention model</em>, GitHub,<span>&nbsp;</span><a href="https://github.com/jlindsey15/RAM" target="_blank" class="calibre11">https://github.com/jlindsey15/RAM</a>, Feb 2018.</li>
<li class="calibre8"><span>QihongL,&nbsp;<em class="calibre29">A TensorFlow implementation of the recurrent attention model</em>, GitHub,</span><span>&nbsp;</span><a href="https://github.com/QihongL/RAM" target="_blank" class="calibre11">https://github.com/QihongL/RAM</a>, Feb 2018.</li>
<li class="calibre8"><span>Amasky,&nbsp;<em class="calibre29">Recurrent Attention Model</em>, GitHub,&nbsp;</span><a href="https://github.com/amasky/ram" target="_blank" class="calibre11">https://github.com/amasky/ram</a>, Feb 2018.</li>
</ol>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-166">
        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">The attention mechanism is the hottest topic in deep learning today and is conceived to be in the center of most of the cutting-edge algorithms under current research, and in probable future applications. Problems such as <span class="calibre10">image captioning</span>, visual question answering, and many more have gotten great solutions by using this approach. In fact, attention is not limited to visual tasks and was conceived earlier for problems such as neural machine translations and other sophisticated NLP problems. Thus, understanding the attention mechanism is vital to mastering many advanced deep learning techniques.</p>
<p class="calibre2">CNNs are used <span class="calibre10">not only&nbsp;</span>for vision but also for many good applications with attention for solving complex NLP problems, such as <strong class="calibre13">modeling sentence pairs and machine translation</strong>. This chapter covered the attention mechanism and its application to some NLP problems, along with image captioning and recurrent vision models. In RAMs, we did not use CNN; instead, we applied RNN and attention to reduced-size representations of an image from the Glimpse Sensor. But there are recent works to apply attention to CNN-based visual models as well.</p>
<p class="calibre2">Readers are highly encouraged to go through the original papers in the references and also explore advanced concepts in using attention, such as multi-level attention, stacked attention models, and the use of RL&nbsp;models (such as the <strong class="calibre13">Asynchronous Advantage Actor-Critic</strong>&nbsp;(<strong class="calibre13">A3C</strong>) model for the hard attention control problem).</p>
<p class="calibre2"></p>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-51">
        <section>

                            <header>
                    <h1 class="header-title">Other Books You May Enjoy</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre10">If you enjoyed this book, you may be interested in these other books by Packt:</span></p>
<p class="calibre2"><span class="calibre10"><a href="https://www.packtpub.com/big-data-and-business-intelligence/neural-network-programming-tensorflow" target="_blank" class="calibre11"><img src="images/000000.png" class="calibre100" /></a></span></p>
<p class="calibre2"><strong class="calibre13">Neural Network Programming with Tensorflow</strong><br class="calibre18" />
Rajdeep Dua,&nbsp;Manpreet Singh Ghotra</p>
<p class="calibre2">ISBN: 978-1-78839-039-2</p>
<ul class="calibre7">
<li class="calibre8"><span>Learn Linear Algebra and mathematics behind neural network.</span></li>
<li class="calibre8"><span>Dive deep into Neural networks from the basic to advanced concepts like CNN, RNN Deep Belief Networks, Deep Feedforward Networks.</span></li>
<li class="calibre8"><span>Explore Optimization techniques for solving problems like Local minima, Global minima, Saddle points</span></li>
<li class="calibre8"><span>Learn through real world examples like Sentiment Analysis.</span></li>
<li class="calibre8"><span>Train different types of generative models and explore autoencoders.</span></li>
<li class="calibre8"><span>Explore TensorFlow as an example of deep learning implementation.</span></li>
</ul>
<p class="calibre2"><a href="https://www.packtpub.com/big-data-and-business-intelligence/tensorflow-1x-deep-learning-cookbook" class="calibre11"><img class="calibre100" src="images/000084.png" /></a></p>
<p class="calibre2"><strong class="calibre13">TensorFlow 1.x Deep Learning Cookbook</strong><br class="calibre18" />
Antonio Gulli, Amita Kapoor</p>
<p class="calibre2">ISBN: 978-1-78829-359-4</p>
<ul class="calibre7">
<li class="calibre8">Install TensorFlow and use it for CPU and GPU operations</li>
<li class="calibre8">Implement DNNs and apply them to solve different AI-driven problems.</li>
<li class="calibre8">Leverage different data sets such as MNIST, CIFAR-10, and Youtube8m with TensorFlow and learn how to access and use them in your code.</li>
<li class="calibre8">Use TensorBoard to understand neural network architectures, optimize the learning process, and peek inside the neural network black box.</li>
<li class="calibre8">Use different regression techniques for prediction and classification problems</li>
<li class="calibre8">Build single and multilayer perceptrons in TensorFlow</li>
<li class="calibre8">Implement CNN and RNN in TensorFlow, and use it to solve real-world use cases.</li>
<li class="calibre8">Learn how restricted Boltzmann Machines can be used to recommend movies.</li>
<li class="calibre8">Understand the implementation of Autoencoders and deep belief networks, and use them for emotion detection.</li>
<li class="calibre8">Master the different reinforcement learning methods to implement game playing agents.</li>
<li class="calibre8">GANs and their implementation using TensorFlow.</li>
</ul>


            </article>

            
        </section>
    </div>



<div class="calibre" id="calibre_link-167">
        <section>

                            <header>
                    <h1 class="header-title">Leave a review - let other readers know what you think</h1>
                </header>
            
            <article>
                
<p class="calibre2">Please share your thoughts on this book with others by leaving a review on the site that you bought it from. <span class="calibre10">If you purchased the book from Amazon, please leave us an honest review on this book's Amazon page.</span> This is vital so that other potential readers can see and use your unbiased opinion to make purchasing decisions, we can understand what our customers think about our products, and our authors can see your feedback on the title that they have worked with Packt to create. It will only take a few minutes of your time, but is valuable to other potential customers, our authors, and Packt. Thank you!</p>


            </article>

            
        </section>
    </div>



</body></html>